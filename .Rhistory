###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:64)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
#write.csv(articles_df, "../Spooky/Extracted/yao_spooky.csv)
#write.csv(articles_df, "../Spooky/Extracted/yao_spooky.csv")
View(articles_df)
setwd("~/Code/CompText_Jour/exercises")
library(pdftools)
library(tidyverse)
library(readtext)
library(quanteda)
maintext <- pdf_text("assets/pdfs/AI_yao_taufiq.PDF")
writeLines(maintext, "homework_pdf_import.txt")
file_path <- "homework_pdf_import.txt"
text_data <- readLines(file_path)
text_combined <- paste(text_data, collapse = "\n")
documents <- strsplit(text_combined, "End of Document")[[1]]
output_dir <- "assets/extracted_text"
for (i in seq_along(documents)) {
output_file <- file.path(output_dir, paste0("AI_extracted", i, ".txt"))
writeLines(documents[[i]], output_file)
}
cat("Files created:", length(documents), "\n")
AI_index <- read_lines("assets/extracted_text/AI_extracted1.txt")
extracted_lines <- AI_index[16:540]
cat(extracted_lines, sep = "\n")
extracted_lines <- extracted_lines |>
as.data.frame()
extracted_lines <- extracted_lines |>
mutate(extracted_lines = str_remove(extracted_lines, "\\|About LexisNexis | Privacy Policy | Terms & Conditions | Copyright © 2020 LexisNexis"))
cleaned_data <- extracted_lines |>
mutate(trimmed_line = str_trim(extracted_lines),
is_title = str_detect(trimmed_line, "^\\d+\\. "),
# Detect dates (e.g., "Aug 14, 2024")
is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
)
aligned_data <- cleaned_data |>
mutate(
date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
) |>
filter(is_title) |>
select(trimmed_line, date)
final_data <- aligned_data |>
rename(
title = trimmed_line,
date = date
)
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
final_data <- final_data |>
mutate(date = as.Date(date2,format = "%b %d, %Y")) |>
mutate(title =str_remove(title, "^\\d+\\. ")) |>
subset(select = -(date2)) |>
mutate(index = row_number()) |>
select(index, date, title, publication)
View(final_data)
files <- list.files("assets/extracted_text", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(index = str_extract(filename, "\\d+")) |>
mutate(index = as.numeric(index))
final_index <- final_data |>
inner_join(files, c("index")) |>
# mutate(filepath = paste0("~/CompText_Jour-main/exercises/assets/AI_extracted/", filename))
mutate(filepath = paste0("~/Code/CompText_Jour/exercises/assets/extracted_text/", filename))
head(final_index)
View(final_index)
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:64)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
View(articles_df)
setwd("~/Code/Moley")
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
#Using pdftools package. Good for basic PDF extraction
#text <- pdf_text("Moley_newsweek_1956-12-10_48_24 copy.pdf")
text <- pdf_text("Moley_column_sim_newsweek-us_1953_41_index.pdf")
#pdf_text reads the text from a PDF file.
writeLines(text, "moley_text.txt")
#writeLines writes this text to a text file
# Key Points:
# Column Boundaries: The page is divided into three columns based on x positions:
#
# The first third of the page is discarded (column 1).
# The second and third columns are processed in order, preserving the separation.
# Preserving Order: By concatenating the text in the correct sequence (column 2 followed by column 3), you avoid intermingling text from different columns.
#
# Flexible Column Boundaries: You can adjust the cutoff values (0.33 * page_width and 0.66 * page_width) if the columns aren’t perfectly aligned across different articles.
#
# This approach should keep the text from the second and third columns separate and avoid any intermingling of content.
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.4 * page_width  # Left column
column_2_cutoff <- 0.7 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_2 <- page_data[page_data$x > column_1_cutoff & page_data$x <= column_2_cutoff, ]
column_3 <- page_data[page_data$x > column_2_cutoff, ]
# Combine the text from each column separately, line by line, to preserve order
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col2_text, col3_text, sep = " ")
return(full_text)
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Save the cleaned text to a file
#writeLines(cleaned_text, "cleaned_perspective2.txt")
# Key Points:
# Column Boundaries: The page is divided into three columns based on x positions:
#
# The second and third columns are processed in order, preserving the separation.
# Preserving Order: By concatenating the text in the correct sequence (column 2 followed by column 3), you avoid intermingling text from different columns.
#
# Flexible Column Boundaries: You can adjust the cutoff values (0.33 * page_width and 0.66 * page_width) if the columns aren’t perfectly aligned across different articles.
#
# This approach should keep the text from the second and third columns separate and avoid any intermingling of content.
# Extract text with spatial information
pdf_info <- pdf_data("Moley_column_sim_newsweek-us_1953_41_index.pdf")
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.33 * page_width  # Left column
column_2_cutoff <- 0.66 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_1 <- page_data %>% filter(x <= column_1_cutoff) %>% arrange(y)
column_2 <- page_data %>% filter(x > column_1_cutoff & x <= column_2_cutoff) %>% arrange(y)
column_3 <- page_data %>% filter(x > column_2_cutoff) %>% arrange(y)
# Combine the text from each column separately, line by line, to preserve order
col1_text <- paste(column_1$text, collapse = " ")
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col1_text, col2_text, col3_text, sep = " ")
return(full_text)
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Save the cleaned text to a file
#writeLines(cleaned_text, "moley_index.txt")
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_37_61")
library(googlesheets4)
googlesheets4::gs4_deauth()
index <- read_sheets("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
View(index)
index <- index |>
mutate(Text = str_replace_all(Text, "?", ".")))
index <- index |>
mutate(Text = str_replace_all(Text, "?", "."))
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
View(index)
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = ".")
View(index1)
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
View(index1)
index1 <- separate(index1, col = column2, into = c("column", "column4"), sep = "\\:")
View(index1)
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index1 <- separate(index1, col = column3, into = c("column", "column4"), sep = "\\:")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv")
View(sample)
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = [2], index = [1])
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
View(sample)
sample <- sample |>
filter(str_detect(list,"1959$"))
View(sample)
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
sample <- sample |>
filter(str_detect(list,"_1959*"))
View(sample)
sample <- sample |>
filter(str_detect(list,"_1959"))
View(sample)
View(newsweek)
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day), page)
)
View(index)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day))
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
googlesheets4::gs4_deauth()
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
library(stringr)
# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
# Create the `date2` column
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day)))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day)))
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 =  str_extract(text, "\\d{4}-\\d{2}-\\d{2}"))
head(sample2)
head(sample)
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}"))
glimpse(sample)
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate(ymd(date2)))
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate::ymd(date2))
glimpse(sample)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2))
glimpse(index)
index1 <- index |>
inner_join(sample, by=c("date2"))
View(index1)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/sim_newsweek-us_", "/", list, "page",  "/", page, "/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/", page, "/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
head(index1$URL)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",page,"/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = (page + 2))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = page + 2)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 2))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
head(index1$URL)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 1))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
head(index1$URL)
urls <- c(
"https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
"https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
)
urls <- index1$URL
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_page_", i, ".png")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Capture the page as a screenshot
screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file
cat(paste0("Downloaded screenshot: ", filename, "\n"))
# Close the session
session$close()
}
library(chromote)
library(base64enc)  # For base64 decoding
# Initialize the Chromote browser
b <- Chromote$new()
# List of URLs to capture as images
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )
urls <- index1$URL
# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_page_", i, ".png")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Capture the page as a screenshot
screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file
cat(paste0("Downloaded screenshot: ", filename, "\n"))
# Close the session
session$close()
}
# Close the Chromote browser
b$close()
