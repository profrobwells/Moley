year_end <- paste0(year, "1231")
result <- cdx_basic_query(cdx_url,
url = paste0("https://archive.org/details/pub_newsweek-us_", year, "*"),
limit = -1)
Sys.sleep(1)  # Add delay to avoid rate limiting
return(result)
}
# Get results for each year
all_results <- map_dfr(year(start_date):year(end_date), get_year_results)
library(wayback)
library(tidyverse)
library(lubridate)
cdx_url <- "http://web.archive.org/cdx/search/cdx"
start_date <- ymd("1962-01-01")
end_date <- ymd("1969-01-01")
# Function to get results for a specific year
get_year_results <- function(year) {
year_start <- paste0(year, "0101")
year_end <- paste0(year, "1231")
result <- cdx_basic_query(cdx_url,
url = paste0("https://archive.org/details/pub_newsweek-us_", year, "*"),
match_type = "prefix",  # Add this line
limit = -1)
Sys.sleep(1)  # Add delay to avoid rate limiting
return(result)
}
# Get results for each year
all_results <- map_dfr(year(start_date):year(end_date), get_year_results)
test_result <- get_year_results(1962)
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- "https://archive.org/advancedsearch.php"
query <- list(
q = paste0('collection:pub_newsweek-us AND date:[', year, ' TO ', year, ']'),
fl = c("identifier", "date", "title"),
sort = c("date asc"),
output = "json",
rows = 1000  # Adjust this if you need more results per year
)
response <- GET(url, query = query)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
if (length(data$response$docs) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- as_tibble(data$response$docs)
results$year <- year
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to 1969
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- "https://archive.org/advancedsearch.php"
query <- list(
q = paste0('collection:pub_newsweek-us AND date:[', year, ' TO ', year, ']'),
fl = paste(c("identifier", "date", "title"), collapse = ","),
sort = c("date asc"),
output = "json",
rows = 1000  # Adjust this if you need more results per year
)
response <- GET(url, query = query)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
if (length(data$response$docs) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- as_tibble(data$response$docs)
results$year <- year
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to 1969
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Process results
processed_results <- all_results %>%
mutate(date = as_date(date)) %>%
filter(!is.na(date)) %>%
arrange(date)
# Display the first few results
head(processed_results)
# Save results to a CSV file
write_csv(processed_results, "newsweek_issues.csv")
View(processed_results)
head(processed_results)
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- "https://archive.org/advancedsearch.php"
query <- list(
q = paste0('collection:pub_newsweek-us AND date:[', year, ' TO ', year, '] AND -identifier:*_index'),
fl = paste(c("identifier", "date", "title"), collapse = ","),
sort = "date asc",
output = "json",
rows = 1000  # Adjust this if you need more results per year
)
response <- GET(url, query = query)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
if (length(data$response$docs) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- as_tibble(data$response$docs)
results$year <- year
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Process results
processed_results <- all_results %>%
mutate(date = as_date(date)) %>%
filter(!is.na(date)) %>%
arrange(date) %>%
mutate(url = paste0("https://archive.org/details/", identifier))
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
base_url <- "https://archive.org/metadata/pub_newsweek-us"
response <- GET(base_url)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
files <- as_tibble(data$files)
results <- files %>%
filter(str_detect(name, paste0("^sim_newsweek-us_", year))) %>%
filter(!str_detect(name, "_index")) %>%
mutate(
date = as_date(str_extract(name, "\\d{4}-\\d{2}-\\d{2}")),
identifier = str_replace(name, "\\..*$", ""),
title = str_replace_all(name, "_", " "),
year = year
) %>%
select(date, identifier, title, year)
if (nrow(results) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Process results
processed_results <- all_results %>%
filter(!is.na(date)) %>%
arrange(date) %>%
mutate(url = paste0("https://archive.org/details/", identifier))
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
base_url <- "https://archive.org/metadata/pub_newsweek-us"
response <- GET(base_url)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
files <- as_tibble(data$files)
results <- files %>%
filter(str_detect(name, paste0("^sim_newsweek-us_", year))) %>%
filter(!str_detect(name, "_index")) %>%
mutate(
date_string = str_extract(name, "\\d{4}-\\d{2}-\\d{2}"),
identifier = str_replace(name, "\\..*$", ""),
title = str_replace_all(name, "_", " "),
year = year
) %>%
select(date_string, identifier, title, year)
if (nrow(results) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Process results
processed_results <- all_results %>%
mutate(date = as_date(date_string)) %>%
filter(!is.na(date)) %>%
arrange(date) %>%
mutate(url = paste0("https://archive.org/details/", identifier)) %>%
select(date, identifier, title, year, url)
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
base_url <- "https://archive.org/metadata/pub_newsweek-us"
response <- GET(base_url)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
files <- as_tibble(data$files)
results <- files %>%
filter(str_detect(name, paste0("^sim_newsweek-us_", year))) %>%
filter(!str_detect(name, "_index")) %>%
mutate(
date = as_date(str_extract(name, "\\d{4}-\\d{2}-\\d{2}")),
identifier = str_replace(name, "\\..*$", ""),
title = str_replace_all(name, "_", " "),
year = year,
url = paste0("https://archive.org/details/", identifier)
) %>%
select(date, identifier, title, year, url) %>%
filter(!is.na(date)) %>%
arrange(date)
if (nrow(results) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- "https://archive.org/advancedsearch.php"
query <- list(
q = paste0('collection:pub_newsweek-us AND date:[', year, '-01-01 TO ', year, '-12-31] AND !identifier:*_index'),
fl = "identifier,date,title",
sort = "date asc",
output = "json",
rows = 1000  # Adjust this if you need more results per year
)
response <- GET(url, query = query)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
if (length(data$response$docs) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- as_tibble(data$response$docs) %>%
mutate(
date = as_date(date),
year = year,
url = paste0("https://archive.org/details/", identifier)
) %>%
arrange(date)
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(httr)
library(rvest)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- paste0("https://archive.org/search.php?query=collection%3A%28pub_newsweek-us%29%20AND%20date%3A", year, "&sort=-date")
page <- read_html(url)
titles <- page %>% html_nodes(".item-title") %>% html_text()
dates <- page %>% html_nodes(".item-date") %>% html_text()
identifiers <- page %>% html_nodes(".item-ia") %>% html_attr("data-id")
if (length(titles) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- tibble(
title = titles,
date = dates,
identifier = identifiers,
year = year,
url = paste0("https://archive.org/details/", identifiers)
) %>%
mutate(date = as_date(date)) %>%
filter(!str_detect(identifier, "_index")) %>%
arrange(date)
Sys.sleep(2)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(tidyverse)
library(lubridate)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble()
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (month(date) != month) next  # Skip invalid dates
volume <- 59 + (year - 1962)  # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
library(tidyverse)
library(lubridate)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next  # Skip invalid dates
volume <- 59 + (year - 1962)  # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
View(all_results)
head(all_results)
head(all_results$url)
newsweek1962 <- all_results %>%
filter(str_detect(identifier, "1962"))
View(newsweek1962)
library(tidyverse)
library(lubridate)
library(httr)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next # Skip invalid dates
volume <- 59 + (year - 1962) # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
response <- HEAD(url)
if (status_code(response) == 200) {
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
library(tidyverse)
library(lubridate)
library(httr)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next # Skip invalid dates
volume <- 59 + (year - 1962) # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
response <- HEAD(url)
if (status_code(response) == 200) {
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1963
all_results <- generate_newsweek_issues(start_year, end_year)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
View(all_results)
head(all_results$url)
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_48_68_results")
newsweek1942 <- newsweek %>%
filter(str_detect(identifier, "1942"))
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
newsweek1942_clean <- gsub("c\\(\"|\"\\)|\\n", "", newsweek1942)
x_clean <- newsweek1942_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)
head(x_list)
results <- x_list
head(results)
reticulate::repl_python()
