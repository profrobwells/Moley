View(index1)
index1 <- separate(index1, col = column2, into = c("column", "column4"), sep = "\\:")
View(index1)
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index1 <- separate(index1, col = column3, into = c("column", "column4"), sep = "\\:")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv")
View(sample)
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = [2], index = [1])
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
View(sample)
sample <- sample |>
filter(str_detect(list,"1959$"))
View(sample)
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
sample <- sample |>
filter(str_detect(list,"_1959*"))
View(sample)
sample <- sample |>
filter(str_detect(list,"_1959"))
View(sample)
View(newsweek)
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day), page)
)
View(index)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day))
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
googlesheets4::gs4_deauth()
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
library(stringr)
# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
# Create the `date2` column
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d_%s", Year, month_num, as.integer(day)))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day)))
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 =  str_extract(text, "\\d{4}-\\d{2}-\\d{2}"))
head(sample2)
head(sample)
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}"))
glimpse(sample)
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate(ymd(date2)))
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate::ymd(date2))
glimpse(sample)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2))
glimpse(index)
index1 <- index |>
inner_join(sample, by=c("date2"))
View(index1)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/sim_newsweek-us_", "/", list, "page",  "/", page, "/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/", page, "/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
head(index1$URL)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",page,"/","mode/2up"))
head(index1$URL)
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/",page,"/","mode/2up"))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = (page + 2))
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = page + 2)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 2))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
head(index1$URL)
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 1))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
head(index1$URL)
urls <- c(
"https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
"https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
)
urls <- index1$URL
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_page_", i, ".png")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Capture the page as a screenshot
screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file
cat(paste0("Downloaded screenshot: ", filename, "\n"))
# Close the session
session$close()
}
library(chromote)
library(base64enc)  # For base64 decoding
# Initialize the Chromote browser
b <- Chromote$new()
# List of URLs to capture as images
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )
urls <- index1$URL
# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_page_", i, ".png")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Capture the page as a screenshot
screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file
cat(paste0("Downloaded screenshot: ", filename, "\n"))
# Close the session
session$close()
}
# Close the Chromote browser
b$close()
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
#text <- pdf_text("Moley_newsweek_1956-12-10_48_24 copy.pdf")
text <- pdf_text("newsweek_page_1.png")
system("brew install tesseract")
system("brew install xpdf")
system("xcode-select --install")
system("brew install libtiff")
system("brew install ghostscript")
system("brew install imagemagick")
# Convert PNG to searchable PDF
system("tesseract ~/Code/Moley/newsweek_page_1.png output")
# # Convert the searchable PDF to text
system("pdftotext output.pdf output.txt")
# Convert PNG to searchable PDF
system("tesseract ~/Code/Moley/newsweek_page_1.png output")
# # Convert the searchable PDF to text
system("pdftotext output.pdf output.txt")
# Convert PNG to searchable PDF
system("tesseract ~/Code/Moley/newsweek_page_1.png output.pdf")
# # Convert the searchable PDF to text
system("pdftotext output.pdf output.txt")
getwd()
system("tesseract /Users/robwells/Code/Moley/newsweek_page_1.png output")
library(googlesheets4)
googlesheets4::gs4_deauth()
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
#replace question marks for column splitting
index <- index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period
index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")
index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")
library(stringr)
# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
# Create the `date2` column
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 1))
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
#filtering
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate::ymd(date2))
index1 <- index |>
inner_join(sample, by=c("date2")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
View(index1)
library(chromote)
library(base64enc)  # For base64 decoding
# Initialize the Chromote browser
b <- Chromote$new()
# List of URLs to capture as images
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )
urls <- index1$URL
# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_nov6page_", i, ".png")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Set viewport size and device scale factor for higher resolution
session$Emulation$setDeviceMetricsOverride(
width = 1920,        # Width in pixels
height = 1080,       # Height in pixels
deviceScaleFactor = 2,  # Increase the pixel density (higher means higher resolution)
mobile = FALSE
)
# Capture the page as a screenshot
screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file
cat(paste0("Downloaded screenshot: ", filename, "\n"))
# Close the session
session$close()
}
# Close the Chromote browser
b$close()
# Install tesseract if not already installed
#system("brew install tesseract")
# Convert PNG to searchable PDF
system("tesseract ~/Code/Moley/newsweek_nov6page_1.png output.pdf")
# # Convert the searchable PDF to text
system("pdftotext output.pdf output.txt")
library(chromote)
library(base64enc)  # For base64 decoding
# Initialize the Chromote browser
b <- Chromote$new()
# List of URLs to capture as PDFs
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )
urls <- index1$URL
# Loop through each URL and save it as a PDF
for (i in seq_along(urls)) {
url <- urls[i]
filename <- paste0("newsweek_nov6page_", i, ".pdf")
# Open a new session for each URL
session <- b$new_session()
# Navigate to the URL
session$Page$navigate(url)
session$Page$loadEventFired()  # Wait until the initial page load event fires
Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely
# Capture the page as a PDF
pdf_data <- session$Page$printToPDF(
printBackground = TRUE,  # Include background graphics
preferCSSPageSize = TRUE # Use CSS-defined page sizes
)
writeBin(base64decode(pdf_data$data), filename)  # Save binary data to a PDF file
cat(paste0("Downloaded PDF: ", filename, "\n"))
# Close the session
session$close()
}
# Close the Chromote browser
b$close()
View(index1)
index1$URL
# Load required libraries
library(rvest)
library(pdftools)
# URL of the page to scrape
url <- "https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/n89/mode/2up"
# Read the HTML content of the page
page <- read_html(url)
# Inspect the HTML structure to locate the "Perspective" article
# (e.g., find the appropriate CSS selector)
# Try to select nodes that contain the text, adjust selector if needed
article_text <- page %>%
html_nodes("div") %>%  # Update this selector based on page inspection
html_text()
# Filter out only the relevant article text (Perspective article)
# You might need to use regular expressions or `stringr` functions
perspective_text <- grep("Perspective", article_text, value = TRUE)
# Combine the text if it's split into multiple elements
perspective_text <- paste(perspective_text, collapse = "\n\n")
# Check the extracted text
cat(perspective_text)
# Save the extracted text to a PDF
pdf_file <- "Perspective_Article.pdf"
pdf_text <- c(perspective_text)  # pdftools expects text as a character vector
pdf(pdf_file, width = 8.5, height = 11)
text(perspective_text, pos = 4, cex = 0.8)
# Load required libraries
library(rvest)
library(pdftools)
library(grid)
# URL of the page to scrape
url <- "https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/n89/mode/2up"
# Read the HTML content of the page
page <- read_html(url)
# Inspect the HTML structure to locate the "Perspective" article
# (e.g., find the appropriate CSS selector)
# Try to select nodes that contain the text, adjust selector if needed
article_text <- page %>%
html_nodes("div") %>%  # Update this selector based on page inspection
html_text()
# Filter out only the relevant article text (Perspective article)
# You might need to use regular expressions or `stringr` functions
perspective_text <- grep("Perspective", article_text, value = TRUE)
# Combine the text if it's split into multiple elements
perspective_text <- paste(perspective_text, collapse = "\n\n")
# Check the extracted text
cat(perspective_text)
# Save the extracted text to a PDF using grid.text for layout control
pdf_file <- "Perspective_Article.pdf"
pdf(pdf_file, width = 8.5, height = 11)
grid.text(perspective_text, x = 0.1, y = 0.9, just = "left", gp = gpar(fontsize = 10), draw = TRUE)
dev.off()
cat(paste0("PDF saved as: ", pdf_file))
# Load required libraries
library(rvest)
# URL of the page to scrape
url <- "https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/n89/mode/2up"
# Read the HTML content of the page
page <- read_html(url)
# Inspect the HTML structure to locate the "Perspective" article
# Try to select nodes that contain the text, adjust selector if needed
article_text <- page %>%
html_nodes("div") %>%  # Update this selector based on page inspection
html_text()
# Filter out only the relevant article text (Perspective article)
# You might need to use regular expressions or `stringr` functions
perspective_text <- grep("Perspective", article_text, value = TRUE)
# Combine the text if it's split into multiple elements
perspective_text <- paste(perspective_text, collapse = "\n\n")
# Check the extracted text
cat(perspective_text)
# Save the extracted text to a PDF with line breaks
pdf_file <- "Perspective_Article.pdf"
pdf(pdf_file, width = 8.5, height = 11)
# Start a blank plot
plot.new()
par(mar = c(1, 1, 1, 1))  # Adjust margins
# Split text into lines to fit on page
lines <- strwrap(perspective_text, width = 80)  # Adjust width for line length
# Add text to PDF line by line
text(x = 0.05, y = seq(0.95, 0.05, length.out = length(lines)),
labels = lines, adj = 0, cex = 0.8)
# Close the PDF device
dev.off()
cat(paste0("PDF saved as: ", pdf_file))
# Load required libraries
library(rvest)
library(grid)
# URL of the page to scrape
url <- "https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/n89/mode/2up"
# Read the HTML content of the page
page <- read_html(url)
# Delay for 5 seconds to allow the page to fully load
Sys.sleep(5)
# Locate the "Perspective" article - adjust selector as needed
article_text <- page %>%
html_nodes("div") %>%  # Adjust this selector based on inspection
html_text()
# Filter to include only the relevant text containing "Perspective"
perspective_text <- grep("Perspective", article_text, value = TRUE)
# Combine text if it's in multiple elements
perspective_text <- paste(perspective_text, collapse = "\n\n")
# Wrap text to ensure it fits within PDF dimensions
lines <- strwrap(perspective_text, width = 80)
# Create PDF and set text layout with grid
pdf_file <- "Perspective_Article.pdf"
pdf(pdf_file, width = 8.5, height = 11)
# Start grid and draw text line by line
grid.newpage()
y_position <- 0.9  # Start near the top of the page
for (line in lines) {
grid.text(line, x = 0.1, y = y_position, just = "left", gp = gpar(fontsize = 10))
y_position <- y_position - 0.03  # Move down for each line
}
dev.off()
cat(paste0("PDF saved as: ", pdf_file))
