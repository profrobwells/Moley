count(word, sort = TRUE)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_bronte %>%
count(word, sort = TRUE)
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
pivot_wider(names_from = author, values_from = proportion) %>%
pivot_longer(`Brontë Sisters`:`H.G. Wells`,
names_to = "author", values_to = "proportion")
frequency
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`,
color = abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
setwd("~/Code/Moley")
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
#create an index with the file name
mutate(index = str_extract(filename, "\\d+")) |>
mutate(index = as.numeric(index))
#load final data if you haven't already
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls")
View(final_data)
View(files)
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names() |>
mutate(index2 = str_replace_all(index, ".pdf",""))
library(janitor)
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names() |>
mutate(index2 = str_replace_all(index, ".pdf",""))
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(index2 = str_replace_all(index, "_page0.txt",""))
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(index2 = str_replace_all(filename, "_page0.txt",""))
final_index <- final_data |>
inner_join(files, c("index2")) |>
mutate(filepath = paste0("./perspective_extracted/", filename))
head(final_index)
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#write.csv(articles_df, "../exercises/assets/extracted_text/kemi_df2.csv")
View(articles_df)
write.csv(articles_df, "moley_perspective_text.csv")
head(articles_df)
cleaned_articles_df <- articles_df %>%
group_by(filename) %>%
mutate(row_num = row_number()) %>%
filter(row_num >= which(str_detect(sentence, "PERSPECTIVE"))[1]) %>%
select(-row_num) %>%
ungroup()
View(cleaned_articles_df)
View(articles_df)
cleaned_articles_df <- articles_df %>%
group_by(filename) %>%
mutate(row_num = row_number()) %>%
filter(!(row_num == 1 & str_detect(sentence, "^Here's"))) %>%
select(-row_num) %>%
ungroup()
View(cleaned_articles_df)
cleaned_articles_df %>%
group_by(filename) %>%
slice(1) %>%
select(filename, sentence) %>%
head(10)
137868-136596
write.csv(cleaned_articles_df, "moley_cleaned_perspective_text.csv")
failed <- rio::import("https://docs.google.com/spreadsheets/d/1J4cBKY_xTz5D7u7_S_0mL1Qm2nFxZ2d7e6Ip-IjJu6s/edit?gid=1417593840#gid=1417593840", which="Copy of failed scans")
View(failed)
View(final_data)
failed2 <- failed |>
inner_join(final_data, by=c("Errors"="index"))
View(failed2)
write.csv(failed2, "perspective_failed_index.csv")
library(textdata)
library(tidyverse)
library(pdftools)
library(dplyr)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
library(formattable)
library(forcats)
library(readtext)
#topic modeling
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
articles_text <-  read_xlsx("matching with extract-Perspective_full_index_1967_1937.xls")
article_index <-  rio::import("matching with extract-Perspective_full_index_1967_1937.xls")
article_index <-  rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names()
library(janitor)
article_index <-  rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names()
articles_text <-  read_csv("'/Users/gizmofo/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/moley_cleaned_perspective_text.csv'")
articles_text <-  read_csv("/Users/gizmofo/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/moley_cleaned_perspective_text.csv")
nrows <- nrow(article_index)
ncols <- ncol(article_index)
mean_year <- mean(article_index$year)
min_year <- min(article_index$year)
max_year <- max(article_index$year)
year_counts <-
article_index %>%
count(year)
max_count <- max(year_counts$n)
max_years <- year_counts %>%
select(year) %>%
filter(year_counts$n == max_count)
glue::glue("There are {nrows} articles");
glue::glue("The earliest year of publication is {as.integer(min_year)}, and the latest is {max_year}.")
glue::glue("The average year of publication is {as.integer(mean_year)}, with the majority of articles written in {max_years$year}.")
mean_year <- mean(article_index$year, na.rm = TRUE)
glimpse(article_index)
article_index <-  rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names() |>
mutate(year = as.numeric(year),
date = as.Date(pubdate, format="%b %d, %Y"))
glimpse(article_index)
mean_year <- mean(article_index$year, na.rm = TRUE)
mean_year <- round(mean(article_index$year, na.rm = TRUE),0)
glue::glue("There are {nrows} articles");
glue::glue("The earliest year of publication is {as.integer(min_year)}, and the latest is {max_year}.")
glue::glue("The average year of publication is {as.integer(mean_year)}, with the majority of articles written in {max_years$year}.")
nrows <- nrow(articles_text)
ncols <- ncol(articles_text)
glue::glue("The number of rows is {nrows}")
glue::glue("The number of columns is {ncols}")
nrows <- nrow(articles_text)
ncols <- ncol(articles_text)
glue::glue("The number of rows in the article text dataframe is {nrows}")
glue::glue("The number of article text columns is {ncols}")
names(articles_text)
data(stop_words)
one_word_per_row <- article_text %>%
mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=1 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!is.na(word))
data(stop_words)
one_word_per_row <- articles_text %>%
mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "- ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=1 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!is.na(word))
View(one_word_per_row)
bigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "raymond", "")) %>%
mutate(text = str_replace_all(text, "newsweek", "")) %>%
mutate(text = str_replace_all(text, "image", "")) %>%
mutate(text = str_replace_all(text, "perspective", "")) %>%
mutate(text = str_replace_all(text, "- ", "")) %>%
mutate(text = str_replace_all(text, " -", "")) %>%
mutate(text = str_replace_all(text, " - ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=2 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!word == "minor inaccuracies") %>%
filter(!word == "text extraction") %>%
filter(!word == "text version") %>%
filter(!is.na(word))
bigrams <- bigrams %>%
select(word, date, year, month, day, filename)
names(articles_text)
bigrams <- bigrams %>%
select(word, date, year, filename)
articles_text <-  read_csv("/Users/gizmofo/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/moley_cleaned_perspective_text.csv") |>
mutate(year = as.numeric(year),
date = as.Date(pubdate, format="%b %d, %Y"))
bigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "raymond", "")) %>%
mutate(text = str_replace_all(text, "newsweek", "")) %>%
mutate(text = str_replace_all(text, "image", "")) %>%
mutate(text = str_replace_all(text, "perspective", "")) %>%
mutate(text = str_replace_all(text, "- ", "")) %>%
mutate(text = str_replace_all(text, " -", "")) %>%
mutate(text = str_replace_all(text, " - ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=2 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!word == "minor inaccuracies") %>%
filter(!word == "text extraction") %>%
filter(!word == "text version") %>%
filter(!is.na(word))
bigrams <- bigrams %>%
select(word, date, year, filename)
View(bigrams)
bigrams_separated <- bigrams %>%
separate(word, c("word1", "word2"), sep = " ")
#bigrams with stop words filtered
bigrams_filtered <-
bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
top_20_bigrams <- bigram_counts |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
View(top_20_bigrams)
bigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "raymond", "")) %>%
mutate(text = str_replace_all(text, "newsweek", "")) %>%
mutate(text = str_replace_all(text, "image", "")) %>%
mutate(text = str_replace_all(text, "perspective", "")) %>%
mutate(text = str_replace_all(text, "patent   office", "")) %>%
mutate(text = str_replace_all(text, "u.s   patent", "")) %>%
mutate(text = str_replace_all(text, "registered   u.s.", "")) %>%
mutate(text = str_replace_all(text, "- ", "")) %>%
mutate(text = str_replace_all(text, " -", "")) %>%
mutate(text = str_replace_all(text, " - ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=2 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!word == "minor inaccuracies") %>%
filter(!word == "text extraction") %>%
filter(!word == "text version") %>%
filter(!is.na(word))
bigrams <- bigrams %>%
select(word, date, year, filename)
bigrams_separated <- bigrams %>%
separate(word, c("word1", "word2"), sep = " ")
#bigrams with stop words filtered
bigrams_filtered <-
bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
top_20_bigrams <- bigram_counts |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
View(top_20_bigrams)
View(articles_text)
bigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |>
mutate(text = tolower(sentence)) |>
mutate(text = gsub("\\d+", "", text)) |>
mutate(text = str_replace_all(text, "raymond", "")) %>%
mutate(text = str_replace_all(text, "newsweek", "")) %>%
mutate(text = str_replace_all(text, "image", "")) %>%
mutate(text = str_replace_all(text, "perspective", "")) %>%
mutate(text = str_replace_all(text, "registered u.s. patent office", "")) %>%
mutate(text = str_replace_all(text, "- ", "")) %>%
mutate(text = str_replace_all(text, " -", "")) %>%
mutate(text = str_replace_all(text, " - ", "")) %>%
unnest_tokens(word, text, token="ngrams", n=2 ) %>%
filter(!word %in% stop_words$word) %>%
filter(!word == "minor inaccuracies") %>%
filter(!word == "text extraction") %>%
filter(!word == "text version") %>%
filter(!is.na(word))
bigrams <- bigrams %>%
select(word, date, year, filename)
bigrams_separated <- bigrams %>%
separate(word, c("word1", "word2"), sep = " ")
#bigrams with stop words filtered
bigrams_filtered <-
bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
top_20_bigrams <- bigram_counts |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
datatable(bigram_counts,
caption = "Top Bigrams",
options = list(pageLength = 20))
top_20_bigrams <- bigram_counts |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
ggplot(top_20_bigrams, aes(n, bigram, fill=n)) +
geom_bar(stat="identity", position="dodge") +
labs(title = "Twenty Most Common Two-Word Phrases in Articles By Raymond Moley, 1937-1967",
x = "Count Across All Articles",
y = "Two-Word Phrase")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
labs(title = "Twenty Most Common Two-Word Phrases in Articles By Raymond Moley, 1937-1967",
x = "Count Across All Articles",
y = "Two-Word Phrase")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
theme(legend.position="none") +
labs(title = "Raymond Moley Common Phrases, 1937-1967",
x = "Count",
y = "Bigrams")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
theme(legend.position="none") +
geom_text(aes(label=n), hjust=-0.2) +
labs(title = "Raymond Moley Common Phrases, 1937-1967",
x = "Count",
y = "Bigrams")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
theme(legend.position="none") +
geom_text(aes(label=n), hjust=-0.2, size=2) +
labs(title = "Raymond Moley Common Phrases, 1937-1967",
x = "Count",
y = "Bigrams")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
theme(legend.position="none") +
geom_text(aes(label=n), hjust=-0.2, size=3) +
labs(title = "Raymond Moley Common Phrases, 1937-1967",
x = "Count",
y = "Bigrams")
articles <- articles_text |>
distinct(filename) |>
count()
View(articles)
nrows1 <- nrow(articles_text)
ncols1 <- ncol(articles_text)
articles <- articles_text |>
distinct(filename) |>
count()
nrows <- nrow(article_index)
ncols <- ncol(article_index)
nrows1 <- nrow(articles_text)
ncols1 <- ncol(articles_text)
articles <- articles_text |>
distinct(filename) |>
count()
glue::glue("The number of rows in the article text dataframe is {nrows1}")
glue::glue("The number of article text columns is {ncols1}")
glue::glue("The article text has {articles} articles. That's less than the {nrows} entries in the article index. The difference was due to about 70 failed AI scans.")
glue::glue("The article text has {articles} articles. That's less than the {nrows} entries in the article index. The difference was due to about {articles - nows} failed AI scans.")
glue::glue("The article text has {articles} articles. That's less than the {nrows} entries in the article index. The difference was due to about {articles - nrows} failed AI scans.")
1562-1487
glue::glue("The article text has {articles} articles. That's less than the {nrows} entries in the article index. The difference was due to about {nrows-articles} failed AI scans.")
ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
geom_bar(stat="identity", position="dodge") +
theme(legend.position="none") +
geom_text(aes(label=n), hjust=-0.2, size=3) +
labs(title = "Raymond Moley Common Phrases, 1937-1967",
subtitle = "Analysis of 1,487 Newsweek columns",
x = "Count",
y = "Bigrams",
caption = "Source: Newsweek. Graphic by Rob Wells and Bridget Lang, 2-14-2024")
count_year <- articles_text %>%
count(year) %>%
group_by(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1935, 1970, 5))) +
labs(title = "Moley Newsweek Columns By Year, 1937-1967",
subtitle = "Extracted Text Only",
caption = "n=1,487  articles. Graphic by Rob Wells, 2/14/2025",
y="Count of Pages",
x="Year")
count_year
articles_text %>%
count(year)
articles_text %>%
count(year) %>%
group_by(year)
articles_text %>%
distinct(filename) |>
count(year)
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year)
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(labels = c(seq(1935, 1970, 5))) +
labs(title = "Moley Newsweek Columns By Year, 1937-1967",
subtitle = "Extracted Text Only",
caption = "n=1,487  articles. Graphic by Rob Wells, 2/14/2025",
y="Count of Pages",
x="Year")
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year)
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none")
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(breaks = seq(min(year), max(year), by = 5))+
labs(title = "Moley Newsweek Columns By Year, 1937-1967",
subtitle = "Extracted Text Only",
caption = "n=1,487  articles. Graphic by Rob Wells, 2/14/2025",
y="Count of Pages",
x="Year")
articles_text %>%
distinct(filename, .keep_all = TRUE) |>
count(year) %>%
#Sandwich it onto a simple ggplot
ggplot(aes(x = year, y = n, fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
scale_x_continuous(breaks = seq(1935, 1970, by = 5))+
labs(title = "Moley Newsweek Columns By Year, 1937-1967",
subtitle = "Extracted Text Only",
caption = "n=1,487  articles. Graphic by Rob Wells, 2/14/2025",
y="Count of Pages",
x="Year")
