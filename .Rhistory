install.packages("tidyverse")
# CRAN will not have spaCy installed, so create static vignette
knitr::opts_chunk$set(eval = FALSE)
install.packages("cleanNLP")
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(sotu)
install.packages("sotu")
library(sotu)
cnlp_init_spacy()
# CRAN will not have spaCy installed, so create static vignette
knitr::opts_chunk$set(eval = FALSE)
#install.packages("cleanNLP")
#install.packages("sotu")
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(sotu)
cnlp_init_spacy()
input <- sotu_meta
input$text <- sotu_text
anno <- cnlp_annotate(input)
cnlp_init_spacy()
pip install cleannlp
cnlp_init_spacy()
# CRAN will not have spaCy installed, so create static vignette
knitr::opts_chunk$set(eval = FALSE)
#install.packages("cleanNLP")
#install.packages("sotu")
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(sotu)
cnlp_init_spacy()
install.packages("reticulate")
library(reticulate)
py_discover_config(required_module="cleannlp")
install.packages("reticulate")
#install.packages("cleanNLP")
#install.packages("sotu")
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(sotu)
# CRAN will not have spaCy installed, so create static vignette
knitr::opts_chunk$set(eval = FALSE)
cnlp_init_spacy()
#install.packages("reticulate")
library(reticulate)
py_discover_config(required_module="cleannlp")
py_discover_config(required_module="cleannlp")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
py_discover_config(required_module="cleannlp")
install.packages("cleanNLP")
install.packages("cleanNLP")
library(cleanNLP)
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_init_corenlp(lang="en")
Sys.setenv(RETICULATE_PYTHON = /Applications/Python 3.11)
cnlp_download_spacy(model_name="en")
reticulate::py_last_error()
py_discover_config(required_module="cleannlp")
sys.setenv(reticulate_python = /Users/gizmofo/anaconda3)
sys.setenv(reticulate_python = Users/gizmofo/anaconda3)
sys.setenv(reticulate_python = gizmofo/anaconda3)
sys.setenv(reticulate_python = /Users/gizmofo/anaconda3)
sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3")
sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
pip install cleannlp
cnlp_init_spacy(model_name="en")
cnlp_download_spacy(model_name="en")
cnlp_download_corenlp(lang="en")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_download_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_download_corenlp()
cnlp_init_corenlp(lang="en")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_init_corenlp(lang="en")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_init_corenlp(lang="en")
cnlp_download_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy("en")
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
# CRAN will not have spaCy installed, so create static vignette
knitr::opts_chunk$set(eval = FALSE)
#install.packages("cleanNLP")
#install.packages("sotu")
library(magrittr)
library(dplyr)
library(ggplot2)
library(cleanNLP)
library(sotu)
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python3.11")
py_discover_config(required_module="cleannlp")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
py_discover_config(required_module="cleannlp")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_download_spacy(model_name="en")
cnlp_download_spacy("en")
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy(model_name="en")
spacy.load('en_core_web_sm')
cnlp_init_spacy(model_name="en")
py_discover_config(required_module="cleannlp")
#install.packages("reticulate")
library(reticulate)
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_init_corenlp(lang="en")
input <- sotu_meta
input$text <- sotu_text
anno <- cnlp_annotate(input)
reticulate::py_last_error()
cnlp_init_spacy()
cnlp_download_spacy("en")
cnlp_init_spacy()
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
py_discover_config(required_module="cleannlp")
cnlp_download_corenlp()
cnlp_download_spacy("en")
cnlp_download_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy("en")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy("en")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
py_discover_config(required_module="cleannlp")
#Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python")
Sys.setenv(reticulate_python = "/Users/gizmofo/anaconda3/bin/python3.11/site-packages/cleannlp")
py_discover_config(required_module="cleannlp")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy("en")
spacy.load('en_core_web_sm')
cnlp_init_corenlp(lang="en")
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_spacy(model_name="en")
cnlp_download_spacy(model_name="en")
cnlp_download_spacy("en")
spacy.load('en_core_web_sm')
cnlp_init_stringi(locale="en_GB")
cnlp_init_udpipe(model_name="english")
cnlp_init_spacy(model_name="en")
cnlp_init_corenlp(lang="en")
cnlp_init_udpipe()
annotation <- cnlp_annotate(input = c(
"Here is the first text. It is short.",
"Here's the second. It is short too!",
"The third text is the shortest."
))
annotation
cnlp_init_spacy()
library(tidyverse)
install.packages("archiveRetriever", force = TRUE)
library(archiveRetriever) # Systematically retrieving web data from the Internet Archive
nytimes_overview <- archive_overview(homepage = "https://www.nytimes.com/",
startDate = "2020-10-01",
endDate = "2020-12-31")
View(nytimes_overview)
nytimes_overview
nytimes_mementos <- retrieve_urls(homepage = "https://www.nytimes.com/",
startDate = "2020-10-01",
endDate = "2020-12-31")
nytimes_mementos[1:10]
#>  [1] "http://web.archive.org/web/20201001000041/https://www.nytimes.com/"
#>  [2] "http://web.archive.org/web/20201002000016/http://nytimes.com/"
#>  [3] "http://web.archive.org/web/20201003000006/https://nytimes.com/"
#>  [4] "http://web.archive.org/web/20201004000201/https://www.nytimes.com/"
#>  [5] "http://web.archive.org/web/20201005000047/http://nytimes.com/"
#>  [6] "http://web.archive.org/web/20201006000036/http://nytimes.com/"
#>  [7] "http://web.archive.org/web/20201007000202/https://www.nytimes.com/"
#>  [8] "http://web.archive.org/web/20201008000222/https://www.nytimes.com/"
#>  [9] "http://web.archive.org/web/20201009000201/https://www.nytimes.com/"
#> [10] "http://web.archive.org/web/20201010000605/http://nytimes.com/"
nytimes_links <- retrieve_links(ArchiveUrls = "http://web.archive.org/web/20201001000041/https://www.nytimes.com/")
View(nytimes_links)
newsweek_links <- retrieve_links(ArchiveUrls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links <- retrieve_urls(ArchiveUrls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links <- retrieve_urls(homepage = "https://archive.org/details/pub_newsweek-us",
startDate = "1937-01-01",
endDate = "1939-12-31")
newsweek_links<- scrape_urls(Urls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D",
Paths = c(title = "//h1[@itemprop='headline']",
author = "//span[@itemprop='name']",
date = "//time//text()",
article = "//section[@itemprop='articleBody']//p"))
newsweek_links<- scrape_urls(Urls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links<- retrieveArchiveLinks(Urls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links<- retrieve_links(Urls = "https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links<- retrieve_links("https://archive.org/details/pub_newsweek-us?query=Moley&sin=TXT&page=2&sort=-date&and%5B%5D=year%3A%5B1937+TO+1940%5D")
newsweek_links<- retrieve_links(ArchiveUrls = "https://archive.org/details/sim_newsweek-us_1937-04-03_9_14")
newsweek_links <- retrieve_urls(homepage = "https://www.newsweek.com/",
startDate = "1937-01-01",
endDate = "1939-12-31")
newsweek_overview <- archive_overview(homepage = "https://www.newsweek.com/",
startDate = "2020-10-01",
endDate = "2020-12-31")
newsweek_overview
newsweek_overview <- archive_overview(homepage = "https://www.newsweek.com/",
startDate = "1937-01-01",
endDate = "1939-12-31")
newsweek_overview
newsweek_mementos <- retrieve_urls(homepage = "https://www.newsweek.com/",
startDate = "1937-10-01",
endDate = "1937-12-31")
newsweek_mementos[1:10]
newsweek_mementos <- retrieve_urls(homepage = "https://www.newsweek.com/",
startDate = "1957-10-01",
endDate = "1957-12-31")
newsweek_mementos[1:10]
newsweek_mementos <- retrieve_urls(homepage = "https://www.newsweek.com/",
startDate = "1987-10-01",
endDate = "1987-12-31")
newsweek_mementos[1:10]
library(tidyverse)
library(tidytext)
setwd("~/Code/CompText_Jour/exercises/future_exercises")
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text
text_df <- tibble(line = 1:4, text = text)
text_df
text_df %>%
unnest_tokens(word, text)
library(janeaustenr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
original_books
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
stopwords <- stop_words
View(stopwords)
stopwords |>
count(lexicon)
data(stop_words)
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE)
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
install.packages("gutenbergr")
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
load("assets/data/hgwells.rda")
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_hgwells %>%
count(word, sort = TRUE)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_bronte %>%
count(word, sort = TRUE)
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
pivot_wider(names_from = author, values_from = proportion) %>%
pivot_longer(`Brontë Sisters`:`H.G. Wells`,
names_to = "author", values_to = "proportion")
frequency
library(scales)
# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`,
color = abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
setwd("~/Code/Moley")
#install.packages("pdftools")
library(tidyverse)
library(pdftools)
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
#create an index with the file name
mutate(index = str_extract(filename, "\\d+")) |>
mutate(index = as.numeric(index))
#load final data if you haven't already
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls")
View(final_data)
View(files)
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names() |>
mutate(index2 = str_replace_all(index, ".pdf",""))
library(janitor)
final_data <- rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |>
clean_names() |>
mutate(index2 = str_replace_all(index, ".pdf",""))
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(index2 = str_replace_all(index, "_page0.txt",""))
files <- list.files("./perspective_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
mutate(index2 = str_replace_all(filename, "_page0.txt",""))
final_index <- final_data |>
inner_join(files, c("index2")) |>
mutate(filepath = paste0("./perspective_extracted/", filename))
head(final_index)
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#write.csv(articles_df, "../exercises/assets/extracted_text/kemi_df2.csv")
View(articles_df)
write.csv(articles_df, "moley_perspective_text.csv")
head(articles_df)
cleaned_articles_df <- articles_df %>%
group_by(filename) %>%
mutate(row_num = row_number()) %>%
filter(row_num >= which(str_detect(sentence, "PERSPECTIVE"))[1]) %>%
select(-row_num) %>%
ungroup()
View(cleaned_articles_df)
View(articles_df)
cleaned_articles_df <- articles_df %>%
group_by(filename) %>%
mutate(row_num = row_number()) %>%
filter(!(row_num == 1 & str_detect(sentence, "^Here's"))) %>%
select(-row_num) %>%
ungroup()
View(cleaned_articles_df)
cleaned_articles_df %>%
group_by(filename) %>%
slice(1) %>%
select(filename, sentence) %>%
head(10)
137868-136596
write.csv(cleaned_articles_df, "moley_cleaned_perspective_text.csv")
failed <- rio::import("https://docs.google.com/spreadsheets/d/1J4cBKY_xTz5D7u7_S_0mL1Qm2nFxZ2d7e6Ip-IjJu6s/edit?gid=1417593840#gid=1417593840", which="Copy of failed scans")
View(failed)
View(final_data)
failed2 <- failed |>
inner_join(final_data, by=c("Errors"="index"))
View(failed2)
write.csv(failed2, "perspective_failed_index.csv")
