get_newsweek_issues <- function(year) {
base_url <- "https://archive.org/metadata/pub_newsweek-us"
response <- GET(base_url)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
files <- as_tibble(data$files)
results <- files %>%
filter(str_detect(name, paste0("^sim_newsweek-us_", year))) %>%
filter(!str_detect(name, "_index")) %>%
mutate(
date = as_date(str_extract(name, "\\d{4}-\\d{2}-\\d{2}")),
identifier = str_replace(name, "\\..*$", ""),
title = str_replace_all(name, "_", " "),
year = year,
url = paste0("https://archive.org/details/", identifier)
) %>%
select(date, identifier, title, year, url) %>%
filter(!is.na(date)) %>%
arrange(date)
if (nrow(results) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- "https://archive.org/advancedsearch.php"
query <- list(
q = paste0('collection:pub_newsweek-us AND date:[', year, '-01-01 TO ', year, '-12-31] AND !identifier:*_index'),
fl = "identifier,date,title",
sort = "date asc",
output = "json",
rows = 1000  # Adjust this if you need more results per year
)
response <- GET(url, query = query)
if (status_code(response) != 200) {
warning(paste("Failed to retrieve data for year", year))
return(NULL)
}
content <- content(response, "text")
data <- fromJSON(content)
if (length(data$response$docs) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- as_tibble(data$response$docs) %>%
mutate(
date = as_date(date),
year = year,
url = paste0("https://archive.org/details/", identifier)
) %>%
arrange(date)
Sys.sleep(1)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(httr)
library(rvest)
library(tidyverse)
library(lubridate)
get_newsweek_issues <- function(year) {
url <- paste0("https://archive.org/search.php?query=collection%3A%28pub_newsweek-us%29%20AND%20date%3A", year, "&sort=-date")
page <- read_html(url)
titles <- page %>% html_nodes(".item-title") %>% html_text()
dates <- page %>% html_nodes(".item-date") %>% html_text()
identifiers <- page %>% html_nodes(".item-ia") %>% html_attr("data-id")
if (length(titles) == 0) {
warning(paste("No results found for year", year))
return(NULL)
}
results <- tibble(
title = titles,
date = dates,
identifier = identifiers,
year = year,
url = paste0("https://archive.org/details/", identifiers)
) %>%
mutate(date = as_date(date)) %>%
filter(!str_detect(identifier, "_index")) %>%
arrange(date)
Sys.sleep(2)  # Add delay to avoid rate limiting
return(results)
}
# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- map_dfr(start_year:end_year, get_newsweek_issues)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
library(tidyverse)
library(lubridate)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble()
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (month(date) != month) next  # Skip invalid dates
volume <- 59 + (year - 1962)  # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
library(tidyverse)
library(lubridate)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next  # Skip invalid dates
volume <- 59 + (year - 1962)  # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
View(all_results)
head(all_results)
head(all_results$url)
newsweek1962 <- all_results %>%
filter(str_detect(identifier, "1962"))
View(newsweek1962)
library(tidyverse)
library(lubridate)
library(httr)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next # Skip invalid dates
volume <- 59 + (year - 1962) # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
response <- HEAD(url)
if (status_code(response) == 200) {
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1969
all_results <- generate_newsweek_issues(start_year, end_year)
library(tidyverse)
library(lubridate)
library(httr)
generate_newsweek_issues <- function(start_year, end_year) {
all_issues <- tibble(
date = as.Date(character()),
identifier = character(),
title = character(),
year = integer(),
url = character()
)
for (year in start_year:end_year) {
for (month in 1:12) {
for (day in 1:31) {
date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
if (is.na(date) || month(date) != month) next # Skip invalid dates
volume <- 59 + (year - 1962) # Assuming volume increments each year
issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
url <- paste0("https://archive.org/details/", identifier)
response <- HEAD(url)
if (status_code(response) == 200) {
all_issues <- all_issues %>%
add_row(
date = date,
identifier = identifier,
title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
year = year,
url = url
)
}
return(all_issues)
}
# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1963
all_results <- generate_newsweek_issues(start_year, end_year)
# Display the first few results
print(head(all_results))
# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
View(all_results)
head(all_results$url)
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_48_68_results")
newsweek1942 <- newsweek %>%
filter(str_detect(identifier, "1942"))
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
newsweek1942_clean <- gsub("c\\(\"|\"\\)|\\n", "", newsweek1942)
x_clean <- newsweek1942_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)
head(x_list)
results <- x_list
head(results)
reticulate::repl_python()
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
#Using pdftools package. Good for basic PDF extraction
text <- pdf_text("Moley_newsweek_1956-12-10_48_24 copy.pdf")
#pdf_text reads the text from a PDF file.
writeLines(text, "moley_text.txt")
#writeLines writes this text to a text file
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Moley_newsweek_1956-12-10_48_24 copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Get the width of the page from the bounding boxes
page_width <- max(sapply(page_data, function(x) x$width))
# Keep only the text that is not in the left third
filtered_text <- page_data[sapply(page_data, function(x) x$x > page_width / 3)]
# Combine the remaining text
paste(sapply(filtered_text, function(x) x$text), collapse = " ")
}
# Apply the function to each page
cleaned_text <- sapply(pdf_info, filter_left_column)
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Moley_newsweek_1956-12-10_48_24 copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Get the width of the page from the bounding boxes of words
page_width <- max(sapply(page_data, function(word) word$x + word$width))
# Filter out words where the x-coordinate falls in the left third
filtered_text <- page_data[sapply(page_data, function(word) word$x > page_width / 3)]
# Combine the remaining text into a single string
paste(sapply(filtered_text, function(word) word$text), collapse = " ")
}
# Apply the function to each page
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Moley_newsweek_1956-12-10_48_24 copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Each `page_data` is a data frame with columns: x, y, width, height, space, and text
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Filter out words that are located in the left third of the page
filtered_words <- page_data[page_data$x > (page_width / 3), ]
# Combine the remaining words into a single string
paste(filtered_words$text, collapse = " ")
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "cleaned_moley_text.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Each `page_data` is a data frame with columns: x, y, width, height, space, and text
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Filter out words that are located in the left third of the page
filtered_words <- page_data[page_data$x > (page_width / 3), ]
# Combine the remaining words into a single string
paste(filtered_words$text, collapse = " ")
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "perspective.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Each `page_data` is a data frame with columns: x, y, width, height, space, and text
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Filter out words that are located in the left third of the page
filtered_words <- page_data[page_data$x > (page_width / 3), ]
# Combine the remaining words into a single string
paste(filtered_words$text, collapse = " ")
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "perspective.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to filter out text in the left third of the page
filter_left_column <- function(page_data) {
# Each `page_data` is a data frame with columns: x, y, width, height, and text
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Filter out words that are located in the left third of the page
filtered_words <- page_data[page_data$x > (page_width / 3), ]
# Combine the remaining words into a single string
paste(filtered_words$text, collapse = " ")
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "perspective.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to filter out text in the left column and ignore small fragments
filter_left_column <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Use 40% of the page width as the cutoff for the left column
cutoff <- 0.4 * page_width
# Filter out words in the left 40% of the page
filtered_words <- page_data[page_data$x > cutoff, ]
# Further remove very small text fragments (likely ads)
filtered_words <- filtered_words[nchar(filtered_words$text) > 2, ]
# Combine the remaining words into a single string
paste(filtered_words$text, collapse = " ")
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_left_column(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "cleaned_perspective_text.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.33 * page_width  # Left column
column_2_cutoff <- 0.66 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_2 <- page_data[page_data$x > column_1_cutoff & page_data$x <= column_2_cutoff, ]
column_3 <- page_data[page_data$x > column_2_cutoff, ]
# Combine the text from each column separately, line by line, to preserve order
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col2_text, col3_text, sep = " ")
return(full_text)
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "cleaned_perspective2.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.4 * page_width  # Left column
column_2_cutoff <- 0.7 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_2 <- page_data[page_data$x > column_1_cutoff & page_data$x <= column_2_cutoff, ]
column_3 <- page_data[page_data$x > column_2_cutoff, ]
# Combine the text from each column separately, line by line, to preserve order
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col2_text, col3_text, sep = " ")
return(full_text)
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "cleaned_perspective2.txt")
library(pdftools)
# Extract text with spatial information
pdf_info <- pdf_data("Perspective_1948-12-06_sim_newsweek-us_copy.pdf")
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.42 * page_width  # Left column
column_2_cutoff <- 0.7 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_2 <- page_data[page_data$x > column_1_cutoff & page_data$x <= column_2_cutoff, ]
column_3 <- page_data[page_data$x > column_2_cutoff, ]
# Combine the text from each column separately, line by line, to preserve order
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col2_text, col3_text, sep = " ")
return(full_text)
}
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Save the cleaned text to a file
writeLines(cleaned_text, "cleaned_perspective2.txt")
library(pdftools)
# Set the path to the folder containing the PDF files
folder_path <- "moley_newsweek"
# Get a list of all PDF files in the folder
pdf_files <- list.files(folder_path, pattern = "\\.pdf$", full.names = TRUE)
# Function to segment columns and remove the first column
filter_columns <- function(page_data) {
# Get the width of the page
page_width <- max(page_data$x + page_data$width)
# Define column boundaries
column_1_cutoff <- 0.4 * page_width  # Left column
column_2_cutoff <- 0.7 * page_width  # Middle column
# Split the text into three columns based on x-coordinates
column_2 <- page_data[page_data$x > column_1_cutoff & page_data$x <= column_2_cutoff, ]
column_3 <- page_data[page_data$x > column_2_cutoff, ]
# Combine the text from each column separately, line by line, to preserve order
col2_text <- paste(column_2$text, collapse = " ")
col3_text <- paste(column_3$text, collapse = " ")
# Combine the text from columns 2 and 3
full_text <- paste(col2_text, col3_text, sep = " ")
return(full_text)
}
# Function to process a single PDF file
process_pdf <- function(pdf_file) {
# Extract the PDF data
pdf_info <- pdf_data(pdf_file)
# Apply the function to each page in the PDF
cleaned_text <- sapply(pdf_info, function(page) filter_columns(page))
# Create the output file name by replacing .pdf with .txt
output_file <- sub(".pdf$", ".txt", basename(pdf_file))
# Save the cleaned text to the new file
writeLines(cleaned_text, file.path(folder_path, output_file))
}
# Process each PDF file in the folder
lapply(pdf_files, process_pdf)
