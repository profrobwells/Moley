search_results <- ia_search('collection:pub_newsweek-us')
# Function to search Internet Archive
ia_search <- function(query, rows = 50) {
base_url <- "https://archive.org/advancedsearch.php"
response <- GET(url = base_url,
query = list(q = query,
fl = "identifier,title",
rows = rows,
output = "json"))
if (status_code(response) == 200) {
content <- content(response, "text")
parsed <- fromJSON(content)
return(list(
num_found = parsed$response$numFound,
docs = parsed$response$docs
))
} else {
stop("Failed to retrieve results from Internet Archive")
}
# Perform the search
search_results <- ia_search('collection:pub_newsweek-us')
print(paste("Number of items found:", search_results$num_found))
sample <- c(
"sim_newsweek-us_1954-01-25_43_4",
"sim_newsweek-us_1954-02-01_43_5",
"sim_newsweek-us_1954-03-29_43_13",
"sim_newsweek-us_1954-04-19_43_16",
"sim_newsweek-us_1954-05-17_43_20",
"sim_newsweek-us_1954-06-14_43_24",
"sim_newsweek-us_1954-07-05_44_1",
"sim_newsweek-us_1954-08-09_44_6",
"sim_newsweek-us_1954-09-13_44_11",
"sim_newsweek-us_1954-10-25_44_17",
"sim_newsweek-us_1954-11-08_44_19",
"sim_newsweek-us_1954-12-27_44_26"
)
home_directory <- path.expand("~")
download_directory <- file.path(home_directory, 'Code', 'Moley', 'Newsweek_54')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
print(paste("Files will be saved in:", download_directory))
# Construct the query
query <- paste(paste0('identifier:', sample), collapse = ' OR ')
download_item <- function(identifier, destdir, retries = 3) {
for (attempt in 1:retries) {
tryCatch({
ia_download(identifier, files = "*.pdf", destdir = destdir)
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Perform the search and download the files
walk(search_results$docs$identifier, ~download_item(.x, download_directory))
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(internetarchive)
library(httr)
library(purrr)
library(jsonlite)
# Function to search Internet Archive
ia_search <- function(query, rows = 50) {
base_url <- "https://archive.org/advancedsearch.php"
response <- GET(url = base_url,
query = list(q = query,
fl = "identifier,title",
rows = rows,
output = "json"))
if (status_code(response) == 200) {
content <- content(response, "text")
parsed <- fromJSON(content)
return(list(
num_found = parsed$response$numFound,
docs = parsed$response$docs
))
} else {
stop("Failed to retrieve results from Internet Archive")
}
# Perform the search
search_results <- ia_search('collection:pub_newsweek-us')
print(paste("Number of items found:", search_results$num_found))
# Define the stratified sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4",
"sim_newsweek-us_1954-02-01_43_5",
"sim_newsweek-us_1954-03-29_43_13",
"sim_newsweek-us_1954-04-19_43_16",
"sim_newsweek-us_1954-05-17_43_20",
"sim_newsweek-us_1954-06-14_43_24",
"sim_newsweek-us_1954-07-05_44_1",
"sim_newsweek-us_1954-08-09_44_6",
"sim_newsweek-us_1954-09-13_44_11",
"sim_newsweek-us_1954-10-25_44_17",
"sim_newsweek-us_1954-11-08_44_19",
"sim_newsweek-us_1954-12-27_44_26"
)
# Define the directory where you want to save the files within your home directory
home_directory <- path.expand("~")
download_directory <- file.path(home_directory, 'Code', 'Moley', 'Newsweek_54')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
print(paste("Files will be saved in:", download_directory))
# Construct the query
query <- paste(paste0('identifier:', sample), collapse = ' OR ')
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
for (attempt in 1:retries) {
tryCatch({
ia_download(identifier, files = "*.pdf", destdir = destdir)
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Perform the search and download the files
walk(search_results$docs$identifier, ~download_item(.x, download_directory))
head(sample)
query <- paste(paste0('identifier:(', paste(sample, collapse = " OR "), ')'), 'AND collection:pub_newsweek-us')
search_results <- ia_search(query)
download_item <- function(identifier, destdir, retries = 3) {
for (attempt in 1:retries) {
tryCatch({
ia_download(identifier, files = "*.pdf", destdir = destdir)
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Perform the search and download the files
walk(sample, ~download_item(.x, download_directory))
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(internetarchive)
library(httr)
library(purrr)
library(jsonlite)
# Function to search Internet Archive
ia_search <- function(query, rows = 50) {
base_url <- "https://archive.org/advancedsearch.php"
response <- GET(url = base_url,
query = list(q = query,
fl = "identifier,title",
rows = rows,
output = "json"))
if (status_code(response) == 200) {
content <- content(response, "text")
parsed <- fromJSON(content)
return(list(
num_found = parsed$response$numFound,
docs = parsed$response$docs
))
} else {
stop("Failed to retrieve results from Internet Archive")
}
# Perform the search
search_results <- ia_search('collection:pub_newsweek-us')
print(paste("Number of items found:", search_results$num_found))
# Define the stratified sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4",
"sim_newsweek-us_1954-02-01_43_5",
"sim_newsweek-us_1954-03-29_43_13",
"sim_newsweek-us_1954-04-19_43_16",
"sim_newsweek-us_1954-05-17_43_20",
"sim_newsweek-us_1954-06-14_43_24",
"sim_newsweek-us_1954-07-05_44_1",
"sim_newsweek-us_1954-08-09_44_6",
"sim_newsweek-us_1954-09-13_44_11",
"sim_newsweek-us_1954-10-25_44_17",
"sim_newsweek-us_1954-11-08_44_19",
"sim_newsweek-us_1954-12-27_44_26"
)
# Define the directory where you want to save the files within your home directory
home_directory <- path.expand("~")
download_directory <- file.path(home_directory, 'Code', 'Moley', 'Newsweek_54')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
print(paste("Files will be saved in:", download_directory))
# Construct the query
query <- paste(paste0('identifier:(', paste(sample, collapse = " OR "), ')'), 'AND collection:pub_newsweek-us')
search_results <- ia_search(query)
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
for (attempt in 1:retries) {
tryCatch({
ia_download(identifier, files = "*.pdf", destdir = destdir)
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Perform the search and download the files
walk(sample, ~download_item(.x, download_directory))
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)
# Function to search Internet Archive
ia_search <- function(query, rows = 50) {
base_url <- "https://archive.org/advancedsearch.php"
response <- GET(url = base_url,
query = list(q = query,
fl = "identifier",
rows = rows,
output = "json"))
if (status_code(response) == 200) {
content <- content(response, "text")
parsed <- fromJSON(content)
return(list(
num_found = parsed$response$numFound,
identifiers = parsed$response$docs$identifier
))
} else {
stop("Failed to retrieve results from Internet Archive")
}
# Define the sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4",
"sim_newsweek-us_1954-02-01_43_5",
"sim_newsweek-us_1954-03-29_43_13",
"sim_newsweek-us_1954-04-19_43_16",
"sim_newsweek-us_1954-05-17_43_20",
"sim_newsweek-us_1954-06-14_43_24",
"sim_newsweek-us_1954-07-05_44_1",
"sim_newsweek-us_1954-08-09_44_6",
"sim_newsweek-us_1954-09-13_44_11",
"sim_newsweek-us_1954-10-25_44_17",
"sim_newsweek-us_1954-11-08_44_19",
"sim_newsweek-us_1954-12-27_44_26"
)
# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_54')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
for (attempt in 1:retries) {
tryCatch({
ia_download(identifier, files = "*.pdf", destdir = destdir)
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Perform the download for the sample items
walk(sample, ~download_item(.x, download_directory))
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
file_path <- file.path(destdir, paste0(identifier, ".pdf"))
for (attempt in 1:retries) {
tryCatch({
GET(url, write_disk(file_path, overwrite = TRUE))
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Define the sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4",
"sim_newsweek-us_1954-02-01_43_5",
"sim_newsweek-us_1954-03-29_43_13",
"sim_newsweek-us_1954-04-19_43_16",
"sim_newsweek-us_1954-05-17_43_20",
"sim_newsweek-us_1954-06-14_43_24",
"sim_newsweek-us_1954-07-05_44_1",
"sim_newsweek-us_1954-08-09_44_6",
"sim_newsweek-us_1954-09-13_44_11",
"sim_newsweek-us_1954-10-25_44_17",
"sim_newsweek-us_1954-11-08_44_19",
"sim_newsweek-us_1954-12-27_44_26"
)
# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_54')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
# Perform the download for the sample items
walk(sample, ~download_item(.x, download_directory))
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "/Users/robwells/Code/Moley/Newsweek_54"
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
str_detect(text, regex("Moley", ignore_case = FALSE))  |
str_detect(text, regex("by Raymond", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- paste0("Moley_column_", basename(pdf_path))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_48_68_results")
newsweek1958 <- newsweek %>%
filter(str_detect(identifier, "1958"))
newsweek1958_clean <- gsub("c\\(\"|\"\\)|\\n", "", newsweek1958)
x_clean <- newsweek1958_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)
head(x_list)
results <- x_list
# Create a function to generate a stratified sample
stratified_sample_generator <- function(issue_list) {
library(dplyr)
library(stringr)
# Convert issue_list to a data frame
issues_df <- data.frame(issue = issue_list, stringsAsFactors = FALSE)
# Extract year and month
issues_df <- issues_df %>%
mutate(date_part = str_extract(issue, "\\d{4}-\\d{2}"),
month = str_sub(date_part, 1, 7))
# Group by month and sample one issue per month
stratified_sample <- issues_df %>%
group_by(month) %>%
sample_n(1) %>%
pull(issue)
return(stratified_sample)
}
# Generate the stratified sample
sample <- stratified_sample_generator(results)
print(sample)
# Clean the text
cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", sample)
cleaned_text <- gsub("\n\\s+", "\n", cleaned_text)
items <- unlist(strsplit(cleaned_text, "\\r?\\n"))
# Enclose each string in quotes and add a comma
formatted_items <- paste0("\"", items, "\",")
# Print each formatted item
for (item in formatted_items) {
cat(item, "\n")
}
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
file_path <- file.path(destdir, paste0(identifier, ".pdf"))
for (attempt in 1:retries) {
tryCatch({
GET(url, write_disk(file_path, overwrite = TRUE))
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Define the sample
sample <- c(
"sim_newsweek-us_1958-01-27_51_4",
"sim_newsweek-us_1958-02-17_51_7",
"sim_newsweek-us_1958-03-24_51_12",
"sim_newsweek-us_1958-04-07_51_14",
"sim_newsweek-us_1958-05-12_51_19",
"sim_newsweek-us_1958-06-16_51_24",
"sim_newsweek-us_1958-07-21_52_3",
"sim_newsweek-us_1958-08-25_52_8",
"sim_newsweek-us_1958-09-22_52_12",
"sim_newsweek-us_1958-10-13_52_15",
"sim_newsweek-us_1958-11-17_52_20",
"sim_newsweek-us_1958-12-08_52_23"
)
# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_58')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
# Perform the download for the sample items
walk(sample, ~download_item(.x, download_directory))
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "/Users/robwells/Code/Moley/Newsweek_58"
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
str_detect(text, regex("Moley", ignore_case = FALSE))  |
str_detect(text, regex("by Raymond", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- paste0("Moley_column_", basename(pdf_path))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
