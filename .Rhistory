# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
str_detect(text, regex("Patent Office", ignore_case = FALSE)) &
str_detect(text, regex("by Raymond Moley", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extractedperspective", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
str_detect(text, regex("Patent Office", ignore_case = FALSE)) &
str_detect(text, regex("by Raymond Moley", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extractedperspective", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
str_detect(text, regex("by Raymond Moley", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extractedperspective", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extractedperspective", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# # Find page with both "perspective" and "Moley"
# page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
#                  str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# # Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# # Find page with both "perspective" and "Moley"
# page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
#                  str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# # Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"
# Create output directory if it doesn't exist
#dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) |
str_detect(text, regex("\\bby\\s+Raymond\\s+Moley\\b", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
View(process_identifiers)
library(googlesheets4)
googlesheets4::gs4_deauth()
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
View(index)
sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
sample <- read_csv("newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
library(janitor)
sample <- read_csv("newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
library(tidyverse)
sample <- read_csv("newsweek_sample_1953_55_57_59.csv") |>
rename(list = 2, index = 1)
#filtering
sample <- sample |>
filter(str_detect(list,"_1959")) |>
mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(date2 = lubridate::ymd(date2))
View(sample)
View(index)
index <- read.csv("test_1951_moley_articles.csv")
# Split the index string into individual lines
index_lines <- str_split(index, ",\\s*")[[1]]
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
index <- read.csv("test_1951_moley_articles.csv")
# Split the index string into individual lines
index_lines <- str_split(index, ",\\s*")[[1]]
# Initialize an empty data frame with consistent data types
df <- data.frame(year = character(), title = character(), date = character(), volume = numeric(), stringsAsFactors = FALSE)
# Define a function to parse a line
parse_line <- function(line) {
# Extract the title
title <- str_trim(str_extract(line, "^[^\\.]+"))
# Extract the date and volume
date_volume <- str_extract(line, "[A-Za-z]+\\s[0-9]+:\\s*[0-9]+")
date <- str_trim(str_extract(date_volume, "^[A-Za-z]+\\s[0-9]+"))
volume <- as.numeric(str_extract(date_volume, "[0-9]+$"))
year <- "1967"
return(data.frame(year = year, title = title, date = date, volume = volume, stringsAsFactors = FALSE))
}
# Parse each line and append to the data frame
for (line in index_lines) {
# Only process non-empty lines
if (nchar(line) > 0 && !is.na(line)) {
parsed_line <- parse_line(line)
# Append the parsed line to the data frame if it contains valid data
if (!is.na(parsed_line$title) && !is.na(parsed_line$date) && !is.na(parsed_line$volume)) {
df <- bind_rows(df, parsed_line)
}
View(df)
index_lines
View(sample)
View(index)
index <- read.csv("test_1951_moley_articles.csv")
# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
# Create the `date2` column
index <- index %>%
mutate(
# Extract month abbreviation and replace using lookup
month = str_extract(date, "[A-Za-z]+"),
day = str_extract(date, "\\d+"),
month_num = month_lookup[month],
# Format `date2` as "YYYY-MM-DD_page"
date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |>
mutate(date2 = lubridate::ymd(date2)) |>
mutate(page = str_squish(page)) |>
mutate(real_page = as.numeric(page)) |>
mutate(real_page = (real_page + 1))
index <- index %>%
mutate(month_num = month_lookup[month])
index <- index %>%
mutate(month_num = month_lookup[month],
year= "1951",
date2 = sprintf("%s-%s-%02d", year, month_num, as.integer(day)))
glimpse(index)
index <- index %>%
mutate(month_num = month_lookup[month],
year= "1951",
date2 = sprintf("%s-%s-%02d", year, month_num, as.integer(day)),
date = lubridate::ymd(date2),
page = str_squish(page),
real_page = as.numeric(page),
real_page = (real_page + 1))
index <- index %>%
mutate(month_num = month_lookup[month],
year= "1951",
date2 = sprintf("%s-%s-%02d", year, month_num, as.integer(day)),
date = lubridate::ymd(date2),
page = str_squish(page),
real_page = as.numeric(page),
real_page = (real_page + 1))
index <- read.csv("test_1951_moley_articles.csv")
# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
"My" = "05", "Je" = "06", "Jl" = "07", "Ag" = "08",
"S" = "09", "O" = "10", "N" = "11", "D" = "12")
# Create the `date2` column
index <- index %>%
mutate(month_num = month_lookup[month],
year= "1951",
date2 = sprintf("%s-%s-%02d", year, month_num, as.integer(day)),
date = lubridate::ymd(date2),
page = str_squish(page),
real_page = as.numeric(page),
real_page = (real_page + 1))
View(sample)
library(googlesheets4)
googlesheets4::gs4_deauth()
main_index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
#replace question marks for column splitting
main_index <- main_index |>
mutate(Text = str_replace_all(Text, "\\?", "."))
View(main_index)
names(main_index)
main_index <- main_index |>
mutate(Text = str_replace_all(Issue, "\\?", "."))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = lubridate::ymd(new_date2))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = lubridate::ymd(new_date))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date = lubridate::ymd(new_date))
main_index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
#replace question marks for column splitting
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date = lubridate::ymd(new_date))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", "")),
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}"),
new_date = lubridate::ymd(new_date))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}"),
new_date1 = lubridate::ymd(new_date1))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}"),
new_date1 = lubridate::ymd(new_date1)) |>
mutate(new_date1 = ifelse(is.na(new_date1), new_date, new_date1))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}"),
new_date1 = lubridate::ymd(new_date1)) |>
mutate(new_date2 = new_date1, new_date)
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(new_date2 = ifelse(is.na(new_date1), new_date, new_date1)) |>
mutate(new_date2 = lubridate::ymd(new_date2))
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(new_date2 = ifelse(is.na(new_date1), new_date, new_date1)) |>
mutate(new_date = lubridate::ymd(new_date2))
glimpse(main_index)
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(new_date2 = ifelse(is.na(new_date1), new_date, new_date1)) |>
mutate(new_date = lubridate::ymd(new_date2))  |>
select(-new_date2, -new_date1)
glimpse(index)
index1 <- index |>
inner_join(main_index, by=c("date"="new_date"))
View(index1)
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/", Issue, "/","page","/n",real_page,"/","mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/", Issue "/","page","/n",real_page,"/","mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/",Issue,"/","page","/n",real_page,"/","mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/", Issue, "/page/n", real_page, "/mode/2up"))
View(index1)
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(URL = paste0("https://archive.org/details/", str_trim(Issue), "/page/n", real_page, "/mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(Issue = str_replace(Issue, ",$", "")) |>
mutate(URL = paste0("https://archive.org/details/", str_trim(Issue), "/page/", real_page, "/mode/2up"))
index1 <- index |>
inner_join(main_index, by=c("date"="new_date")) |>
mutate(Issue = str_replace(Issue, ",$", "")) |>
mutate(URL = paste0("https://archive.org/details/", str_trim(Issue), "/page/", page, "/mode/2up"))
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
library(googlesheets4)
googlesheets4::gs4_deauth()
main_index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")
#replace question marks for column splitting
main_index <- main_index |>
mutate(new_date = str_replace_all(Issue, "sim_newsweek-us_", ""),
new_date1 = str_extract(new_date, "\\d{4}-\\d{2}-\\d{2}")) |>
mutate(new_date2 = ifelse(is.na(new_date1), new_date, new_date1)) |>
mutate(new_date = lubridate::ymd(new_date2))  |>
select(-new_date2, -new_date1)
View(main_index)
