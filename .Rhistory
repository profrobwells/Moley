if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Extract title from the cover page
title_text <- pdf_text[i]
# Extract everything before "ABSTRACT"
title <- str_extract(title_text, "^.*?(?=\\n.*?ABSTRACT)")
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(title != "") {
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- if(article_info[[i]]$title != "") {
paste0(article_info[[i]]$title, ".pdf")
} else {
paste0("article_", i, ".pdf")
}
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "perspective_test.pdf"
split_newsweek_articles(input_pdf, "split_articles")
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Extract title from the cover page
title_text <- pdf_text[i]
# Extract title (everything up to the first occurrence of "ProQuest")
title <- str_extract(title_text, "^.*?(?=\\n.*?ProQuest)")
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
# Check if title is not NULL and not empty
if(!is.null(title) && nchar(title) > 0) {
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- if(nchar(article_info[[i]]$title) > 0) {
paste0(article_info[[i]]$title, ".pdf")
} else {
paste0("article_", i, ".pdf")
}
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "perspective_test.pdf"
split_newsweek_articles(input_pdf, "split_articles")
# Install and load required packages
# Function to process PDF and split articles
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages to identify article boundaries
pdf_text <- pdf_text(input_pdf)
# Get total number of pages
total_pages <- length(pdf_text)
# Initialize variables
article_starts <- c()
article_titles <- c()
current_page <- 1
# Find article start pages and titles
while(current_page <= total_pages) {
if(grepl("ABSTRACT", pdf_text[current_page], fixed = TRUE)) {
article_starts <- c(article_starts, current_page)
# Extract title (appears before ABSTRACT)
title <- str_extract(pdf_text[current_page], "(?<=\\n).*?(?=\\n.*ABSTRACT)")
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
article_titles <- c(article_titles, title)
}
current_page <- current_page + 1
}
# Add the end page for the last article
article_starts <- c(article_starts, total_pages + 1)
# Split PDF into articles
for(i in 1:(length(article_starts)-1)) {
start_page <- article_starts[i]
end_page <- article_starts[i+1] - 1
# Create filename
if(is.na(article_titles[i]) || article_titles[i] == "") {
filename <- paste0("article_", i, ".pdf")
} else {
filename <- paste0(article_titles[i], ".pdf")
}
# Create full path
filepath <- file.path(output_dir, filename)
# Extract pages for this article
pdf_subset(
input_pdf,
filepath,
pages = start_page:end_page
)
cat("Saved article:", filename, "\n")
}
# Usage example:
input_pdf <- "perspective_test.pdf"
split_newsweek_articles(input_pdf, "split_articles")
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
# input_pdf <- "path/to/your/newsweek.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
# Usage example:
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "'/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/Perspective all issues/perspective 1-400 to 1967.pdf'"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/Perspective all issues/perspective 1-400 to 1967.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "perspective_78_400.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
# Usage example:
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "perspective_256_400.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
# Create output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir)
}
# Read PDF text by pages
pdf_text <- pdf_text(input_pdf)
total_pages <- length(pdf_text)
# Initialize variables
article_info <- list()
# Function to safely extract and clean title
extract_title <- function(text) {
# Look for text that appears between the start of the page and "Moley, Raymond"
title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
if(is.na(title)) return("untitled")
# Clean the title
title <- str_trim(title)
title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
title <- str_replace_all(title, "\\s+", "_")
if(nchar(title) == 0) return("untitled")
return(title)
}
# Process pages in groups of 3
for(i in seq(1, total_pages, by = 3)) {
# Check if we have enough pages left for a complete article
if(i + 2 <= total_pages) {
# Get the title
title <- extract_title(pdf_text[i])
# Add article info
article_info[[length(article_info) + 1]] <- list(
title = title,
article_page = i + 1  # The actual article is the second page in each group
)
cat("Found article:", title, "on page", i + 1, "\n")
}
# Extract each article
for(i in seq_along(article_info)) {
# Create filename
filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
# Create full path
filepath <- file.path(output_dir, filename)
# Extract just the article page
tryCatch({
pdf_subset(
input_pdf,
filepath,
pages = article_info[[i]]$article_page
)
cat("Saved article:", filename, "\n")
}, error = function(e) {
cat("Error saving", filename, ":", conditionMessage(e), "\n")
})
}
cat("Processed", length(article_info), "articles\n")
}
# Usage example:
input_pdf <- "perspective_256_400.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
# Usage example:
