processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
#install.packages("formattable")
articles_decades <- textdata %>%
distinct(doc_id, .keep_all=TRUE) %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")
#install.packages("rio")
#install.packages("readtext")
install.packages("formattable")
#install.packages("formattable")
articles_decades <- textdata %>%
distinct(doc_id, .keep_all=TRUE) %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=9,589, 10/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 9589 articles
#sum(articles_decades$n)
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
View(vizDataFrame)
topics <- topicNames |>
as.data.frame()
View(topics)
topics <- topicNames |>
as.data.frame() |>
mutate(number <- rownames(topicNames))
topicNames$number <- rownames(topicNames)
topics <- topicNames |>
as.data.frame() |>
mutate(number <- rownames(topics))
topics$number <- rownames(topics)
View(topics)
topics <- topicNames |>
as.data.frame() |>
mutate(number <- rownames())
topics <- topicNames |>
as.data.frame() |>
mutate(number <- rownames(topicNames))
topics <- topicNames |>
as.data.frame() |>
mutate(number = row_number())
View(topics)
topics <- topicNames |>
as.data.frame()
View(topics)
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
topics <- topicNames |>
as.data.frame()
View(topics)
View(topicNames)
topicNames
topics <- enframe(topicNames, name = "number", value = "text") %>%
mutate(number = as.character(number))  # Ensure number is character
View(topics)
topics$text
topics
topics <- enframe(topicNames, name = "number", value = "text") %>%
unnest(cols = c(text))
topics
theta2 <- as.data.frame(theta)
View(theta2)
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1)
View(topic1)
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1) |>
mutate(file = row_number())
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1) |>
mutate(file = rowname())
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1) |>
mutate(file = row_name())
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1) |>
mutate(file = rownames())
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(topic1) |>
mutate(file = rownames(theta2))
topic1 <- theta2 %>%
#renaming for a general topic
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1))
View(topic1)
theta2
topic1 <- theta2 %>%
#renaming for a general topic
rownames_to_column(var = "file") %>%
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(file, topic1)
topic1 <- theta2 %>%
#renaming for a general topic
rownames_to_column(var = "file") %>%
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(file, topic1) |>
mutate(file = str_detect_all(file, "X", ""))
topic1 <- theta2 %>%
#renaming for a general topic
rownames_to_column(var = "file") %>%
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(file, topic1) |>
mutate(file = str_remove(file, "^X"))
topic1 <- theta2 %>%
#renaming for a general topic
rownames_to_column(var = "file") %>%
mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
line = str_extract(file, "(?<=\\.txt)\\.\\d+")) %>%  # Extract number after .txt
mutate(file = str_remove(file, "\\.\\d+$")) %>%
rename(topic1 = '1') %>%
top_n(20, topic1) %>%
arrange(desc(topic1)) %>%
select(file, line, topic1)
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_37_61")
file <- newsweek %>%
mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
#filter(year >= 1938 & year <= 1941) %>%
filter(year == 1947 & year >= 1949 & year >= 1951) %>%
# Filter on year range
select(-year)
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
newsweek_47_49_51_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
View(file)
file <- newsweek %>%
mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
#filter(year >= 1938 & year <= 1941) %>%
filter(year == 1947 | year >= 1949 & year >= 1951) %>%
# Filter on year range
select(-year)
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
newsweek_47_49_51_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
View(file)
file <- newsweek %>%
mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
#filter(year >= 1938 & year <= 1941) %>%
filter(year == 1947 | year == 1949 | year == 1950 | year == 1951) %>%
# Filter on year range
select(-year)
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
newsweek_47_49_51_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
View(file)
file <- newsweek %>%
mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
#filter(year >= 1938 & year <= 1941) %>%
filter(year == 1947 | year == 1949 | year == 1950 | year == 1951) %>%
# Filter on year range
select(-year)
# newsweek1962 <- all_results %>%
#   filter(str_detect(identifier, "1962"))
file_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
x_clean <- file_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)
head(x_list)
results <- x_list
# Create a function to generate a stratified sample
stratified_sample_generator <- function(issue_list) {
library(dplyr)
library(stringr)
# Convert issue_list to a data frame
issues_df <- data.frame(issue = issue_list, stringsAsFactors = FALSE)
# Extract year and month
issues_df <- issues_df %>%
mutate(date_part = str_extract(issue, "\\d{4}-\\d{2}"),
month = str_sub(date_part, 1, 7))
# Group by month and sample one issue per month
stratified_sample <- issues_df %>%
group_by(month) %>%
sample_n(1) %>%
pull(issue)
return(stratified_sample)
}
# Generate the stratified sample
sample <- stratified_sample_generator(results)
print(sample)
sample_df <- sample %>%
as.data.frame()
#write.csv(sample_df, "newsweek_sample_1943-45.csv")
getwd()
setwd("~/Code/Moley")
write.csv(sample_df, "\moley_newsweek\newsweek_sample_1947_49_50_51.csv")
write.csv(sample_df, "moley_newsweek\newsweek_sample_1947_49_50_51.csv")
write.csv(sample_df, "~\moley_newsweek\newsweek_sample_1947_49_50_51.csv")
write.csv(sample_df, "~/Code/Moley/moley_newsweek/newsweek_sample_1947_49_50_51.csv")
# Clean the text
cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", sample)
cleaned_text <- gsub("\n\\s+", "\n", cleaned_text)
items <- unlist(strsplit(cleaned_text, "\\r?\\n"))
# Enclose each string in quotes and add a comma
formatted_items <- paste0("\"", items, "\",")
# Print each formatted item
for (item in formatted_items) {
cat(item, "\n")
}
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
file_path <- file.path(destdir, paste0(identifier, ".pdf"))
for (attempt in 1:retries) {
tryCatch({
GET(url, write_disk(file_path, overwrite = TRUE))
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Define the sample
# sample <- c(
#  "sim_newsweek-us_1964_63_index",
#  "sim_newsweek-us_1964_64_index" ,
#  "sim_newsweek-us_1965_65_index" ,
#  "sim_newsweek-us_1965_66_index" ,
#  "sim_newsweek-us_1966_67_index"  ,
#  "sim_newsweek-us_1966_68_index" ,
#  "sim_newsweek-us_january-1-june-24-1968_71_index" ,
#  "sim_newsweek-us_january-2-june-1967_69_index",
#  "sim_newsweek-us_july-1-december-30-1968_72_index",
#  "sim_newsweek-us_july-3-december-25-1967_70_index"
# )
# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_47_49_50_51')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
# Perform the download for the sample items
walk(sample, ~download_item(.x, download_directory))
library(pdftools)
library(stringr)
library(fs)
# Specify the folder containing the PDF files
pdf_folder <- "~/Code/Moley/Newsweek_47_49_50_51"
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")
# Function to extract perspective page
extract_perspective <- function(pdf_path) {
tryCatch({
# Extract all text
text <- pdf_text(pdf_path)
# Find page with both "perspective" and "Moley"
#In case it is case sensitive, change PERSPECTIVE and MOLEY
page_num <- which(str_detect(text, regex("PERSPECTIVE", ignore_case = FALSE)) |
str_detect(text, regex("MOLEY", ignore_case = FALSE))  |
str_detect(text, regex("by Raymond", ignore_case = FALSE)))
# Extract the specific page
if (length(page_num) > 0) {
output_file <- paste0("Moley_column_", basename(pdf_path))
pdf_subset(pdf_path, pages = page_num, output = output_file)
return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
} else {
return(paste("Perspective not found in", basename(pdf_path)))
}
}, error = function(e) {
return(paste("Error processing", basename(pdf_path), ":", e$message))
})
}
# Process each PDF file
results <- sapply(pdf_files, extract_perspective)
# Print results
for (result in results) {
cat(result, "\n")
}
View(sample_df)
sample_df |>
str_detect(., "1951"))
sample_df |>
filter(grepl("1951",.))
sample_1951<- sample_df |>
filter(grepl("1951",.))
View(sample_1951)
# Clean the text
cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", sample_1951)
cleaned_text <- gsub("\n\\s+", "\n", cleaned_text)
items <- unlist(strsplit(cleaned_text, "\\r?\\n"))
# Enclose each string in quotes and add a comma
formatted_items <- paste0("\"", items, "\",")
# Print each formatted item
for (item in formatted_items) {
cat(item, "\n")
}
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)
# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
file_path <- file.path(destdir, paste0(identifier, ".pdf"))
for (attempt in 1:retries) {
tryCatch({
GET(url, write_disk(file_path, overwrite = TRUE))
print(paste("Downloaded", identifier, "to", destdir))
break
}, error = function(e) {
if (attempt < retries) {
print(paste("Error occurred, retrying...", attempt, "/", retries))
} else {
print(paste("Failed to download", identifier, "after", retries, "attempts"))
}
})
}
# Define the sample
# sample <- c(
#  "sim_newsweek-us_1964_63_index",
#  "sim_newsweek-us_1964_64_index" ,
#  "sim_newsweek-us_1965_65_index" ,
#  "sim_newsweek-us_1965_66_index" ,
#  "sim_newsweek-us_1966_67_index"  ,
#  "sim_newsweek-us_1966_68_index" ,
#  "sim_newsweek-us_january-1-june-24-1968_71_index" ,
#  "sim_newsweek-us_january-2-june-1967_69_index",
#  "sim_newsweek-us_july-1-december-30-1968_72_index",
#  "sim_newsweek-us_july-3-december-25-1967_70_index"
# )
# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_1951')
if (!dir.exists(download_directory)) {
dir.create(download_directory, recursive = TRUE)
}
# Perform the download for the sample items
walk(sample_1951, ~download_item(.x, download_directory))
