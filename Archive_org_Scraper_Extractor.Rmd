---
name: Archive.org scraper, extractor
output:
  html_document: default
  pdf_document: default
---


# Scrape Archive.org for Newsweek back issues


```{r}
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
```

#Import spreadsheet of Newsweek back issues
```{r}
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_37_61")

newsweek1960s <- newsweek %>% 
  filter(str_detect(identifier, "1960$")) 


newsweek1960s <- newsweek %>%
  mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
  filter(year >= 1962 & year <= 1969) %>%                             # Filter on year range
  select(-year)       

# newsweek1962 <- all_results %>% 
#   filter(str_detect(identifier, "1962")) 

newsweek1946_clean <- gsub("c\\(\"|\"\\)|\\n", "", newsweek1946)

x_clean <- newsweek1946_clean


# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))

x_list <- gsub("\\\"", "", x_list)

head(x_list)


results <- x_list

```



#Stratified sample generator in R

```{r}

# Create a function to generate a stratified sample
stratified_sample_generator <- function(issue_list) {
  library(dplyr)
  library(stringr)
  
  # Convert issue_list to a data frame
  issues_df <- data.frame(issue = issue_list, stringsAsFactors = FALSE)
  
  # Extract year and month
  issues_df <- issues_df %>%
    mutate(date_part = str_extract(issue, "\\d{4}-\\d{2}"),
           month = str_sub(date_part, 1, 7))
  
  # Group by month and sample one issue per month
  stratified_sample <- issues_df %>%
    group_by(month) %>%
    sample_n(1) %>%
    pull(issue)
  
  return(stratified_sample)
}

# Generate the stratified sample
sample <- stratified_sample_generator(results)

print(sample)

```




```{r}
# Clean the text
cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", sample)
cleaned_text <- gsub("\n\\s+", "\n", cleaned_text)
items <- unlist(strsplit(cleaned_text, "\\r?\\n"))


# Enclose each string in quotes and add a comma
formatted_items <- paste0("\"", items, "\",")

# Print each formatted item
for (item in formatted_items) {
  cat(item, "\n")
}
```

#Article extractor
```{r}
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)


# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
  url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
  file_path <- file.path(destdir, paste0(identifier, ".pdf"))
  
  for (attempt in 1:retries) {
    tryCatch({
      GET(url, write_disk(file_path, overwrite = TRUE))
      print(paste("Downloaded", identifier, "to", destdir))
      break
    }, error = function(e) {
      if (attempt < retries) {
        print(paste("Error occurred, retrying...", attempt, "/", retries))
      } else {
        print(paste("Failed to download", identifier, "after", retries, "attempts"))
      }
    })
  }
}

# Define the sample
sample <- c(
  "sim_newsweek-us_1962_59_index",                  
 "sim_newsweek-us_1962_60_index",                   
 "sim_newsweek-us_1963_61_index" ,                  
 "sim_newsweek-us_1963_62_index",                   
 "sim_newsweek-us_1964_63_index",                   
 "sim_newsweek-us_1964_64_index" ,                  
 "sim_newsweek-us_1965_65_index" ,                  
 "sim_newsweek-us_1965_66_index" ,                  
 "sim_newsweek-us_1966_67_index"  ,                 
 "sim_newsweek-us_1966_68_index" ,                  
 "sim_newsweek-us_january-1-june-24-1968_71_index" ,
 "sim_newsweek-us_january-2-june-1967_69_index",    
 "sim_newsweek-us_july-1-december-30-1968_72_index",
 "sim_newsweek-us_july-3-december-25-1967_70_index"  
)

# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_1960_indexes')
if (!dir.exists(download_directory)) {
  dir.create(download_directory, recursive = TRUE)
}

# Perform the download for the sample items
walk(sample, ~download_item(.x, download_directory))
```


# Extracts the Moley column from a folder of pdfs

```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "/Users/robwells/Code/Moley/Newsweek_46"

# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Extract all text
    text <- pdf_text(pdf_path)
    
    # Find page with both "perspective" and "Moley"
    #In case it is case sensitive, change PERSPECTIVE and MOLEY
    page_num <- which(str_detect(text, regex("PERSPECTIVE", ignore_case = FALSE)) |
                      str_detect(text, regex("MOLEY", ignore_case = FALSE))  |
                      str_detect(text, regex("by Raymond", ignore_case = FALSE)))
    
    # Extract the specific page
    if (length(page_num) > 0) {
      output_file <- paste0("Moley_column_", basename(pdf_path))
      pdf_subset(pdf_path, pages = page_num, output = output_file)
      return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
    } else {
      return(paste("Perspective not found in", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file
results <- sapply(pdf_files, extract_perspective)

# Print results
for (result in results) {
  cat(result, "\n")
}
```


# NOTES
```{r}
Here's a quick recap of what we did to solve the problem:

We replaced the ia_download function with a custom download_item function.
We used httr::GET to download the PDF files directly from the Internet Archive.
We constructed the URL for each PDF based on the item identifier.
We maintained the retry mechanism for robustness.

This solution gives you more control over the download process and eliminates dependencies on potentially problematic packages.

```



```{r}
# Provided list of issues

results <- c(
"sim_newsweek-us_1952-01-07_39_1",
"sim_newsweek-us_1952-01-14_39_2",
"sim_newsweek-us_1952-01-21_39_3",
"sim_newsweek-us_1952-01-28_39_4",
"sim_newsweek-us_1952-02-04_39_5",
"sim_newsweek-us_1952-02-11_39_6",
"sim_newsweek-us_1952-02-18_39_7",
"sim_newsweek-us_1952-02-25_39_8",
"sim_newsweek-us_1952-03-03_39_9",
"sim_newsweek-us_1952-03-10_39_10",
"sim_newsweek-us_1952-03-17_39_11",
"sim_newsweek-us_1952-03-24_39_12",
"sim_newsweek-us_1952-03-31_39_13",
"sim_newsweek-us_1952-04-07_39_14",
"sim_newsweek-us_1952-04-14_39_15",
"sim_newsweek-us_1952-04-21_39_16",
"sim_newsweek-us_1952-04-28_39_17",
"sim_newsweek-us_1952-05-05_39_18",
"sim_newsweek-us_1952-05-12_39_19",
"sim_newsweek-us_1952-05-19_39_20",
"sim_newsweek-us_1952-05-26_39_21",
"sim_newsweek-us_1952-06-02_39_22",
"sim_newsweek-us_1952-06-09_39_23",
"sim_newsweek-us_1952-06-16_39_24",
"sim_newsweek-us_1952-06-23_39_25",
"sim_newsweek-us_1952-06-30_39_26",
"sim_newsweek-us_1952-07-07_40_1",
"sim_newsweek-us_1952-07-14_40_2",
"sim_newsweek-us_1952-07-21_40_3",
"sim_newsweek-us_1952-07-28_40_4",
"sim_newsweek-us_1952-08-04_40_5",
"sim_newsweek-us_1952-08-11_40_6",
"sim_newsweek-us_1952-08-18_40_7",
"sim_newsweek-us_1952-08-25_40_8",
"sim_newsweek-us_1952-09-01_40_9",
"sim_newsweek-us_1952-09-08_40_10",
"sim_newsweek-us_1952-09-15_40_11",
"sim_newsweek-us_1952-09-22_40_12",
"sim_newsweek-us_1952-09-29_40_13",
"sim_newsweek-us_1952-10-06_40_14",
"sim_newsweek-us_1952-10-13_40_15",
"sim_newsweek-us_1952-10-20_40_16",
"sim_newsweek-us_1952-10-27_40_17",
"sim_newsweek-us_1952-11-03_40_18",
"sim_newsweek-us_1952-11-10_40_19",
"sim_newsweek-us_1952-11-17_40_21",
"sim_newsweek-us_1952-11-24_40_22",
"sim_newsweek-us_1952-12-01_40_23",
"sim_newsweek-us_1952-12-08_40_24",
"sim_newsweek-us_1952-12-15_40_25",
"sim_newsweek-us_1952-12-22_40_26",
"sim_newsweek-us_1952-12-29_40_27"
)
```


# Past Samples
```{}

#1958 sample
sample <- c(
"sim_newsweek-us_1958-01-27_51_4", 
"sim_newsweek-us_1958-02-17_51_7", 
"sim_newsweek-us_1958-03-24_51_12", 
"sim_newsweek-us_1958-04-07_51_14", 
"sim_newsweek-us_1958-05-12_51_19", 
"sim_newsweek-us_1958-06-16_51_24", 
"sim_newsweek-us_1958-07-21_52_3", 
"sim_newsweek-us_1958-08-25_52_8", 
"sim_newsweek-us_1958-09-22_52_12", 
"sim_newsweek-us_1958-10-13_52_15", 
"sim_newsweek-us_1958-11-17_52_20", 
"sim_newsweek-us_1958-12-08_52_23"
)

#1954 sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4", 
"sim_newsweek-us_1954-02-01_43_5", 
"sim_newsweek-us_1954-03-29_43_13", 
"sim_newsweek-us_1954-04-19_43_16", 
"sim_newsweek-us_1954-05-17_43_20", 
"sim_newsweek-us_1954-06-14_43_24", 
"sim_newsweek-us_1954-07-05_44_1", 
"sim_newsweek-us_1954-08-09_44_6", 
"sim_newsweek-us_1954-09-13_44_11", 
"sim_newsweek-us_1954-10-25_44_17", 
"sim_newsweek-us_1954-11-08_44_19", 
"sim_newsweek-us_1954-12-27_44_26"
)

#1952 sample
"sim_newsweek-us_1952-01-21_39_3", 
"sim_newsweek-us_1952-02-18_39_7" ,
"sim_newsweek-us_1952-03-03_39_9", 
"sim_newsweek-us_1952-04-14_39_15",
"sim_newsweek-us_1952-05-19_39_20",
"sim_newsweek-us_1952-06-23_39_25",
"sim_newsweek-us_1952-07-21_40_3", 
"sim_newsweek-us_1952-08-18_40_7", 
"sim_newsweek-us_1952-09-22_40_12",
"sim_newsweek-us_1952-10-27_40_17",
"sim_newsweek-us_1952-11-24_40_22",
"sim_newsweek-us_1952-12-29_40_27"

#1948 sample
"sim_newsweek-us_1948-01-26_31_4" 
"sim_newsweek-us_1948-02-09_31_6" 
"sim_newsweek-us_1948-03-01_31_9" 
"sim_newsweek-us_1948-04-19_31_16"
"sim_newsweek-us_1948-05-10_31_19"
"sim_newsweek-us_1948-06-07_31_23"
"sim_newsweek-us_1948-07-26_32_4" 
"sim_newsweek-us_1948-08-09_32_6" 
"sim_newsweek-us_1948-09-06_32_10"
"sim_newsweek-us_1948-10-18_32_16"
"sim_newsweek-us_1948-11-15_32_20"
"sim_newsweek-us_1948-12-06_32_23"


#1946 sample
"sim_newsweek-us_1946-01-21_27_3", 
"sim_newsweek-us_1946-02-04_27_5", 
"sim_newsweek-us_1946-03-11_27_10", 
"sim_newsweek-us_1946-04-29_27_17", 
"sim_newsweek-us_1946-05-13_27_19", 
"sim_newsweek-us_1946-06-24_27_25", 
"sim_newsweek-us_1946-07-08_28_2", 
"sim_newsweek-us_1946-08-19_28_8", 
"sim_newsweek-us_1946-09-16_28_12", 
"sim_newsweek-us_1946-10-14_28_16", 
"sim_newsweek-us_1946-11-11_28_20", 
"sim_newsweek-us_1946-12-09_28_24"

#1942 sample
  
  
"sim_newsweek-us_1942-01-19_19_3", 
"sim_newsweek-us_1942-02-02_19_5", 
"sim_newsweek-us_1942-03-23_19_12", 
"sim_newsweek-us_1942-04-20_19_16", 
"sim_newsweek-us_1942-05-11_19_19", 
"sim_newsweek-us_1942-06-01_19_22", 
"sim_newsweek-us_1942-07-06_20_1", 
"sim_newsweek-us_1942-08-24_20_8", 
"sim_newsweek-us_1942-09-28_20_13", 
"sim_newsweek-us_1942-10-19_20_16", 
"sim_newsweek-us_1942-11-16_20_20", 
"sim_newsweek-us_1942-12-07_20_23"

 1937 sample
 sample = [
    "sim_newsweek-us_1937-01-09_9_2",
    "sim_newsweek-us_1937-02-27_9_9",
    "sim_newsweek-us_1937-03-13_9_11",
    "sim_newsweek-us_1937-04-10_9_15",
    "sim_newsweek-us_1937-05-22_9_21",
    "sim_newsweek-us_1937-06-05_9_23",
    "sim_newsweek-us_1937-07-31_10_5",
    "sim_newsweek-us_1937-08-14_10_7",
    "sim_newsweek-us_1937-09-06_10_10",
    "sim_newsweek-us_1937-10-25_10_17",
    "sim_newsweek-us_1937-11-15_10_20",
    "sim_newsweek-us_1937-12-06_10_23"
]


```



```{r}
url2 <- "https://archive.org/download/sim_newsweek-us_1945-"
test  <- url2 %>%
  read_html() %>%
  html_table() 


# Specify the base URL of the webpage you want to scrape
base_url <- "https://archive.org/details/pub_newsweek-us?and%5B%5D=year%3A%221938%22&and%5B%5D=year%3A%5B1937+TO+1969%5D"



# Read the HTML code from the website
webpage <- read_html(base_url)

# Use CSS selectors to scrape the links to the files
file_urls <- webpage %>%
  html_nodes("a") %>%
  html_attr("href")

# Filter the URLs to include only those that start with 'sim_newsweek-us_'
newsweek_urls <- file_urls[str_detect(file_urls, "^sim_newsweek-us_")]

# Append the base URL to each of the Newsweek URLs
newsweek_urls <- paste0(base_url, newsweek_urls)

# Print the Newsweek URLs
print(newsweek_urls)

#Part 2 - works

# Specify the base URL
base_url <- "https://archive.org/download/sim_newsweek-us_"

# Specify the dates of the issues you're interested in
dates <- c("1948-06-28_31_26", "1948-07-05_31_27")  # Add more dates as needed

# Generate the URLs
urls <- paste0(base_url, dates, "/sim_newsweek-us_", dates, ".pdf")

# Print the URLs
print(urls)

```
#from this tutorialL https://github.com/hrbrmstr/wayback
```{r}
devtools::install_github("hrbrmstr/wayback")
```

```{r}
library(wayback)
library(tidyverse)

# current verison
packageVersion("wayback")

archive_available("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")

get_mementos("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")


newsweek_timemap <- get_timemap("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")

cdx_basic_query("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml", limit = 10) %>% 
  glimpse()

mem <- read_memento("https://www.r-project.org/news.html")
res <- stringi::stri_split_lines(mem)[[1]]
cat(paste0(res[187:200], collaspe="\n"))

glimpse(
  ia_scrape("lemon curry")
)


(newsweek<- ia_scrape("identifier:pub_newsweek-us", count=100L))

## <ia_scrape object>
## Cursor: W3siaWRlbnRpZmllciI6IjAzLTEwLTE4X1NwYWNlLXRvLUdyb3VuZHMuemlwIn1d

(item <- ia_retrieve(newsweek$identifier[1]))

write.csv(item, "/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/newsweek_data.csv")

download.file(item$link[1], file.path("~/Library/CloudStorage/Dropbox/Classes_Teaching_Archive/Data Journalism Classes/Data-Analysis-Class-Jour-405v-5003", item$file[1]))

```

```{r}

library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)

get_newsweek_issues <- function(year) {
  url <- "https://archive.org/advancedsearch.php"
  
  query <- list(
    q = paste0('collection:pub_newsweek-us AND date:[', year, ' TO ', year, ']'),
   fl = paste(c("identifier", "date", "title"), collapse = ","),
    sort = c("date asc"),
    output = "json",
    rows = 1000  # Adjust this if you need more results per year
  )
  
  response <- GET(url, query = query)
  
  if (status_code(response) != 200) {
    warning(paste("Failed to retrieve data for year", year))
    return(NULL)
  }
  
  content <- content(response, "text")
  data <- fromJSON(content)
  
  if (length(data$response$docs) == 0) {
    warning(paste("No results found for year", year))
    return(NULL)
  }
  
  results <- as_tibble(data$response$docs)
  results$year <- year
  
  Sys.sleep(1)  # Add delay to avoid rate limiting
  
  return(results)
}

# Get results for each year from 1962 to 1969
start_year <- 1962
end_year <- 1969

all_results <- map_dfr(start_year:end_year, get_newsweek_issues)

# Process results
processed_results <- all_results %>%
  mutate(date = as_date(date)) %>%
  filter(!is.na(date)) %>%
  arrange(date)

# Display the first few results
head(processed_results)

# Save results to a CSV file
write_csv(processed_results, "newsweek_issues.csv")
```


```{r}
library(httr)
library(rvest)
library(tidyverse)
library(lubridate)

get_newsweek_issues <- function(year) {
  url <- paste0("https://archive.org/search.php?query=collection%3A%28pub_newsweek-us%29%20AND%20date%3A", year, "&sort=-date")
  
  page <- read_html(url)
  
  titles <- page %>% html_nodes(".item-title") %>% html_text()
  dates <- page %>% html_nodes(".item-date") %>% html_text()
  identifiers <- page %>% html_nodes(".item-ia") %>% html_attr("data-id")
  
  if (length(titles) == 0) {
    warning(paste("No results found for year", year))
    return(NULL)
  }
  
  results <- tibble(
    title = titles,
    date = dates,
    identifier = identifiers,
    year = year,
    url = paste0("https://archive.org/details/", identifiers)
  ) %>%
    mutate(date = as_date(date)) %>%
    filter(!str_detect(identifier, "_index")) %>%
    arrange(date)
  
  Sys.sleep(2)  # Add delay to avoid rate limiting
  
  return(results)
}

# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969

all_results <- map_dfr(start_year:end_year, get_newsweek_issues)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
```

#after much difficulty... index 1962-1969

```{r}

library(tidyverse)
library(lubridate)

generate_newsweek_issues <- function(start_year, end_year) {
  all_issues <- tibble(
    date = as.Date(character()),
    identifier = character(),
    title = character(),
    year = integer(),
    url = character()
  )
  
  for (year in start_year:end_year) {
    for (month in 1:12) {
      for (day in 1:31) {
        date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
        
        if (is.na(date) || month(date) != month) next  # Skip invalid dates
        
        volume <- 59 + (year - 1962)  # Assuming volume increments each year
        issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
        
        identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
        url <- paste0("https://archive.org/details/", identifier)
        
        all_issues <- all_issues %>% 
          add_row(
            date = date,
            identifier = identifier,
            title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
            year = year,
            url = url
          )
      }
    }
  }
  
  return(all_issues)
}

# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969

all_results <- generate_newsweek_issues(start_year, end_year)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
```


```{r}
library(tidyverse)
library(lubridate)
library(httr)

generate_newsweek_issues <- function(start_year, end_year) {
  all_issues <- tibble(
    date = as.Date(character()),
    identifier = character(),
    title = character(),
    year = integer(),
    url = character()
  )
  
  for (year in start_year:end_year) {
    for (month in 1:12) {
      for (day in 1:31) {
        date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
        
        if (is.na(date) || month(date) != month) next # Skip invalid dates
        
        volume <- 59 + (year - 1962) # Assuming volume increments each year
        issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
        
        identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
        url <- paste0("https://archive.org/details/", identifier)
        
        response <- HEAD(url)
        
        if (status_code(response) == 200) {
          all_issues <- all_issues %>%
            add_row(
              date = date,
              identifier = identifier,
              title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
              year = year,
              url = url
            )
        }
      }
    }
  }
  
  return(all_issues)
}

# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1963

all_results <- generate_newsweek_issues(start_year, end_year)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")

```

