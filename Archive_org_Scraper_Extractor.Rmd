---
name: Archive.org scraper, extractor
output:
  html_document: default
  pdf_document: default
---

# Scrape Archive.org for Newsweek back issues

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(janitor)
library(stringr)
```

#Import spreadsheet of Newsweek back issues

```{r}
newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_37_61")
```

#Filter for years

```{r}
file <- newsweek %>%
  mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
  #filter(year >= 1938 & year <= 1941) %>%   
  # filter(year == 1947 | year == 1949 | year == 1950 | year == 1951) 
  filter(year == 1953 | year == 1955 | year == 1957 | year == 1959) %>%   
    # Filter on year range
  select(-year)       

# newsweek1962 <- all_results %>% 
#   filter(str_detect(identifier, "1962")) 

file_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
```


# Filter for index- Nov 1
```{r}
file <- newsweek %>%
    mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
  #filter(year >= 1938 & year <= 1941) %>%   
  # filter(year == 1947 | year == 1949 | year == 1950 | year == 1951) 
  filter(year == 1953 | year == 1955 | year == 1957 | year == 1959)# %>%  
   filter(str_detect(identifier, "index")) |> 
  select(-year)      

# newsweek1962 <- all_results %>% 
#   filter(str_detect(identifier, "1962")) 

file_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
x_clean <- file_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)

head(x_list)
results <- x_list

#This skips to line 160, extractor
```

# Dec 4 list
```{r}
file <- newsweek %>%
    mutate(year = as.numeric(str_extract(identifier, "19\\d{2}")))

file_clean <- gsub("c\\(\"|\"\\)|\\n", "", file)
x_clean <- file_clean
# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))
x_list <- gsub("\\\"", "", x_list)

head(x_list)
results <- x_list
```


#Cleaning sequence

```{r}
x_clean <- file_clean


# Split the string into a vector based on ", "
x_list <- unlist(strsplit(x_clean, ", "))

x_list <- gsub("\\\"", "", x_list)

head(x_list)


results <- x_list

```

#Stratified sample generator in R

```{r}

# Create a function to generate a stratified sample
stratified_sample_generator <- function(issue_list) {
  library(dplyr)
  library(stringr)
  
  # Convert issue_list to a data frame
  issues_df <- data.frame(issue = issue_list, stringsAsFactors = FALSE)
  
  # Extract year and month
  issues_df <- issues_df %>%
    mutate(date_part = str_extract(issue, "\\d{4}-\\d{2}"),
           month = str_sub(date_part, 1, 7))
  
  # Group by month and sample one issue per month
  stratified_sample <- issues_df %>%
    group_by(month) %>%
    sample_n(1) %>%
    pull(issue)
  
  return(stratified_sample)
}

# Generate the stratified sample
sample <- stratified_sample_generator(results)

print(sample)

sample_df <- sample %>% 
  as.data.frame() |> 
  rename(identifier = 1)

# write.csv(sample_df, "~/Code/Moley/moley_newsweek/newsweek_sample_1953_55_57_59.csv")
# 
# sample_1951<- sample_df |> 
#   filter(grepl("1951",.))

```

# Code for a specific sample

```{r}
# sample_1951 <- read.csv("~/Code/Moley/moley_newsweek/newsweek_sample_1947_49_50_51.csv")
# 
# sample_1951<- sample_1951 |> 
#   filter(grepl("1951",.))

# sample2 <- c("sim_newsweek-us_1941-09-29_18_13",
# "sim_newsweek-us_1941-10-27_18_17",
# "sim_newsweek-us_1941-11-03_18_18",
# "sim_newsweek-us_1941-12-29_18_26")
```

```{r}
# Clean the text
cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", to_1939)
#cleaned_text <- gsub("^\\[\\d+\\]\\s*\"|\"$", "", sample)
cleaned_text <- gsub("\n\\s+", "\n", cleaned_text)
items <- unlist(strsplit(cleaned_text, "\\r?\\n"))


# Enclose each string in quotes and add a comma
formatted_items <- paste0("\"", items, "\",")

# Print each formatted item
for (item in formatted_items) {
  cat(item, "\n")
}
```

#Article extractor

```{r}
# Install required packages if not already installed
# if (!requireNamespace("internetarchive", quietly = TRUE)) {
#   remotes::install_github("ropensci/internetarchive")
# }
# if (!requireNamespace("httr", quietly = TRUE)) {
#   install.packages("httr")
# }
# if (!requireNamespace("purrr", quietly = TRUE)) {
#   install.packages("purrr")
# }
library(httr)
library(jsonlite)
library(purrr)

# download_item <- function(identifier, destdir, retries = 3) {
#   url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
#   file_path <- file.path(destdir, paste0(identifier, ".pdf"))

# Function to download item with retry mechanism
download_item <- function(identifier, destdir, retries = 3) {
  url <- paste0("https://archive.org/download/", identifier, "/", identifier, ".pdf")
  file_path <- file.path(destdir, paste0(identifier, ".pdf"))
  
  for (attempt in 1:retries) {
    tryCatch({
      GET(url, write_disk(file_path, overwrite = TRUE))
      print(paste("Downloaded", identifier, "to", destdir))
      break
    }, error = function(e) {
      if (attempt < retries) {
        print(paste("Error occurred, retrying...", attempt, "/", retries))
      } else {
        print(paste("Failed to download", identifier, "after", retries, "attempts"))
      }
    })
  }
}

# Define the sample

# sample <- c(
#  "sim_newsweek-us_1964_63_index",                   
#  "sim_newsweek-us_1964_64_index" ,                  
#  "sim_newsweek-us_1965_65_index" ,                  
#  "sim_newsweek-us_1965_66_index" ,                  
#  "sim_newsweek-us_1966_67_index"  ,                 
#  "sim_newsweek-us_1966_68_index" ,                  
#  "sim_newsweek-us_january-1-june-24-1968_71_index" ,
#  "sim_newsweek-us_january-2-june-1967_69_index",    
#  "sim_newsweek-us_july-1-december-30-1968_72_index",
#  "sim_newsweek-us_july-3-december-25-1967_70_index"  
# )

# Set up the download directory
download_directory <- file.path(path.expand("~"), 'Code', 'Moley', 'Newsweek_1938_39')
if (!dir.exists(download_directory)) {
  dir.create(download_directory, recursive = TRUE)
}

# Perform the download for the sample items
walk(to_1939, ~download_item(.x, download_directory))
# walk(sample, ~download_item(.x, download_directory))
#walk(results, ~download_item(.x, download_directory))
```

# Extracts the Moley column from a folder of pdfs

```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "~/Code/Moley/Newsweek_1953_55_57_59"

# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Extract all text
    text <- pdf_text(pdf_path)
    
    # Find page with both "perspective" and "Moley"
    #In case it is case sensitive, change PERSPECTIVE and MOLEY
    # page_num <- which(str_detect(text, regex("PERSPECTIVE", ignore_case = FALSE)) |
    #                   str_detect(text, regex("MOLEY", ignore_case = FALSE))  |
    #                   str_detect(text, regex("by Raymond", ignore_case = FALSE)))
    
    #FOR THE INDEX ONLY
    page_num <- which(str_detect(text, regex("Perspective Articles", ignore_case = FALSE)) |
                      str_detect(text, regex("MOLEY, RAYMOND", ignore_case = FALSE))) 
    
    # Extract the specific page
    if (length(page_num) > 0) {
      output_file <- paste0("Moley_column_", basename(pdf_path))
      pdf_subset(pdf_path, pages = page_num, output = output_file)
      return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
    } else {
      return(paste("Perspective not found in", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file
results <- sapply(pdf_files, extract_perspective)

# Print results
for (result in results) {
  cat(result, "\n")
}
```

#Formats the index

```{r}
index <- c("
Taxation as Discipline Ja 2: 68;
Coordinating the GOP Ja 9: 72;
It was No Monolith J 23: 104;
Unfair Tax-Rates: F 6: 108;
Wilson, Bullitt, Freud F 20: 104;
Creative Federalism Mr 6. 100;
Free for Whom? Mr 20: 112;
Wallace Threat Ap 3: 100;
Campaign Agonies Ap 17: 120;
Mr O'Briens Leviathan My 1: 96;
A New Electric Age My 15: 108;
The Battleship Returns My 29: 104;
The US, UN and UK Je 12: 104;
Ass in Lions Skin Je 26: 80;
Titan Unbound, I Jl 24: 80;
Tit Unbound, II Ag 7: 84;
What Kind of City? Ag 21: 72;
Rockfeller-Reagan S 18: 112;
Subsidy or Windfall O 2: 100;
Those Alleged Postal Subsidies O 16: 112;
Portrait of the GOP O 30: 108;
The GOP Mainstream N 13: 126;
A Look Beyond the War N 27: 108;
Romney the Incredible D 11: 116;
A Personal Note, Last column for Newsweek D 25: 76;
")




  # Split the index string into individual lines
index_lines <- str_split(index, ";\\s*")[[1]]

# Initialize an empty data frame with consistent data types
df <- data.frame(year = character(), title = character(), date = character(), volume = numeric(), stringsAsFactors = FALSE)

# Define a function to parse a line
parse_line <- function(line) {
  # Extract the title
  title <- str_trim(str_extract(line, "^[^\\.]+"))
  
  # Extract the date and volume
  date_volume <- str_extract(line, "[A-Za-z]+\\s[0-9]+:\\s*[0-9]+")
  date <- str_trim(str_extract(date_volume, "^[A-Za-z]+\\s[0-9]+"))
  volume <- as.numeric(str_extract(date_volume, "[0-9]+$"))
  
  year <- "1967"
  
  return(data.frame(year = year, title = title, date = date, volume = volume, stringsAsFactors = FALSE))
}

# Parse each line and append to the data frame
for (line in index_lines) {
  # Only process non-empty lines
  if (nchar(line) > 0 && !is.na(line)) {
    parsed_line <- parse_line(line)
    
    # Append the parsed line to the data frame if it contains valid data
    if (!is.na(parsed_line$title) && !is.na(parsed_line$date) && !is.na(parsed_line$volume)) {
      df <- bind_rows(df, parsed_line)
    }
  }
}

write.csv(df, "df_index.csv")

```


Notes on 1951-1959 issues (and possibly beyond) - they are no longer available for download
Fix:
Download the yearly indexes
https://archive.org/details/sim_newsweek-us_1959_54_index/page/n33/mode/2up
Search for the Moley "Perspective Articles" entry
scrape that page
compile into an index
take the page numbers and add to specific urls
https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up
Use the screenshot capture below to extract the text


# Screenshot capture
captureScreenshot(): This function captures the entire page content as an image, allowing you to retain visual elements like images.

PNG Format: Saving as a .png file maintains the quality of the screenshot.

# Import sample article index
```{r}

sample <- read_csv("moley_newsweek/newsweek_sample_1953_55_57_59.csv") |> 
  rename(list = 2, index = 1)

#filtering
sample <- sample |> 
  filter(str_detect(list,"_1959")) |> 
   mutate(date2 = str_extract(list, "\\d{4}-\\d{2}-\\d{2}")) |> 
  mutate(date2 = lubridate::ymd(date2))

```



# Compile Perspective Entries into an index
```{r}
library(googlesheets4)
googlesheets4::gs4_deauth()
index <- read_sheet("https://docs.google.com/spreadsheets/d/1GCvfNHgEN_TP1KA6YdpBf-Bp0YdwGOeG8x9uSzjHvGI/edit?usp=sharing")

#replace question marks for column splitting
index <- index |> 
  mutate(Text = str_replace_all(Text, "\\?", "."))
#split column on period

index1 <- separate(index, col = Text, into = c("column2", "column3"), sep = "\\.")

index <- separate(index1, col = column3, into = c("date", "page"), sep = "\\:")  

library(stringr)

# Define the month abbreviation mapping
month_lookup <- c("Ja" = "01", "F" = "02", "Mr" = "03", "Ap" = "04",
                  "My" = "05", "Jn" = "06", "Jl" = "07", "Ag" = "08",
                  "S" = "09", "O" = "10", "N" = "11", "D" = "12")

# Create the `date2` column
index <- index %>%
  mutate(
    # Extract month abbreviation and replace using lookup
    month = str_extract(date, "[A-Za-z]+"),
    day = str_extract(date, "\\d+"),
    month_num = month_lookup[month],
    # Format `date2` as "YYYY-MM-DD_page"
    date2 = sprintf("%s-%s-%02d", Year, month_num, as.integer(day))) |> 
  mutate(date2 = lubridate::ymd(date2)) |> 
  mutate(page = str_squish(page)) |> 
  mutate(real_page = as.numeric(page)) |> 
  mutate(real_page = (real_page + 1)) 
 

```


# Join index and sample
```{r}
index1 <- index |> 
  inner_join(sample, by=c("date2")) |> 
  mutate(URL = paste0("https://archive.org/details/", list, "/","page","/n",real_page,"/","mode/2up"))


```

https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/16/mode/2up

https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/88/mode/2up

https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up

# Capture to PNG
```{r}
library(chromote)
library(base64enc)  # For base64 decoding

# Initialize the Chromote browser
b <- Chromote$new()

# List of URLs to capture as images
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )

urls <- index1$URL

# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("newsweek_nov6page_", i, ".png")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the initial page load event fires
  
  Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely

    # Set viewport size and device scale factor for higher resolution
  session$Emulation$setDeviceMetricsOverride(
    width = 1920,        # Width in pixels
    height = 1080,       # Height in pixels
    deviceScaleFactor = 2,  # Increase the pixel density (higher means higher resolution)
    mobile = FALSE
  )

  # Capture the page as a screenshot
  screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
  writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file

  cat(paste0("Downloaded screenshot: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()


```

# Scraping, screenshot and cropping png

```{r}
library(chromote)
library(base64enc)  # For base64 decoding
library(magick)     # For image cropping

# Initialize the Chromote browser
b <- Chromote$new()

urls <- index1$URL
# List of URLs to capture as images
# urls <- index1 |>
#   filter(month == "Jl") |>
#   select(URL) |>
#   as.character()

# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("perspective_1959_", i, ".png")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the initial page load event fires
  
  Sys.sleep(8)  # Wait an additional time for full content loading

  # Set viewport size and device scale factor for higher resolution
  session$Emulation$setDeviceMetricsOverride(
    width = 180,         # Width approximating columns 2 and 3
    height = 700,        # Height capturing the middle 50% of the page
    deviceScaleFactor = 6,  
    mobile = FALSE
  )

  # Capture the page as a screenshot
  screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
  writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file

  # Load the saved image for further cropping
  img <- image_read(filename)

  # Get the image dimensions
  img_width <- image_info(img)$width
  img_height <- image_info(img)$height

  # Calculate the regions to keep based on image dimensions
  # Removing top 20%, preserving columns 2 and 3, and removing bottom 25%

  # 1. Calculate the vertical crop:
  # Top 20% removal
  top_crop_height <- round(img_height * 0.2)
  # Bottom 25% removal
  bottom_crop_height <- round(img_height * 0.25)

  # Define the new height by excluding top and bottom crop
  new_height <- img_height - top_crop_height - bottom_crop_height

  # 2. Calculate the horizontal crop to isolate columns 2 and 3:
  # Assuming columns 2 and 3 are the central 33-66% of the image width
  left_crop_width <- round(img_width * 0.37)
  new_width <- round(img_width * 0.55)

  # Apply cropping: remove top 20%, bottom 25%, and crop horizontally to columns 2 and 3
  img_cropped <- image_crop(
    img,
    geometry = sprintf("%dx%d+%d+%d", 
                       new_width,             # Width of columns 2 and 3
                       new_height,            # Height after top and bottom removal
                       left_crop_width,       # Offset from the left to start at column 2
                       top_crop_height)       # Offset from the top to skip 20%
  )

  # Save the final cropped image
  image_write(img_cropped, filename)

  cat(paste0("Downloaded and cropped screenshot: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()

```



#Cropping PNG
```{r}
library(chromote)
library(base64enc)  # For base64 decoding
#install.packages("magick")
library(magick)     # For image cropping

# Initialize the Chromote browser
b <- Chromote$new()

# List of URLs to capture as images
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )

# urls <- index1$URL
urls <- index1 |>
  filter(month == "Jl") |>
  select(URL) |>
  as.character()

# Loop through each URL and save it as an image
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("perspective_1959_", i, ".png")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the initial page load event fires
  
  Sys.sleep(8)  # Wait an additional 5 seconds for content to load completely

  # Set viewport size and device scale factor for higher resolution
  # Approximate columns 2 and 3 and middle 50% height
  session$Emulation$setDeviceMetricsOverride(
    width = 180,         # 640 Width approximating columns 2 and 3
    height = 700,        # 540 Height capturing the middle 50% of the page
    deviceScaleFactor = 6,  
    mobile = FALSE
  )

  # Capture the page as a screenshot
  screenshot_data <- session$Page$captureScreenshot()  # Get the screenshot binary data
  writeBin(base64decode(screenshot_data$data), filename)  # Save binary data to an image file

  # # Further crop the saved image to fine-tune top/bottom 25% removal
# img <- image_read(filename)
# img <- image_crop(img, "1340x740+0+435")  # Crop to middle 50%
# image_write(img, filename)
img <- image_read(filename)
img <- image_crop(img, "1840x2500+0+835")  # Crop to middle 50%

image_write(img, filename)

  cat(paste0("Downloaded and cropped screenshot: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()

```

#Further cropping
```{r}
library(magick)

# Load the image
img <- image_read("Xperspective_1959_1.png")

# Get the image dimensions
img_width <- image_info(img)$width
img_height <- image_info(img)$height

# Calculate the regions to keep based on image dimensions
# Removing top 20%, preserving columns 2 and 3, and removing bottom 25%

# 1. Calculate the vertical crop:
# Top 20% removal
top_crop_height <- round(img_height * 0.2)
# Bottom 25% removal
bottom_crop_height <- round(img_height * 0.25)

# Define the new height by excluding top and bottom crop
new_height <- img_height - top_crop_height - bottom_crop_height

# 2. Calculate the horizontal crop to isolate columns 2 and 3:
# Assuming columns 2 and 3 are the central 33-66% of the image width
left_crop_width <- round(img_width * 0.37)
new_width <- round(img_width * 0.55)

# Apply cropping: remove top 20%, bottom 25%, and crop horizontally to columns 2 and 3
img_cropped <- image_crop(
  img,
  geometry = sprintf("%dx%d+%d+%d", 
                     new_width,             # Width of columns 2 and 3
                     new_height,            # Height after top and bottom removal
                     left_crop_width,       # Offset from the left to start at column 2
                     top_crop_height)       # Offset from the top to skip 20%
)

# Save or display the cropped image
image_write(img_cropped, "cropped_image.png")
print(img_cropped)


```


# Capture to PDF

```{r}
library(chromote)
library(base64enc)  # For base64 decoding

# Initialize the Chromote browser
b <- Chromote$new()

# List of URLs to capture as PDFs
# urls <- c(
#   "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
#   "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
# )

urls <- index1$URL

# Loop through each URL and save it as a PDF
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("newsweek_nov6page_", i, ".pdf")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the initial page load event fires
  
  Sys.sleep(5)  # Wait an additional 5 seconds for content to load completely

  # Capture the page as a PDF
  pdf_data <- session$Page$printToPDF(
    printBackground = TRUE,  # Include background graphics
    preferCSSPageSize = TRUE # Use CSS-defined page sizes
  )
  
  writeBin(base64decode(pdf_data$data), filename)  # Save binary data to a PDF file

  cat(paste0("Downloaded PDF: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()

```

# HTML scraping
```{r}
# Load required libraries
library(rvest)
library(grid)

# URL of the page to scrape
url <- "https://archive.org/details/sim_newsweek-us_1959-07-06_54_1/page/n89/mode/2up"

# Read the HTML content of the page
page <- read_html(url)


# Delay for 5 seconds to allow the page to fully load
Sys.sleep(5)


# Locate the "Perspective" article - adjust selector as needed
article_text <- page %>%
  html_nodes("div") %>%  # Adjust this selector based on inspection
  html_text() 

# Filter to include only the relevant text containing "Perspective"
perspective_text <- grep("Perspective", article_text, value = TRUE)

# Combine text if it's in multiple elements
perspective_text <- paste(perspective_text, collapse = "\n\n")

# Wrap text to ensure it fits within PDF dimensions
lines <- strwrap(perspective_text, width = 80)

# Create PDF and set text layout with grid
pdf_file <- "Perspective_Article.pdf"
pdf(pdf_file, width = 8.5, height = 11)

# Start grid and draw text line by line
grid.newpage()
y_position <- 0.9  # Start near the top of the page

for (line in lines) {
  grid.text(line, x = 0.1, y = y_position, just = "left", gp = gpar(fontsize = 10))
  y_position <- y_position - 0.03  # Move down for each line
}

dev.off()

cat(paste0("PDF saved as: ", pdf_file))

```


# NOTES

#Original Import spreadsheet of Newsweek back issues

```{r}
# newsweek <- rio::import("newsweek_sample_size.xlsx", sheet="newsweek_index_37_61")
# 
# newsweek1960s <- newsweek %>% 
#   filter(str_detect(identifier, "1960$")) 
# 
# 
# newsweek1960s <- newsweek %>%
#   mutate(year = as.numeric(str_extract(identifier, "19\\d{2}"))) %>%  # Extract year as numeric
#   filter(year >= 1962 & year <= 1969) %>%                             # Filter on year range
#   select(-year)       
# 
# # newsweek1962 <- all_results %>% 
# #   filter(str_detect(identifier, "1962")) 
# 
# newsweek1946_clean <- gsub("c\\(\"|\"\\)|\\n", "", newsweek1946)
# 
# x_clean <- newsweek1946_clean
# 
# 
# # Split the string into a vector based on ", "
# x_list <- unlist(strsplit(x_clean, ", "))
# 
# x_list <- gsub("\\\"", "", x_list)
# 
# head(x_list)
# 
# 
# results <- x_list

```

```{r}
Here's a quick recap of what we did to solve the problem:

We replaced the ia_download function with a custom download_item function.
We used httr::GET to download the PDF files directly from the Internet Archive.
We constructed the URL for each PDF based on the item identifier.
We maintained the retry mechanism for robustness.

This solution gives you more control over the download process and eliminates dependencies on potentially problematic packages.

```

```{r}
# Provided list of issues

results <- c(
"sim_newsweek-us_1952-01-07_39_1",
"sim_newsweek-us_1952-01-14_39_2",
"sim_newsweek-us_1952-01-21_39_3",
"sim_newsweek-us_1952-01-28_39_4",
"sim_newsweek-us_1952-02-04_39_5",
"sim_newsweek-us_1952-02-11_39_6",
"sim_newsweek-us_1952-02-18_39_7",
"sim_newsweek-us_1952-02-25_39_8",
"sim_newsweek-us_1952-03-03_39_9",
"sim_newsweek-us_1952-03-10_39_10",
"sim_newsweek-us_1952-03-17_39_11",
"sim_newsweek-us_1952-03-24_39_12",
"sim_newsweek-us_1952-03-31_39_13",
"sim_newsweek-us_1952-04-07_39_14",
"sim_newsweek-us_1952-04-14_39_15",
"sim_newsweek-us_1952-04-21_39_16",
"sim_newsweek-us_1952-04-28_39_17",
"sim_newsweek-us_1952-05-05_39_18",
"sim_newsweek-us_1952-05-12_39_19",
"sim_newsweek-us_1952-05-19_39_20",
"sim_newsweek-us_1952-05-26_39_21",
"sim_newsweek-us_1952-06-02_39_22",
"sim_newsweek-us_1952-06-09_39_23",
"sim_newsweek-us_1952-06-16_39_24",
"sim_newsweek-us_1952-06-23_39_25",
"sim_newsweek-us_1952-06-30_39_26",
"sim_newsweek-us_1952-07-07_40_1",
"sim_newsweek-us_1952-07-14_40_2",
"sim_newsweek-us_1952-07-21_40_3",
"sim_newsweek-us_1952-07-28_40_4",
"sim_newsweek-us_1952-08-04_40_5",
"sim_newsweek-us_1952-08-11_40_6",
"sim_newsweek-us_1952-08-18_40_7",
"sim_newsweek-us_1952-08-25_40_8",
"sim_newsweek-us_1952-09-01_40_9",
"sim_newsweek-us_1952-09-08_40_10",
"sim_newsweek-us_1952-09-15_40_11",
"sim_newsweek-us_1952-09-22_40_12",
"sim_newsweek-us_1952-09-29_40_13",
"sim_newsweek-us_1952-10-06_40_14",
"sim_newsweek-us_1952-10-13_40_15",
"sim_newsweek-us_1952-10-20_40_16",
"sim_newsweek-us_1952-10-27_40_17",
"sim_newsweek-us_1952-11-03_40_18",
"sim_newsweek-us_1952-11-10_40_19",
"sim_newsweek-us_1952-11-17_40_21",
"sim_newsweek-us_1952-11-24_40_22",
"sim_newsweek-us_1952-12-01_40_23",
"sim_newsweek-us_1952-12-08_40_24",
"sim_newsweek-us_1952-12-15_40_25",
"sim_newsweek-us_1952-12-22_40_26",
"sim_newsweek-us_1952-12-29_40_27"
)
```

# Past Samples

```         

1951, 53, 55, 57, 59 samples
Show in New Window
[1] "sim_newsweek-us_1953-01-05_41_1" "sim_newsweek-us_1953-01-12_41_2"
[3] "sim_newsweek-us_1953-01-19_41_3" "sim_newsweek-us_1953-01-26_41_4"
[5] "sim_newsweek-us_1953-02-02_41_5" "sim_newsweek-us_1953-02-09_41_6"
Show in New Window
"sim_newsweek-us_1953-01-26_41_4", 
"sim_newsweek-us_1953-02-23_41_8", 
"sim_newsweek-us_1953-03-23_41_12", 
"sim_newsweek-us_1953-04-06_41_14", 
"sim_newsweek-us_1953-05-25_41_21", 
"sim_newsweek-us_1953-06-22_41_25", 
"sim_newsweek-us_1953-07-13_42_2", 
"sim_newsweek-us_1953-08-24_42_8", 
"sim_newsweek-us_1953-09-28_42_13", 
"sim_newsweek-us_1953-10-12_42_15", 
"sim_newsweek-us_1953-11-30_42_22", 
"sim_newsweek-us_1953-12-07_42_23", 
"sim_newsweek-us_1955-01-03_45_1", 
"sim_newsweek-us_1955-02-21_45_8", 
"sim_newsweek-us_1955-03-28_45_13", 
"sim_newsweek-us_1955-04-11_45_15", 
"sim_newsweek-us_1955-05-30_45_22", 
"sim_newsweek-us_1955-06-13_45_24", 
"sim_newsweek-us_1955-07-18_46_3", 
"sim_newsweek-us_1955-08-08_46_6", 
"sim_newsweek-us_1955-09-19_46_12", 
"sim_newsweek-us_1955-10-10_46_15", 
"sim_newsweek-us_1955-11-28_46_22", 
"sim_newsweek-us_1955-12-05_46_23", 
"sim_newsweek-us_1957-01-28_49_4", 
"sim_newsweek-us_1957-02-25_49_8", 
"sim_newsweek-us_1957-03-18_49_11", 
"sim_newsweek-us_1957-04-29_49_17", 
"sim_newsweek-us_1957-05-06_49_18", 
"sim_newsweek-us_1957-06-03_49_22", 
"sim_newsweek-us_1957-07-08_50_2", 
"sim_newsweek-us_1957-08-26_50_9", 
"sim_newsweek-us_1957-09-16_50_12", 
"sim_newsweek-us_1957-10-21_50_17", 
"sim_newsweek-us_1957-11-18_50_21", 
"sim_newsweek-us_1957-12-09_50_24", 
"sim_newsweek-us_1959-01-26_53_4", 
"sim_newsweek-us_1959-02-23_53_8", 
"sim_newsweek-us_1959-03-02_53_9", 
"sim_newsweek-us_1959-04-20_53_16", 
"sim_newsweek-us_1959-05-04_53_18", 
"sim_newsweek-us_1959-06-29_53_26", 
"sim_newsweek-us_1959-07-06_54_1", 
"sim_newsweek-us_1959-08-31_54_9", 
"sim_newsweek-us_1959-09-28_54_13", 
"sim_newsweek-us_1959-10-26_54_17", 
"sim_newsweek-us_1959-11-30_54_22", 
"sim_newsweek-us_1959-12-14_54_24", 
"sim_newsweek-us_1953_41_index", 

#1958 sample
sample <- c(
"sim_newsweek-us_1958-01-27_51_4", 
"sim_newsweek-us_1958-02-17_51_7", 
"sim_newsweek-us_1958-03-24_51_12", 
"sim_newsweek-us_1958-04-07_51_14", 
"sim_newsweek-us_1958-05-12_51_19", 
"sim_newsweek-us_1958-06-16_51_24", 
"sim_newsweek-us_1958-07-21_52_3", 
"sim_newsweek-us_1958-08-25_52_8", 
"sim_newsweek-us_1958-09-22_52_12", 
"sim_newsweek-us_1958-10-13_52_15", 
"sim_newsweek-us_1958-11-17_52_20", 
"sim_newsweek-us_1958-12-08_52_23"
)

#1954 sample
sample <- c(
"sim_newsweek-us_1954-01-25_43_4", 
"sim_newsweek-us_1954-02-01_43_5", 
"sim_newsweek-us_1954-03-29_43_13", 
"sim_newsweek-us_1954-04-19_43_16", 
"sim_newsweek-us_1954-05-17_43_20", 
"sim_newsweek-us_1954-06-14_43_24", 
"sim_newsweek-us_1954-07-05_44_1", 
"sim_newsweek-us_1954-08-09_44_6", 
"sim_newsweek-us_1954-09-13_44_11", 
"sim_newsweek-us_1954-10-25_44_17", 
"sim_newsweek-us_1954-11-08_44_19", 
"sim_newsweek-us_1954-12-27_44_26"
)

#1952 sample
"sim_newsweek-us_1952-01-21_39_3", 
"sim_newsweek-us_1952-02-18_39_7" ,
"sim_newsweek-us_1952-03-03_39_9", 
"sim_newsweek-us_1952-04-14_39_15",
"sim_newsweek-us_1952-05-19_39_20",
"sim_newsweek-us_1952-06-23_39_25",
"sim_newsweek-us_1952-07-21_40_3", 
"sim_newsweek-us_1952-08-18_40_7", 
"sim_newsweek-us_1952-09-22_40_12",
"sim_newsweek-us_1952-10-27_40_17",
"sim_newsweek-us_1952-11-24_40_22",
"sim_newsweek-us_1952-12-29_40_27"



#1951 sample
"sim_newsweek-us_1951-01-01_37_1", 
"sim_newsweek-us_1951-02-26_37_9", 
"sim_newsweek-us_1951-03-26_37_13", 
"sim_newsweek-us_1951-04-09_37_15", 
"sim_newsweek-us_1951-05-14_37_20", 
"sim_newsweek-us_1951-06-11_37_24", 
"sim_newsweek-us_1951-07-16_38_3", 
"sim_newsweek-us_1951-08-13_38_7", 
"sim_newsweek-us_1951-09-24_38_13", 
"sim_newsweek-us_1951-10-22_38_17", 
"sim_newsweek-us_1951-11-26_38_22", 
"sim_newsweek-us_1951-12-24_38_26", 

#1950 sample
"sim_newsweek-us_1950-01-02_35_1", 
"sim_newsweek-us_1950-02-06_35_6", 
"sim_newsweek-us_1950-03-06_35_10", 
"sim_newsweek-us_1950-04-17_35_16", 
"sim_newsweek-us_1950-05-01_35_18", 
"sim_newsweek-us_1950-06-19_35_25", 
"sim_newsweek-us_1950-07-31_36_5", 
"sim_newsweek-us_1950-08-21_36_8", 
"sim_newsweek-us_1950-09-25_36_13", 
"sim_newsweek-us_1950-10-23_36_17", 
"sim_newsweek-us_1950-11-27_36_22", 
"sim_newsweek-us_1950-12-11_36_24", 

#1949 sample
"sim_newsweek-us_1949-01-24_33_4", 
"sim_newsweek-us_1949-02-14_33_7", 
"sim_newsweek-us_1949-03-14_33_11", 
"sim_newsweek-us_1949-04-18_33_16", 
"sim_newsweek-us_1949-05-09_33_19", 
"sim_newsweek-us_1949-06-27_33_26", 
"sim_newsweek-us_1949-07-18_34_3", 
"sim_newsweek-us_1949-08-01_34_5", 
"sim_newsweek-us_1949-09-19_34_12", 
"sim_newsweek-us_1949-10-24_34_17", 
"sim_newsweek-us_1949-11-21_34_21", 
"sim_newsweek-us_1949-12-12_34_24", 


#1948 sample
"sim_newsweek-us_1948-01-26_31_4" 
"sim_newsweek-us_1948-02-09_31_6" 
"sim_newsweek-us_1948-03-01_31_9" 
"sim_newsweek-us_1948-04-19_31_16"
"sim_newsweek-us_1948-05-10_31_19"
"sim_newsweek-us_1948-06-07_31_23"
"sim_newsweek-us_1948-07-26_32_4" 
"sim_newsweek-us_1948-08-09_32_6" 
"sim_newsweek-us_1948-09-06_32_10"
"sim_newsweek-us_1948-10-18_32_16"
"sim_newsweek-us_1948-11-15_32_20"
"sim_newsweek-us_1948-12-06_32_23"

#1947 sample
"sim_newsweek-us_1947-01-20_29_3", 
"sim_newsweek-us_1947-02-03_29_5", 
"sim_newsweek-us_1947-03-31_29_13", 
"sim_newsweek-us_1947-04-14_29_15", 
"sim_newsweek-us_1947-05-19_29_20", 
"sim_newsweek-us_1947-06-09_29_23", 
"sim_newsweek-us_1947-07-14_30_2", 
"sim_newsweek-us_1947-08-25_30_8", 
"sim_newsweek-us_1947-09-22_30_12", 
"sim_newsweek-us_1947-10-27_30_17", 
"sim_newsweek-us_1947-11-17_30_20", 
"sim_newsweek-us_1947-12-15_30_24", 


#1946 sample
"sim_newsweek-us_1946-01-21_27_3", 
"sim_newsweek-us_1946-02-04_27_5", 
"sim_newsweek-us_1946-03-11_27_10", 
"sim_newsweek-us_1946-04-29_27_17", 
"sim_newsweek-us_1946-05-13_27_19", 
"sim_newsweek-us_1946-06-24_27_25", 
"sim_newsweek-us_1946-07-08_28_2", 
"sim_newsweek-us_1946-08-19_28_8", 
"sim_newsweek-us_1946-09-16_28_12", 
"sim_newsweek-us_1946-10-14_28_16", 
"sim_newsweek-us_1946-11-11_28_20", 
"sim_newsweek-us_1946-12-09_28_24"

#1942 sample
  
  
"sim_newsweek-us_1942-01-19_19_3", 
"sim_newsweek-us_1942-02-02_19_5", 
"sim_newsweek-us_1942-03-23_19_12", 
"sim_newsweek-us_1942-04-20_19_16", 
"sim_newsweek-us_1942-05-11_19_19", 
"sim_newsweek-us_1942-06-01_19_22", 
"sim_newsweek-us_1942-07-06_20_1", 
"sim_newsweek-us_1942-08-24_20_8", 
"sim_newsweek-us_1942-09-28_20_13", 
"sim_newsweek-us_1942-10-19_20_16", 
"sim_newsweek-us_1942-11-16_20_20", 
"sim_newsweek-us_1942-12-07_20_23"

#1941-1938 sample
"sim_newsweek-us_1938-01-17_11_3", 
"sim_newsweek-us_1938-02-07_11_6", 
"sim_newsweek-us_1938-03-28_11_13", 
"sim_newsweek-us_1938-04-25_11_17", 
"sim_newsweek-us_1938-05-30_11_22", 
"sim_newsweek-us_1938-06-27_11_26", 
"sim_newsweek-us_1938-07-11_12_2", 
"sim_newsweek-us_1938-08-22_12_8", 
"sim_newsweek-us_1938-09-12_12_11", 
"sim_newsweek-us_1938-10-31_12_18", 
"sim_newsweek-us_1938-11-07_12_19", 
"sim_newsweek-us_1938-12-19_12_25", 
"sim_newsweek-us_1939-01-16_13_3", 
"sim_newsweek-us_1939-02-06_13_6", 
"sim_newsweek-us_1939-03-27_13_13", 
"sim_newsweek-us_1939-04-10_13_15", 
"sim_newsweek-us_1939-05-08_13_19", 
"sim_newsweek-us_1939-06-26_13_26", 
"sim_newsweek-us_1939-07-03_14_1", 
"sim_newsweek-us_1939-08-14_14_7", 
"sim_newsweek-us_1939-09-11_14_11", 
"sim_newsweek-us_1939-10-30_14_18", 
"sim_newsweek-us_1939-11-06_14_19", 
"sim_newsweek-us_1939-12-25_14_26", 
"sim_newsweek-us_1940-01-01_15_1", 
"sim_newsweek-us_1940-02-26_15_9", 
"sim_newsweek-us_1940-03-04_15_10", 
"sim_newsweek-us_1940-04-01_15_14", 
"sim_newsweek-us_1940-05-06_15_19", 
"sim_newsweek-us_1940-06-10_15_24", 
"sim_newsweek-us_1940-07-29_16_5", 
"sim_newsweek-us_1940-08-26_16_9", 
"sim_newsweek-us_1940-09-02_16_10", 
"sim_newsweek-us_1940-10-14_16_16", 
"sim_newsweek-us_1940-11-11_16_20", 
"sim_newsweek-us_1940-12-09_16_24", 
"sim_newsweek-us_1941-01-06_17_1", 
"sim_newsweek-us_1941-02-17_17_7", 
"sim_newsweek-us_1941-03-03_17_9", 
"sim_newsweek-us_1941-04-28_17_17", 
"sim_newsweek-us_1941-05-26_17_21", 
"sim_newsweek-us_1941-06-23_17_25", 
"sim_newsweek-us_1941-07-14_18_2", 
"sim_newsweek-us_1941-08-11_18_6", 
"sim_newsweek-us_1941-09-15_18_11", 
"sim_newsweek-us_1941-10-13_18_15", 
"sim_newsweek-us_1941-11-24_18_21", 
"sim_newsweek-us_1941-12-29_18_26",



 1937 sample
 sample = [
    "sim_newsweek-us_1937-01-09_9_2",
    "sim_newsweek-us_1937-02-27_9_9",
    "sim_newsweek-us_1937-03-13_9_11",
    "sim_newsweek-us_1937-04-10_9_15",
    "sim_newsweek-us_1937-05-22_9_21",
    "sim_newsweek-us_1937-06-05_9_23",
    "sim_newsweek-us_1937-07-31_10_5",
    "sim_newsweek-us_1937-08-14_10_7",
    "sim_newsweek-us_1937-09-06_10_10",
    "sim_newsweek-us_1937-10-25_10_17",
    "sim_newsweek-us_1937-11-15_10_20",
    "sim_newsweek-us_1937-12-06_10_23"
]
```


# NOTES - OLD CODE

```{r}
url2 <- "https://archive.org/download/sim_newsweek-us_1945-"
test  <- url2 %>%
  read_html() %>%
  html_table() 


# Specify the base URL of the webpage you want to scrape
base_url <- "https://archive.org/details/pub_newsweek-us?and%5B%5D=year%3A%221938%22&and%5B%5D=year%3A%5B1937+TO+1969%5D"



# Read the HTML code from the website
webpage <- read_html(base_url)

# Use CSS selectors to scrape the links to the files
file_urls <- webpage %>%
  html_nodes("a") %>%
  html_attr("href")

# Filter the URLs to include only those that start with 'sim_newsweek-us_'
newsweek_urls <- file_urls[str_detect(file_urls, "^sim_newsweek-us_")]

# Append the base URL to each of the Newsweek URLs
newsweek_urls <- paste0(base_url, newsweek_urls)

# Print the Newsweek URLs
print(newsweek_urls)

#Part 2 - works

# Specify the base URL
base_url <- "https://archive.org/download/sim_newsweek-us_"

# Specify the dates of the issues you're interested in
dates <- c("1948-06-28_31_26", "1948-07-05_31_27")  # Add more dates as needed

# Generate the URLs
urls <- paste0(base_url, dates, "/sim_newsweek-us_", dates, ".pdf")

# Print the URLs
print(urls)

```

#from this tutorialL <https://github.com/hrbrmstr/wayback>

```{r}
devtools::install_github("hrbrmstr/wayback")
```

```{r}
library(wayback)
library(tidyverse)

# current verison
packageVersion("wayback")

archive_available("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")

get_mementos("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")


newsweek_timemap <- get_timemap("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml")

cdx_basic_query("https://archive.org/download/pub_newsweek-us/pub_newsweek-us_files.xml", limit = 10) %>% 
  glimpse()

mem <- read_memento("https://www.r-project.org/news.html")
res <- stringi::stri_split_lines(mem)[[1]]
cat(paste0(res[187:200], collaspe="\n"))

glimpse(
  ia_scrape("lemon curry")
)


(newsweek<- ia_scrape("identifier:pub_newsweek-us", count=100L))

## <ia_scrape object>
## Cursor: W3siaWRlbnRpZmllciI6IjAzLTEwLTE4X1NwYWNlLXRvLUdyb3VuZHMuemlwIn1d

(item <- ia_retrieve(newsweek$identifier[1]))

write.csv(item, "/Users/robwells/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/newsweek_data.csv")

download.file(item$link[1], file.path("~/Library/CloudStorage/Dropbox/Classes_Teaching_Archive/Data Journalism Classes/Data-Analysis-Class-Jour-405v-5003", item$file[1]))

```

```{r}

library(httr)
library(jsonlite)
library(tidyverse)
library(lubridate)

get_newsweek_issues <- function(year) {
  url <- "https://archive.org/advancedsearch.php"
  
  query <- list(
    q = paste0('collection:pub_newsweek-us AND date:[', year, ' TO ', year, ']'),
   fl = paste(c("identifier", "date", "title"), collapse = ","),
    sort = c("date asc"),
    output = "json",
    rows = 1000  # Adjust this if you need more results per year
  )
  
  response <- GET(url, query = query)
  
  if (status_code(response) != 200) {
    warning(paste("Failed to retrieve data for year", year))
    return(NULL)
  }
  
  content <- content(response, "text")
  data <- fromJSON(content)
  
  if (length(data$response$docs) == 0) {
    warning(paste("No results found for year", year))
    return(NULL)
  }
  
  results <- as_tibble(data$response$docs)
  results$year <- year
  
  Sys.sleep(1)  # Add delay to avoid rate limiting
  
  return(results)
}

# Get results for each year from 1962 to 1969
start_year <- 1962
end_year <- 1969

all_results <- map_dfr(start_year:end_year, get_newsweek_issues)

# Process results
processed_results <- all_results %>%
  mutate(date = as_date(date)) %>%
  filter(!is.na(date)) %>%
  arrange(date)

# Display the first few results
head(processed_results)

# Save results to a CSV file
write_csv(processed_results, "newsweek_issues.csv")
```

```{r}
library(httr)
library(rvest)
library(tidyverse)
library(lubridate)

get_newsweek_issues <- function(year) {
  url <- paste0("https://archive.org/search.php?query=collection%3A%28pub_newsweek-us%29%20AND%20date%3A", year, "&sort=-date")
  
  page <- read_html(url)
  
  titles <- page %>% html_nodes(".item-title") %>% html_text()
  dates <- page %>% html_nodes(".item-date") %>% html_text()
  identifiers <- page %>% html_nodes(".item-ia") %>% html_attr("data-id")
  
  if (length(titles) == 0) {
    warning(paste("No results found for year", year))
    return(NULL)
  }
  
  results <- tibble(
    title = titles,
    date = dates,
    identifier = identifiers,
    year = year,
    url = paste0("https://archive.org/details/", identifiers)
  ) %>%
    mutate(date = as_date(date)) %>%
    filter(!str_detect(identifier, "_index")) %>%
    arrange(date)
  
  Sys.sleep(2)  # Add delay to avoid rate limiting
  
  return(results)
}

# Get results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969

all_results <- map_dfr(start_year:end_year, get_newsweek_issues)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
```

#after much difficulty... index 1962-1969

```{r}

library(tidyverse)
library(lubridate)

generate_newsweek_issues <- function(start_year, end_year) {
  all_issues <- tibble(
    date = as.Date(character()),
    identifier = character(),
    title = character(),
    year = integer(),
    url = character()
  )
  
  for (year in start_year:end_year) {
    for (month in 1:12) {
      for (day in 1:31) {
        date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
        
        if (is.na(date) || month(date) != month) next  # Skip invalid dates
        
        volume <- 59 + (year - 1962)  # Assuming volume increments each year
        issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7  # Assuming weekly issues
        
        identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
        url <- paste0("https://archive.org/details/", identifier)
        
        all_issues <- all_issues %>% 
          add_row(
            date = date,
            identifier = identifier,
            title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
            year = year,
            url = url
          )
      }
    }
  }
  
  return(all_issues)
}

# Generate results for each year from 1962 to current year
start_year <- 1962
end_year <- 1969

all_results <- generate_newsweek_issues(start_year, end_year)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")
```

```{r}
library(tidyverse)
library(lubridate)
library(httr)

generate_newsweek_issues <- function(start_year, end_year) {
  all_issues <- tibble(
    date = as.Date(character()),
    identifier = character(),
    title = character(),
    year = integer(),
    url = character()
  )
  
  for (year in start_year:end_year) {
    for (month in 1:12) {
      for (day in 1:31) {
        date <- ymd(sprintf("%04d-%02d-%02d", year, month, day))
        
        if (is.na(date) || month(date) != month) next # Skip invalid dates
        
        volume <- 59 + (year - 1962) # Assuming volume increments each year
        issue <- 1 + as.integer(date - ymd(sprintf("%04d-01-01", year))) %/% 7 # Assuming weekly issues
        
        identifier <- sprintf("sim_newsweek-us_%04d-%02d-%02d_%d_%d", year, month, day, volume, issue)
        url <- paste0("https://archive.org/details/", identifier)
        
        response <- HEAD(url)
        
        if (status_code(response) == 200) {
          all_issues <- all_issues %>%
            add_row(
              date = date,
              identifier = identifier,
              title = sprintf("Newsweek %04d-%02d-%02d", year, month, day),
              year = year,
              url = url
            )
        }
      }
    }
  }
  
  return(all_issues)
}

# Generate results for each year from 1962 to the current year
start_year <- 1962
end_year <- 1963

all_results <- generate_newsweek_issues(start_year, end_year)

# Display the first few results
print(head(all_results))

# Save results to a CSV file
write_csv(all_results, "newsweek_issues.csv")

```


# Oct 27 attempt
```{r}
library(httr)

# URL of the page to download
url <- "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up"

# Save the page as an HTML file locally
response <- GET(url)

if (status_code(response) == 200) {
  writeBin(content(response, "raw"), "newsweek_1959_10_26_page_132.html")
  cat("Downloaded successfully!\n")
} else {
  cat("Failed to download. Status:", status_code(response), "\n")
}
```

```{r}
library(chromote)
library(base64enc)  # For base64 decoding

# Initialize the Chromote browser
b <- Chromote$new()

# List of URLs to download as PDFs
urls <- c(
  "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
  "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
)

# Loop through each URL and save it as a PDF
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("newsweek_page_", i, ".pdf")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the page fully loads

  # Save the page as a PDF
  pdf_data <- session$Page$printToPDF()  # Get the PDF binary data
  writeBin(base64decode(pdf_data$data), filename)  # Save binary data to a PDF file

  cat(paste0("Downloaded: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()


```
#This one yields something but not the article
```{r}
library(chromote)
library(base64enc)  # Ensure this is loaded for base64 decoding

# Initialize the Chromote browser
b <- Chromote$new()

# List of URLs to download as PDFs
urls <- c(
  "https://archive.org/details/sim_newsweek-us_1959-10-26_54_17/page/132/mode/2up",
  "https://archive.org/details/sim_newsweek-us_1959-11-02_54_18/page/108/mode/2up"
)

# Loop through each URL and save it as a PDF
for (i in seq_along(urls)) {
  url <- urls[i]
  filename <- paste0("newsweek_page_", i, ".pdf")

  # Open a new session for each URL
  session <- b$new_session()

  # Navigate to the URL and wait for the page to load
  session$Page$navigate(url)
  session$Page$loadEventFired()  # Wait until the page is fully loaded

  # Save the page as a PDF
  pdf_data <- session$Page$printToPDF()  # Get the PDF binary data
  writeBin(base64decode(pdf_data$data), filename)  # Save the binary data to a PDF file

  cat(paste0("Downloaded: ", filename, "\n"))

  # Close the session
  session$close()
}

# Close the Chromote browser
b$close()

```

