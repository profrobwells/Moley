---
title: "Parse Proquest"
author: "Rob Wells"
date: "2025-02-08"
output: html_document
---
```{r}
# Install and load required packages
#install.packages(c("pdftools", "stringr"))
library(pdftools)
library(stringr)
install.packages("qpdf")
library(pdftools)
library(stringr)
library(qpdf)

```



#this splits to a single article but doesn't rename the file
```{r}

split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
  # Create output directory if it doesn't exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir)
  }
  
  # Read PDF text by pages
  pdf_text <- pdf_text(input_pdf)
  total_pages <- length(pdf_text)
  
  # Initialize variables
  article_info <- list()
  
  # Function to safely extract and clean title
  extract_title <- function(text) {
    # Look for text that appears between the start of the page and "Moley, Raymond"
    title <- str_extract(text, "^.*?(?=\\nMoley, Raymond)")
    
    if(is.na(title)) return("untitled")
    
    # Clean the title
    title <- str_trim(title)
    title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
    title <- str_replace_all(title, "\\s+", "_")
    
    if(nchar(title) == 0) return("untitled")
    
    return(title)
  }
  
  # Process pages in groups of 3
  for(i in seq(1, total_pages, by = 3)) {
    # Check if we have enough pages left for a complete article
    if(i + 2 <= total_pages) {
      # Get the title
      title <- extract_title(pdf_text[i])
      
      # Add article info
      article_info[[length(article_info) + 1]] <- list(
        title = title,
        article_page = i + 1  # The actual article is the second page in each group
      )
      cat("Found article:", title, "on page", i + 1, "\n")
    }
  }
  
  # Extract each article
  for(i in seq_along(article_info)) {
    # Create filename
    filename <- paste0(article_info[[i]]$title, "_", i, ".pdf")
    
    # Create full path
    filepath <- file.path(output_dir, filename)
    
    # Extract just the article page
    tryCatch({
      pdf_subset(
        input_pdf,
        filepath,
        pages = article_info[[i]]$article_page
      )
      cat("Saved article:", filename, "\n")
    }, error = function(e) {
      cat("Error saving", filename, ":", conditionMessage(e), "\n")
    })
  }
  
  cat("Processed", length(article_info), "articles\n")
}

# Usage example:
input_pdf <- "perspective_256_400.pdf"  # Replace with your actual path
split_newsweek_articles(input_pdf, "split_articles")
# Usage example:



```



# first version
this works and extracts three pages per article
```{r}
# Install and load required packages

# Function to process PDF and split articles
split_newsweek_articles <- function(input_pdf, output_dir = "split_articles") {
  # Create output directory if it doesn't exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir)
  }
  
  # Read PDF text by pages to identify article boundaries
  pdf_text <- pdf_text(input_pdf)
  
  # Get total number of pages
  total_pages <- length(pdf_text)
  
  # Initialize variables
  article_starts <- c()
  article_titles <- c()
  current_page <- 1
  
  # Find article start pages and titles
  while(current_page <= total_pages) {
    if(grepl("ABSTRACT", pdf_text[current_page], fixed = TRUE)) {
      article_starts <- c(article_starts, current_page)
      
      # Extract title (appears before ABSTRACT)
      title <- str_extract(pdf_text[current_page], "(?<=\\n).*?(?=\\n.*ABSTRACT)")
      title <- str_trim(title)
      title <- str_replace_all(title, "[^[:alnum:][:space:]]", "")
      title <- str_replace_all(title, "\\s+", "_")
      
      article_titles <- c(article_titles, title)
    }
    current_page <- current_page + 1
  }
  
  # Add the end page for the last article
  article_starts <- c(article_starts, total_pages + 1)
  
  # Split PDF into articles
  for(i in 1:(length(article_starts)-1)) {
    start_page <- article_starts[i]
    end_page <- article_starts[i+1] - 1
    
    # Create filename
    if(is.na(article_titles[i]) || article_titles[i] == "") {
      filename <- paste0("article_", i, ".pdf")
    } else {
      filename <- paste0(article_titles[i], ".pdf")
    }
    
    # Create full path
    filepath <- file.path(output_dir, filename)
    
    # Extract pages for this article
    pdf_subset(
      input_pdf,
      filepath,
      pages = start_page:end_page
    )
    
    cat("Saved article:", filename, "\n")
  }
}

# Usage example:
input_pdf <- "perspective_test.pdf"
split_newsweek_articles(input_pdf, "split_articles")
```

