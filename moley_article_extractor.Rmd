---
title: Moley Article Extractor - using archive.org 
output:
  html_document: default
  pdf_document: default
---



#gemini failed
```{r}
library(httr)
library(stringr)

# Function to create archive.org URLs
create_archive_urls <- function(identifier_string) {
  identifiers <- strsplit(identifier_string, ",")[[1]]
  identifiers <- trimws(identifiers)
  
  urls <- sapply(identifiers, function(id) {
    paste0("https://archive.org/stream/", id, "/", id, "_djvu.txt")
  })
  
  return(urls)
}

# Function to extract and save perspective section
extract_and_save_perspective <- function(url, output_dir = "articles") {
  # Create output directory if it doesn't exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir)
  }
  
  # Extract identifier from URL to use as filename
  identifier <- str_extract(url, "sim_newsweek-us_[\\d-]+_\\d+_\\d+")
  output_file <- file.path(output_dir, paste0(identifier, ".txt"))
  
  read_url <- str_replace(url, "stream", "download")
  headers <- c(
    "User-Agent" = "R-Script/1.0",
    "Accept" = "text/plain"
  )
  
  response <- tryCatch({
    GET(read_url, add_headers(.headers=headers))
  }, error = function(e) {
    warning(paste("Download failed for:", identifier, " -", e$message))
    return(NULL)
  })
  
  if (is.null(response)) {
    return(FALSE)
  }
  
  if (status_code(response) == 200) {
    content <- rawToChar(response$content)
    
    # Attempt to identify approximate page range
    page_breaks <- str_locate_all(content, "Page \\d+?")[[1]] 
    if (length(page_breaks) < 5) {
      warning("Could not reliably identify page breaks in:", identifier)
      # Extract the last 20% of the document as a fallback
      start_pos <- max(1, floor(nchar(content) * 0.8)) 
    } else {
      start_pos <- page_breaks[max(1, length(page_breaks) - 4)] 
    }
    end_pos <- nchar(content) 
    
    # Extract potential article
    potential_article <- substr(content, start_pos, end_pos)
    
    # Refine pattern with negative lookbehind
    pattern <- "(?<!Index|Contents|Table of)Perspective\\s*(?:by Raymond Moley)?"
    loc <- str_locate(potential_article, pattern)
    
    if (!is.na(loc[1])) {
      # Adjust start_pos based on "Perspective" location
      start_pos <- max(start_pos, loc[1] - 50) 
      end_pos <- min(nchar(content), loc[1] + 4000) 
      article <- substr(content, start_pos, end_pos)
      
      # Clean up the text
      cleaned <- str_replace_all(article, "\\n\\s*\\n\\s*\\n+", "\n\n")
      cleaned <- str_replace_all(cleaned, "(?<!\\n)\\n(?!\\n)", " ")
      cleaned <- str_replace_all(cleaned, "\\s+", " ")
      cleaned <- trimws(cleaned)
      
      # Write to file
      write(cleaned, file = output_file)
      cat("Saved article to:", output_file, "\n")
      return(TRUE)
    } else {
      cat("No Perspective section found in:", identifier, "\n")
      return(FALSE)
    }
  } else {
    cat("Failed to download:", identifier, "\n")
    return(FALSE)
  }
}

# Main processing function
process_identifiers <- function(identifier_string, output_dir = "articles") {
  urls <- create_archive_urls(identifier_string)
  
  # Process each URL
  results <- sapply(urls, function(url) {
    cat("Processing:", url, "\n")
    extract_and_save_perspective(url, output_dir)
  })
  
  # Summary
  cat("\nProcessing complete!\n")
  cat("Successfully processed:", sum(results), "articles\n")
  cat("Failed to process:", sum(!results), "articles\n")
}

# Example usage
identifier <- "sim_newsweek-us_1945-01-22_25_4,
sim_newsweek-us_1945-02-19_25_8,
sim_newsweek-us_1945-03-05_25_10,
sim_newsweek-us_1945-04-09_25_15,
sim_newsweek-us_1945-05-28_25_22,
sim_newsweek-us_1945-06-25_25_26,
sim_newsweek-us_1945-07-30_26_5,
sim_newsweek-us_1945-08-13_26_7,
sim_newsweek-us_1945-09-10_26_11,
sim_newsweek-us_1945-10-15_26_16,
sim_newsweek-us_1945-11-26_26_22,
sim_newsweek-us_1945-12-03_26_23"

process_identifiers(identifier)
```


#captures some perspective but also grabs index
```{r}
library(httr)
library(stringr)

# Function to create archive.org URLs
create_archive_urls <- function(identifier_string) {
  identifiers <- strsplit(identifier_string, ",")[[1]]
  identifiers <- trimws(identifiers)
  
  urls <- sapply(identifiers, function(id) {
    paste0("https://archive.org/stream/", id, "/", id, "_djvu.txt")
  })
  
  return(urls)
}

# Function to extract and save perspective section
extract_and_save_perspective <- function(url, output_dir = "articles") {
  # Create output directory if it doesn't exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir)
  }
  
  # Extract identifier from URL to use as filename
  identifier <- str_extract(url, "sim_newsweek-us_[\\d-]+_\\d+_\\d+")
  output_file <- file.path(output_dir, paste0(identifier, ".txt"))
  
  read_url <- str_replace(url, "stream", "download")
  headers <- c(
    "User-Agent" = "R-Script/1.0",
    "Accept" = "text/plain"
  )
  
  response <- GET(read_url, add_headers(.headers=headers))
  
  if(status_code(response) == 200) {
    content <- rawToChar(response$content)
    pattern <- "(?i)Persp\\s*ective"
    loc <- str_locate(content, pattern)
    
    if(!is.na(loc[1])) {
      start_pos <- max(1, loc[1] - 100)
      end_pos <- min(nchar(content), loc[1] + 4800)
      article <- substr(content, start_pos, end_pos)
      
      # Clean up the text
      cleaned <- str_replace_all(article, "\\n\\s*\\n\\s*\\n+", "\n\n")
      cleaned <- str_replace_all(cleaned, "(?<!\\n)\\n(?!\\n)", " ")
      cleaned <- str_replace_all(cleaned, "\\s+", " ")
      cleaned <- trimws(cleaned)
      
      # Write to file
      write(cleaned, file = output_file)
      cat("Saved article to:", output_file, "\n")
      return(TRUE)
    } else {
      cat("No Perspective section found in:", identifier, "\n")
      return(FALSE)
    }
  } else {
    cat("Failed to download:", identifier, "\n")
    return(FALSE)
  }
}

# Main processing function
process_identifiers <- function(identifier_string, output_dir = "articles") {
  urls <- create_archive_urls(identifier_string)
  
  # Process each URL
  results <- sapply(urls, function(url) {
    cat("Processing:", url, "\n")
    extract_and_save_perspective(url, output_dir)
  })
  
  # Summary
  cat("\nProcessing complete!\n")
  cat("Successfully processed:", sum(results), "articles\n")
  cat("Failed to process:", sum(!results), "articles\n")
}

# Example usage
identifier <- "sim_newsweek-us_1945-01-22_25_4,
sim_newsweek-us_1945-02-19_25_8,
sim_newsweek-us_1945-03-05_25_10,
sim_newsweek-us_1945-04-09_25_15,
sim_newsweek-us_1945-05-28_25_22,
sim_newsweek-us_1945-06-25_25_26,
sim_newsweek-us_1945-07-30_26_5,
sim_newsweek-us_1945-08-13_26_7,
sim_newsweek-us_1945-09-10_26_11,
sim_newsweek-us_1945-10-15_26_16,
sim_newsweek-us_1945-11-26_26_22,
sim_newsweek-us_1945-12-03_26_23"

process_identifiers(identifier)
```

```{r}
write.csv(urls, "test_1945_urls.csv")
```



```{r}
create_archive_urls <- function(identifier_string) {
  # Split on commas and clean up whitespace
  identifiers <- strsplit(identifier_string, ",")[[1]]
  identifiers <- trimws(identifiers)
  
  # Create URLs for each identifier
  urls <- sapply(identifiers, function(id) {
    paste0("https://archive.org/stream/", id, "/", id, "_djvu.txt")
  })
  
  return(urls)
}

# Example usage with your identifiers
identifier <- "sim_newsweek-us_1945-01-22_25_4,
sim_newsweek-us_1945-02-19_25_8,
sim_newsweek-us_1945-03-05_25_10,
sim_newsweek-us_1945-04-09_25_15,
sim_newsweek-us_1945-05-28_25_22,
sim_newsweek-us_1945-06-25_25_26,
sim_newsweek-us_1945-07-30_26_5,
sim_newsweek-us_1945-08-13_26_7,
sim_newsweek-us_1945-09-10_26_11,
sim_newsweek-us_1945-10-15_26_16,
sim_newsweek-us_1945-11-26_26_22,
sim_newsweek-us_1945-12-03_26_23"

urls <- create_archive_urls(identifier)

# Print each URL on a new line
the_1945_urls <- cat(urls, sep="\n")


```



#searching the scanned text on archive.org
```{r}
library(httr)
library(stringr)

extract_perspective_section <- function(url) {
  read_url <- str_replace(url, "stream", "download")
  headers <- c(
    "User-Agent" = "R-Script/1.0",
    "Accept" = "text/plain"
  )
  
  response <- GET(read_url, add_headers(.headers=headers))
  
  if(status_code(response) == 200) {
    content <- rawToChar(response$content)
    
    # Find the Perspective section
    pattern <- "(?i)Persp\\s*ective"
    loc <- str_locate(content, pattern)
    
    if(!is.na(loc[1])) {
      # Get the full article (starting a bit before to catch the header, and grabbing 1500 words after)
      start_pos <- max(1, loc[1] - 100)  # Start before the match to catch the header
      end_pos <- min(nchar(content), loc[1] + 4800)  # Grab plenty of text to ensure we get 1500 words
      
      article <- substr(content, start_pos, end_pos)
      
      # Clean up the text
      # Remove excessive newlines while preserving paragraph breaks
      cleaned <- str_replace_all(article, "\\n\\s*\\n\\s*\\n+", "\n\n")
      # Replace single newlines with spaces
      cleaned <- str_replace_all(cleaned, "(?<!\\n)\\n(?!\\n)", " ")
      # Clean up multiple spaces
      cleaned <- str_replace_all(cleaned, "\\s+", " ")
      cleaned <- trimws(cleaned)
      
      return(cleaned)
    }
  }
  
  return("Article not found")
}

# Test with the URL
url <- "https://archive.org/stream/sim_newsweek-us_1945-09-10_26_11/sim_newsweek-us_1945-09-10_26_11_djvu.txt"
result <- extract_perspective_section(url)
cat(result, "\n")
```



# fails because pdf text isn't working properly
```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"

# Create output directory if it doesn't exist
#dir_create(output_folder)

# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Try to safely read the PDF
    text <- tryCatch({
      pdf_text(pdf_path)
    }, error = function(e) {
      # If pdf_text fails, try with different encoding
      message("Attempting alternate PDF reading method for: ", basename(pdf_path))
      pdf_data <- pdf_subset(pdf_path, pages = NULL)  # This creates a temporary copy
      pdf_text(pdf_data)
    })
    
    # Only look at the final 10 pages of the PDF
    total_pages <- length(text)
    if(total_pages > 10) {
      text <- text[(total_pages-10):total_pages]
      offset <- total_pages - 11  # To adjust page numbers later
    } else {
      offset <- 0
    }
    
    # Find pages with "Moley" in the subset
    page_nums <- which(str_detect(text, regex("Moley", ignore_case = FALSE)) |
                      str_detect(text, regex("by Raymond Moley", ignore_case = FALSE)))
    
    # Get the last instance and adjust page number back to original PDF
    if (length(page_nums) > 0) {
      last_page <- max(page_nums) + offset
      
      # Try to extract the page with error handling
      tryCatch({
        output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
        pdf_subset(pdf_path, pages = last_page, output = output_file)
        return(paste("Last Moley page extracted successfully from", basename(pdf_path), "! Page number:", last_page))
      }, error = function(e) {
        return(paste("Failed to extract page from", basename(pdf_path), "- Error:", e$message))
      })
    } else {
      return(paste("Moley not found in final pages of", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file with additional error information
results <- sapply(pdf_files, function(pdf_file) {
  message("Processing: ", basename(pdf_file))
  extract_perspective(pdf_file)
})

# Print results
for (result in results) {
  cat(result, "\n")
}
```


```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"

# Create output directory if it doesn't exist
#dir_create(output_folder)

# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Extract all text
    text <- pdf_text(pdf_path)
    
    # Find pages with required terms but exclude those with Weekly Publications text
    page_num <- which(
      str_detect(text, regex("Perspective", ignore_case = FALSE)) &
      str_detect(text, regex("Patent Office", ignore_case = FALSE)) &
      str_detect(text, regex("by Raymond Moley", ignore_case = FALSE)) &
      !str_detect(text, regex("published weekly by WEEKLY PUBLICATIONS, INC.", ignore_case = TRUE))
    )
    
    # Extract the specific page
    if (length(page_num) > 0) {
      output_file <- file.path(output_folder, paste0("Extracted_perspective_", basename(pdf_path)))
      pdf_subset(pdf_path, pages = page_num, output = output_file)
      return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
    } else {
      return(paste("Perspective not found in", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file
results <- sapply(pdf_files, extract_perspective)

# Print results
for (result in results) {
  cat(result, "\n")
}
```



# Extracts the Moley column from a folder of pdfs

```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "new_scans/source_pdfs"
output_folder <- "new_scans/cropped"

# Create output directory if it doesn't exist
dir_create(output_folder)
# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Extract all text
    text <- pdf_text(pdf_path)
    
    # Find page with both "perspective" and "Moley"
    page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
                      str_detect(text, regex("Moley", ignore_case = FALSE))  |
                      str_detect(text, regex("by Raymond", ignore_case = FALSE)))
    
    # Extract the specific page
    if (length(page_num) > 0) {
      output_file <- paste0("Extracted_perspective_", basename(pdf_path))
      pdf_subset(pdf_path, pages = page_num, output = output_file)
      return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
    } else {
      return(paste("Perspective not found in", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file
results <- sapply(pdf_files, extract_perspective)

# Print results
for (result in results) {
  cat(result, "\n")
}
```


# Scrapes and extracts the Moley column
```{r}
library(pdftools)
library(stringr)
library(httr)

# url <- "https://archive.org/download/sim_newsweek-us_1956-01-30_47_5/sim_newsweek-us_1956-01-30_47_5.pdf"
# temp_pdf <- tempfile(fileext = ".pdf")

url <- "https://archive.org/download/sim_newsweek-us_1960-07-18_56_3/sim_newsweek-us_1960-07-18_56_3.pdf"
temp_pdf <- tempfile(fileext = ".pdf")

# Retry download if it fails
max_retries <- 3
retry_count <- 0

download_file <- function(url, destfile) {
  response <- GET(url, write_disk(destfile, overwrite = TRUE),
                  timeout(300), # Increase timeout to 5 minutes
                  progress())
  return(response$status_code == 200)
}

while (retry_count < max_retries) {
  tryCatch({
    success <- download_file(url, temp_pdf)
    
    if (success && file.exists(temp_pdf) && file.info(temp_pdf)$size > 1000000) { # Check if file is larger than 1MB
      message("Download successful")
      break
    } else {
      stop("Download incomplete or file too small")
    }
  }, error = function(e) {
    retry_count <<- retry_count + 1
    if (retry_count < max_retries) {
      message(paste("Download failed. Attempt", retry_count, "of", max_retries))
      Sys.sleep(5) # Wait 5 seconds before retrying
    } else {
      stop("Download failed after maximum retries.")
    }
  })
}

if (file.exists(temp_pdf)) {
  # Extract all text
  text <- pdf_text(temp_pdf)
  
     # Find page with both "perspective" and "Moley"
    page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
                      str_detect(text, regex("Moley", ignore_case = FALSE))  |
                      str_detect(text, regex("by Raymond", ignore_case = FALSE)))
  
  # Extract the specific page
  if (length(page_num) > 0) {
    pdf_subset(temp_pdf, pages = page_num, output = "Moley_column.pdf")
    print(paste("Perspective page extracted successfully! Page number:", page_num))
  } else {
    print("Perspective by Raymond Moley not found in the document.")
  }
} else {
  print("Failed to download the PDF file.")
}

# Clean up
unlink(temp_pdf)
```




# Scrape Wiki page for racist terms

## Thanks to Sean Mussenden for his Advanced rvest tutorial, which this is shamelessly and ruthlessly stolen and remixed

https://github.com/smussenden/datajournalismbook


```{r}
library(tidyverse)
library(rvest)
library(janitor)
```


Example URL of pages I want to scrape
https://oac.cdlib.org/view?docId=tf6779n7xj&view=dsc&style=oac4&dsc.position=1
Test
```{r}
#this pulls in data from a specific site

url2 <- "https://oac.cdlib.org/view?docId=tf6779n7xj&view=dsc&style=oac4&dsc.position=1"
test  <- url2 %>%
  read_html() %>%
  html_table() 

test1 <- test[[1]] %>%
  clean_names() 

#I want to create an empty container and loop it 1-24 to capture all of the data


DT <- tibble::enframe(test)

DT2 <- tidyr::unnest(DT)

DT3 <- DT2 %>% 
  select(1:5) %>% 
  clean_names() %>% 
  filter(!is.na(term)) 

#write.csv(DT3, "../output/racistterms_all.csv")

racist <- filter(DT3, grepl("United States", location_or_origin))
racist1 <- filter(racist, grepl("Black|African", targets))
# write.csv(racist1, "../output/racist_terms_us_may3.csv")

racist_dictionary <- rio::import("../output/racist_dictionary.csv")

```


# NOTES BELOW: FAILED ATTEMPT TO SCRAPE THE WIKIPAGE
```{r}
page <- read_html("https://en.wikipedia.org/wiki/List_of_ethnic_slurs")
table <- page %>% html_node("table") %>% html_table() %>% 
    clean_names() 

slurs <- data.frame()

# Scrape the page for each letter from A to Z
for (i in LETTERS) {
  # Use tryCatch to catch errors
  tryCatch({
    page <- read_html(paste0("https://en.wikipedia.org/wiki/List_of_ethnic_slurs_(%22", i, "%22)"))
    table <- page %>% html_node("table") %>% html_table()
    slur_letter <- table %>%
      slice(-1) %>% # Remove the header row
      select(term, location_or_origin, targets, meaning_origin_and_notes) %>%
      filter(!is.na(term)) # Remove rows with missing slurs
    slurs <- rbind(slurs, slur_letter) # Add to the main data frame
  }, error = function(e){})
}


```

#notes from previous scraper

```{r}
#this pulls in data from a specific site

url2 <- "https://www.lotteryinsider.com/lottery/arkansas.htm"
test  <- url2 %>%
  read_html() %>%
  html_table()

test <- test[[1]] %>%
  clean_names() %>%
  slice(4:30) %>% 
  select(x1, x2)

arkansas <- test
```


#Following the advancedrevest tutorial

```{r}
# Define parent url of page we want to scrape

url <- "https://www.lotteryinsider.com/lottery/index.htm#us"

# Read in all html from table, store all tables on page as nested list of dataframes.
lottery_industry  <- url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row
```

#build a list of the states, urls
```{r}
lottery_industry <- lottery_industry[[1]] %>%
  clean_names() %>%
  slice(150:205)

states <- lottery_industry %>% 
  select(x1) %>% 
  rename(state = x1)
  
states$state <- tolower(states$state)
states$state2 <- states$state

#rename to align with state abbreviations on lottery website
states <- states %>%
    mutate(state = case_when(
    str_detect(state, "california") ~ "califor",   
    str_detect(state, "connecticut") ~ "connect",                         
    str_detect(state, "district of columbia") ~ "dc", 
    str_detect(state, "indiana") ~ "hoosier",
    str_detect(state, "louisiana") ~ "louisana",
    str_detect(state, "new hampshire") ~ "newham",
    str_detect(state, "new jersey") ~ "njersey",
    str_detect(state, "new mexico") ~ "newmex",
    str_detect(state, "new york") ~ "newyork",
    str_detect(state, "north carolina") ~ "ntcarol",
    str_detect(state, "north dakota") ~ "ntdakota",
    str_detect(state, "massachusetts") ~ "massach",
    str_detect(state, "minnesota") ~ "minnesot",
    str_detect(state, "mississippi") ~ "missip",
    str_detect(state, "pennsylvania") ~ "penvania",
    str_detect(state, "rhode island") ~ "rhode",
    str_detect(state, "south carolina") ~ "carol",
    str_detect(state, "south dakota") ~ "sdakota",
    str_detect(state, "virgin islands") ~ "virgin",
    str_detect(state, "washington") ~ "wash",
    str_detect(state, "west virginia") ~ "westvirg",
    str_detect(state, "wisconsin") ~ "wiscon",
    TRUE ~ state
  ))


#Eliminate jurisdictions

junk <- c("atlantic", "british columbia", "manitoba", "ontario", "quebec", "western canada", "lottery associations")

states <- states %>% 
  filter(!state %in% junk)

```
Example URL
https://www.lotteryinsider.com/lottery/arizona.htm

```{r}

# Make a column with URL for each sector. 
state_links <- states %>%
  mutate(sector_url = paste0("https://www.lotteryinsider.com/lottery/",states$state,".htm"))

# Display it
state_links

state_links2 <- state_links %>% 
  select(sector_url)
```

#xpath
#I don't see a standardized  xpath structure to use to build the scraper.

```{r}
#/html/body/table/tbody/tr/td/table/tbody/tr[2]/td[2]/table[2]/tbody/tr/td/table[12]

# Define url of the page we want to get
url2 <- "https://www.lotteryinsider.com/lottery/index.htm#us"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
lottery_info2 <- url2 %>%
  read_html() %>%
  html_element(xpath = '//*["/html/body/table/tbody/tr/td/table/tbody/tr[2]/td[2]/table[2]/tbody/tr/td/table[12]/tbody/tr[4]"]') #%>%
 # html_table() 


```


#Scraping the lottery website
```{r}
# For loop, iterating over each row 

for(row_number in 1:nrow(state_links2)) {
    
    # Keep only the row for a given row number, get rid of every other row
   each_row_df <- state_links2 %>%
    slice(row_number) 
      
    # Define url of page to get
   url <- each_row_df$sector_url
    
    # Define id of table to ingest
# xpath_lottery_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
  lottery_info <- url %>%
    read_html() %>%
    html_table() 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
  print(lottery_info)
    
}      
```

To this point, lottery_info has loaded everything into a list file
--it seems the list just contains the last value scraped, wyomong, and none of the other states.
