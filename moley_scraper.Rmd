---
output:
  html_document: default
  pdf_document: default
---

# Scrapes and extracts the Moley column
```{r}
library(pdftools)
library(stringr)
library(httr)

url <- "https://archive.org/download/sim_newsweek-us_1956-01-30_47_5/sim_newsweek-us_1956-01-30_47_5.pdf"
temp_pdf <- tempfile(fileext = ".pdf")

# Retry download if it fails
max_retries <- 3
retry_count <- 0

download_file <- function(url, destfile) {
  response <- GET(url, write_disk(destfile, overwrite = TRUE),
                  timeout(300), # Increase timeout to 5 minutes
                  progress())
  return(response$status_code == 200)
}

while (retry_count < max_retries) {
  tryCatch({
    success <- download_file(url, temp_pdf)
    
    if (success && file.exists(temp_pdf) && file.info(temp_pdf)$size > 1000000) { # Check if file is larger than 1MB
      message("Download successful")
      break
    } else {
      stop("Download incomplete or file too small")
    }
  }, error = function(e) {
    retry_count <<- retry_count + 1
    if (retry_count < max_retries) {
      message(paste("Download failed. Attempt", retry_count, "of", max_retries))
      Sys.sleep(5) # Wait 5 seconds before retrying
    } else {
      stop("Download failed after maximum retries.")
    }
  })
}

if (file.exists(temp_pdf)) {
  # Extract all text
  text <- pdf_text(temp_pdf)
  
  # Find page with "Perspective by Ray Moley"
  page_num <- which(str_detect(text, regex("Perspective", ignore_case = TRUE)))
  
  # Extract the specific page
  if (length(page_num) > 0) {
    pdf_subset(temp_pdf, pages = page_num, output = "perspective_page.pdf")
    print(paste("Perspective page extracted successfully! Page number:", page_num))
  } else {
    print("Perspective by Raymond Moley not found in the document.")
  }
} else {
  print("Failed to download the PDF file.")
}

# Clean up
unlink(temp_pdf)
```

# Extracts the Moley column from a folder of pdfs

```{r}
library(pdftools)
library(stringr)
library(fs)

# Specify the folder containing the PDF files
pdf_folder <- "/Users/robwells/Downloads/test"

# Get a list of all PDF files in the folder
pdf_files <- dir_ls(pdf_folder, glob = "*.pdf")

# Function to extract perspective page
extract_perspective <- function(pdf_path) {
  tryCatch({
    # Extract all text
    text <- pdf_text(pdf_path)
    
    # Find page with both "perspective" and "Moley"
    page_num <- which(str_detect(text, regex("Perspective", ignore_case = FALSE)) &
                      str_detect(text, regex("Moley", ignore_case = FALSE))  |
                      str_detect(text, regex("by Raymond", ignore_case = FALSE)))
    
    # Extract the specific page
    if (length(page_num) > 0) {
      output_file <- paste0("Moley_column_", basename(pdf_path))
      pdf_subset(pdf_path, pages = page_num, output = output_file)
      return(paste("Perspective page extracted successfully from", basename(pdf_path), "! Page number:", page_num))
    } else {
      return(paste("Perspective not found in", basename(pdf_path)))
    }
  }, error = function(e) {
    return(paste("Error processing", basename(pdf_path), ":", e$message))
  })
}

# Process each PDF file
results <- sapply(pdf_files, extract_perspective)

# Print results
for (result in results) {
  cat(result, "\n")
}
```


# Scrape Wiki page for racist terms

## Thanks to Sean Mussenden for his Advanced rvest tutorial, which this is shamelessly and ruthlessly stolen and remixed

https://github.com/smussenden/datajournalismbook


```{r}
library(tidyverse)
library(rvest)
library(janitor)
```


Example URL of pages I want to scrape
https://oac.cdlib.org/view?docId=tf6779n7xj&view=dsc&style=oac4&dsc.position=1
Test
```{r}
#this pulls in data from a specific site

url2 <- "https://oac.cdlib.org/view?docId=tf6779n7xj&view=dsc&style=oac4&dsc.position=1"
test  <- url2 %>%
  read_html() %>%
  html_table() 

test1 <- test[[1]] %>%
  clean_names() 

#I want to create an empty container and loop it 1-24 to capture all of the data


DT <- tibble::enframe(test)

DT2 <- tidyr::unnest(DT)

DT3 <- DT2 %>% 
  select(1:5) %>% 
  clean_names() %>% 
  filter(!is.na(term)) 

#write.csv(DT3, "../output/racistterms_all.csv")

racist <- filter(DT3, grepl("United States", location_or_origin))
racist1 <- filter(racist, grepl("Black|African", targets))
# write.csv(racist1, "../output/racist_terms_us_may3.csv")

racist_dictionary <- rio::import("../output/racist_dictionary.csv")

```


# NOTES BELOW: FAILED ATTEMPT TO SCRAPE THE WIKIPAGE
```{r}
page <- read_html("https://en.wikipedia.org/wiki/List_of_ethnic_slurs")
table <- page %>% html_node("table") %>% html_table() %>% 
    clean_names() 

slurs <- data.frame()

# Scrape the page for each letter from A to Z
for (i in LETTERS) {
  # Use tryCatch to catch errors
  tryCatch({
    page <- read_html(paste0("https://en.wikipedia.org/wiki/List_of_ethnic_slurs_(%22", i, "%22)"))
    table <- page %>% html_node("table") %>% html_table()
    slur_letter <- table %>%
      slice(-1) %>% # Remove the header row
      select(term, location_or_origin, targets, meaning_origin_and_notes) %>%
      filter(!is.na(term)) # Remove rows with missing slurs
    slurs <- rbind(slurs, slur_letter) # Add to the main data frame
  }, error = function(e){})
}


```

#notes from previous scraper

```{r}
#this pulls in data from a specific site

url2 <- "https://www.lotteryinsider.com/lottery/arkansas.htm"
test  <- url2 %>%
  read_html() %>%
  html_table()

test <- test[[1]] %>%
  clean_names() %>%
  slice(4:30) %>% 
  select(x1, x2)

arkansas <- test
```


#Following the advancedrevest tutorial

```{r}
# Define parent url of page we want to scrape

url <- "https://www.lotteryinsider.com/lottery/index.htm#us"

# Read in all html from table, store all tables on page as nested list of dataframes.
lottery_industry  <- url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row
```

#build a list of the states, urls
```{r}
lottery_industry <- lottery_industry[[1]] %>%
  clean_names() %>%
  slice(150:205)

states <- lottery_industry %>% 
  select(x1) %>% 
  rename(state = x1)
  
states$state <- tolower(states$state)
states$state2 <- states$state

#rename to align with state abbreviations on lottery website
states <- states %>%
    mutate(state = case_when(
    str_detect(state, "california") ~ "califor",   
    str_detect(state, "connecticut") ~ "connect",                         
    str_detect(state, "district of columbia") ~ "dc", 
    str_detect(state, "indiana") ~ "hoosier",
    str_detect(state, "louisiana") ~ "louisana",
    str_detect(state, "new hampshire") ~ "newham",
    str_detect(state, "new jersey") ~ "njersey",
    str_detect(state, "new mexico") ~ "newmex",
    str_detect(state, "new york") ~ "newyork",
    str_detect(state, "north carolina") ~ "ntcarol",
    str_detect(state, "north dakota") ~ "ntdakota",
    str_detect(state, "massachusetts") ~ "massach",
    str_detect(state, "minnesota") ~ "minnesot",
    str_detect(state, "mississippi") ~ "missip",
    str_detect(state, "pennsylvania") ~ "penvania",
    str_detect(state, "rhode island") ~ "rhode",
    str_detect(state, "south carolina") ~ "carol",
    str_detect(state, "south dakota") ~ "sdakota",
    str_detect(state, "virgin islands") ~ "virgin",
    str_detect(state, "washington") ~ "wash",
    str_detect(state, "west virginia") ~ "westvirg",
    str_detect(state, "wisconsin") ~ "wiscon",
    TRUE ~ state
  ))


#Eliminate jurisdictions

junk <- c("atlantic", "british columbia", "manitoba", "ontario", "quebec", "western canada", "lottery associations")

states <- states %>% 
  filter(!state %in% junk)

```
Example URL
https://www.lotteryinsider.com/lottery/arizona.htm

```{r}

# Make a column with URL for each sector. 
state_links <- states %>%
  mutate(sector_url = paste0("https://www.lotteryinsider.com/lottery/",states$state,".htm"))

# Display it
state_links

state_links2 <- state_links %>% 
  select(sector_url)
```

#xpath
#I don't see a standardized  xpath structure to use to build the scraper.

```{r}
#/html/body/table/tbody/tr/td/table/tbody/tr[2]/td[2]/table[2]/tbody/tr/td/table[12]

# Define url of the page we want to get
url2 <- "https://www.lotteryinsider.com/lottery/index.htm#us"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
lottery_info2 <- url2 %>%
  read_html() %>%
  html_element(xpath = '//*["/html/body/table/tbody/tr/td/table/tbody/tr[2]/td[2]/table[2]/tbody/tr/td/table[12]/tbody/tr[4]"]') #%>%
 # html_table() 


```


#Scraping the lottery website
```{r}
# For loop, iterating over each row 

for(row_number in 1:nrow(state_links2)) {
    
    # Keep only the row for a given row number, get rid of every other row
   each_row_df <- state_links2 %>%
    slice(row_number) 
      
    # Define url of page to get
   url <- each_row_df$sector_url
    
    # Define id of table to ingest
# xpath_lottery_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')
    
    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
  lottery_info <- url %>%
    read_html() %>%
    html_table() 
    
    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
  print(lottery_info)
    
}      
```

To this point, lottery_info has loaded everything into a list file
--it seems the list just contains the last value scraped, wyomong, and none of the other states.
