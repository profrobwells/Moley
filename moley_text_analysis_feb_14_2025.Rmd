---
title: "Content Analysis of Raymond Moleyâ€™s Reporting from 1937 - 1968"
author: "Rob Wells and Bridget Lang"
date: '2025-2-14'
output: html_document
---

This project aims to perform a basic narrative and topic analysis of the journalism of **Raymond Moley**, a prominent columnist who advocated a centrist conservative political vision from 1937 through 1967 in his books, weekly Newsweek column and syndicated newspaper column. 

The goal of this project is to **measure patterns in wording, verbiage, themes, and sentiment** of Raymond Moley's poltitical reporting over time. This research will support a larger effort in analyzing the **influence of journalism and polticial activism on American elections, voting, and democracy at large**. 

I am focusing on the time period of Moley's work from **1937-1968**, with **1400 articles**  analyzed. 

The index has 1562 articles
The earliest year of publication is 1937, and the latest is 1967.
The average year of publication is 1952, with the majority of articles written in 1938.
The article text has 1487 articles. That's less than the 1562 entries in the article index. The difference was due to about 75 failed AI scans.

# Load Libraries
```{r message=FALSE, warning=FALSE, include=FALSE}


library(textdata)
library(tidyverse)
library(pdftools)
library(dplyr)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
library(formattable)
library(forcats)
library(readtext)
#topic modeling
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
library(janitor)
```

## Load Data
```{r echo = F}
#1562 articles in index
article_index <-  rio::import("matching with extract-Perspective_full_index_1967_1937.xls") |> 
  clean_names() |> 
  mutate(year = as.numeric(year),
         date = as.Date(pubdate, format="%b %d, %Y"))

#136596 rows of text
articles_text <-  read_csv("/Users/gizmofo/Library/CloudStorage/Dropbox/Current_Projects/Moley project 2024/moley_cleaned_perspective_text.csv") |> 
    mutate(year = as.numeric(year),
         date = as.Date(pubdate, format="%b %d, %Y"))
```
## Statistics about the index 
```{r}

nrows <- nrow(article_index)
ncols <- ncol(article_index)

mean_year <- round(mean(article_index$year, na.rm = TRUE),0)

min_year <- min(article_index$year)

max_year <- max(article_index$year)


year_counts <- 
  article_index %>%
  count(year)

max_count <- max(year_counts$n)
max_years <- year_counts %>%
  select(year) %>%
  filter(year_counts$n == max_count)
                  

glue::glue("There are {nrows} articles in the index of Moley Newsweek columns.");

glue::glue("The earliest year of publication is {as.integer(min_year)}, and the latest is {max_year}.")

glue::glue("The average year of publication is {as.integer(mean_year)}, with the majority of articles written in {max_years$year}.")




```

## Statistics about the extracted text
```{r echo = F}
nrows1 <- nrow(articles_text)
ncols1 <- ncol(articles_text)

articles <- articles_text |> 
  distinct(filename) |> 
  count()


glue::glue("The article text has {articles} articles. That's less than the {nrows} entries in the article index. The difference was due to about {nrows-articles} failed AI scans.")
glue::glue("The article text dataframe is huge, with {nrows1} rows and {ncols1} columns.")

```


# Figure x: Articles over time

```{r}

#Here is a chart, Figure x, that counts columns by year
count_year <- articles_text %>% 
  distinct(filename, .keep_all = TRUE) |> 
count(year) %>% 
#Sandwich it onto a simple ggplot
  ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  scale_x_continuous(breaks = seq(1935, 1970, by = 5))+
  labs(title = "Moley Newsweek Columns By Year, 1937-1967", 
       subtitle = "Extracted Text Only",
       caption = "n=1,487  articles. Graphic by Rob Wells, 2/14/2025",
       y="Count of Pages",
       x="Year")

count_year

#ggsave(here::here("Figure_X_ columns_by_year_2_14_2025.png"),device = "png",width=9,height=6, dpi=800)

```


# Bigrams
```{r}
bigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "raymond", "")) %>% 
  mutate(text = str_replace_all(text, "newsweek", "")) %>% 
  mutate(text = str_replace_all(text, "image", "")) %>%
  mutate(text = str_replace_all(text, "perspective", "")) %>%
  mutate(text = str_replace_all(text, "registered u.s. patent office", "")) %>%
  mutate(text = str_replace_all(text, "- ", "")) %>%
  mutate(text = str_replace_all(text, " -", "")) %>%
  mutate(text = str_replace_all(text, " - ", "")) %>%
  unnest_tokens(word, text, token="ngrams", n=2 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word == "minor inaccuracies") %>%
  filter(!word == "text extraction") %>%
  filter(!word == "text version") %>%
    filter(!is.na(word))

bigrams <- bigrams %>%
  select(word, date, year, filename)

```

## Datatable with bigrams
```{r}
bigrams_separated <- bigrams %>%
  separate(word, c("word1", "word2"), sep = " ")

#bigrams with stop words filtered

bigrams_filtered <- 
  bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

datatable(bigram_counts,
          caption = "Top Bigrams",
          options = list(pageLength = 20))

```



There are 192 word pairs containing "Nixon."
There are 300 word pairs containing "Dewey."
There are 139 word pairs containing "Goldwater."


```{r}
  
top_20_bigrams <- bigram_counts |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)

ggplot(top_20_bigrams, aes(n, reorder(bigram, n), fill=n)) +
  geom_bar(stat="identity", position="dodge") +
  theme(legend.position="none") +
  geom_text(aes(label=n), hjust=-0.2, size=3) +
  labs(title = "Raymond Moley Common Phrases, 1937-1967",
       subtitle = "Analysis of 1,487 Newsweek columns",
       x = "Count",
       y = "Bigrams",
       caption = "Source: Newsweek. Graphic by Rob Wells and Bridget Lang, 2-14-2024")
```


Counting Word instances
```{r}


articles_text %>%
  # First ensure sentence is character and lowercase
  mutate(sentence = as.character(sentence),
         sentence = tolower(sentence)) %>%
  # Group by filename to avoid double-counting articles
  group_by(filename) %>%
  summarise(
    nixon = sum(str_count(sentence, "nixon"), na.rm = TRUE),
    goldwater = sum(str_count(sentence, "goldwater"), na.rm = TRUE),
    dewey = sum(str_count(sentence, "dewey"), na.rm = TRUE),
    eisenhower = sum(str_count(sentence, "eisenhower"), na.rm = TRUE)
  ) %>%
  # Now get totals across all articles
  summarise(
    nixon = sum(nixon),
    goldwater = sum(goldwater),
    dewey = sum(dewey),
    eisenhower = sum(eisenhower)
  ) %>%
  pivot_longer(
    everything(),
    names_to = "politician",
    values_to = "count"
  ) %>%
  mutate(
    percentage = count / 1487) * 100  # Using actual number of articles
  ) %>%
  arrange(desc(count))
```

## Business Coverage

```{r}
business_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "business"), na.rm = TRUE)) %>%
  filter(mentions >= 2)
```


```{r}

business_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "business")) |> 
  

print(paste("Total number of sentences mentioning business:", nrow(business_mentions)))
```

#### Statism
```{r}
statism<- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
   summarize(mentions = sum(str_detect(sentence, "statism"), na.rm = TRUE)) %>%
   filter(mentions >= 1) |> 
  arrange(date)
```

The search shows Moley used the term "statism" in about 27 different Perspective columns, beginning in 1941.


#### KWIC search, with metadata

```{r}
# 


# Create corpus
my_corpus <- corpus(articles_text, text_field = "sentence")
# Tokenize 
my_tokens <- tokens(my_corpus)
# KWIC with 20 words on each side
business_kwic <- kwic(my_tokens, "business", window = 100)

# quanteda_test2 <- kwic(my_tokens, phrase(c("journalist", "reporter", "newspaper", "killer", "murdered", "killed", "investigate crimes against")), window = 50, valuetype = "regex") %>% as.data.frame() 

kwic_biz <- data.frame(
  docname = business_kwic$docname,
 filename = docvars(my_corpus, "filename")[as.numeric(gsub("text", "", business_kwic$docname))],
  year = docvars(my_corpus, "year")[as.numeric(gsub("text", "", business_kwic$docname))],
  pre = business_kwic$pre,
  keyword = business_kwic$keyword,
  post = business_kwic$post,
  stringsAsFactors = FALSE
) |> 
  filter(str_detect(pre, "politic")) |> 
  arrange(year)

write.table(kwic_biz, "business_kwic.txt", 
            sep = "\t", 
            row.names = FALSE, 
            quote = FALSE,
            fileEncoding = "UTF-8")

```


### KWIC Conservative Group Mentions
```{r}
# Create corpus
my_corpus <- corpus(articles_text, text_field = "sentence")
# Tokenize 
my_tokens <- tokens(my_corpus)
# KWIC with 20 words on each side
#groups_kwic <- kwic(my_tokens, "nixon", window = 100)

groups_kwic <- kwic(my_tokens, phrase(c("National Association of Manufacturers", "DuPont", "John Birch Society", "Liberty League", "Americans for Constitutional Action", "Buckley", "Freeman", "National Review", "Chamber of Commerce", "Milton Friedman", "Hayek")), window = 100, valuetype = "regex") %>% as.data.frame() 

groups_kwic <- data.frame(
  docname = groups_kwic$docname,
 filename = docvars(my_corpus, "filename")[as.numeric(gsub("text", "", groups_kwic$docname))],
  year = docvars(my_corpus, "year")[as.numeric(gsub("text", "", groups_kwic$docname))],
  pre = groups_kwic$pre,
  keyword = groups_kwic$keyword,
  post = groups_kwic$post,
  stringsAsFactors = FALSE
) 

write.table(groups_kwic, "groups_kwic.txt", 
            sep = "\t", 
            row.names = FALSE, 
            quote = FALSE,
            fileEncoding = "UTF-8")
```






## Presidential Candidate Coverage
###Nixon
```{r}
nixon_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "Nixon"), na.rm = TRUE)) %>%
  filter(mentions >= 2) |> 
  mutate(politician = "Nixon")

# Let's also check the intermediate steps to debug:
# First check if we're finding any Nixon mentions at all
nixon_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "Nixon"))

print(paste("Total number of sentences mentioning Nixon:", nrow(nixon_mentions)))
```
###Goldwater
```{r}
goldwater_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "Goldwater"), na.rm = TRUE)) %>%
  filter(mentions >= 2) |> 
    mutate(politician = "Goldwater")

# Let's also check the intermediate steps to debug:
# First check if we're finding any Nixon mentions at all
goldwater_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "Goldwater"))

print(paste("Total number of sentences mentioning Goldwater:", nrow(goldwater_mentions)))
```
###Roosevelt
```{r}
roosevelt_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "Roosevelt"), na.rm = TRUE)) %>%
  filter(mentions >= 2) |> 
    mutate(politician = "Roosevelt")

# Let's also check the intermediate steps to debug:
# First check if we're finding any Nixon mentions at all
roosevelt_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "Roosevelt"))

print(paste("Total number of sentences mentioning Roosevelt:", nrow(roosevelt_mentions)))
```
###Eisenhower
```{r}
eisenhower_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
  group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "Eisenhower"), na.rm = TRUE)) %>%
  filter(mentions >= 2) |> 
    mutate(politician = "Eisenhower")

# Let's also check the intermediate steps to debug:
# First check if we're finding any Nixon mentions at all
eisenhower_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "Eisenhower"))

print(paste("Total number of sentences mentioning eisenhower:", nrow(eisenhower_mentions)))
```
###Dewey
```{r}
dewey_articles2 <- articles_text %>%
  filter(!is.na(sentence)) %>%
   group_by(filename, year, date) %>%
  summarize(mentions = sum(str_detect(sentence, "Dewey"), na.rm = TRUE)) %>%
  filter(mentions >= 2) |> 
    mutate(politician = "Dewey")

# Let's also check the intermediate steps to debug:
# First check if we're finding any Nixon mentions at all
dewey_mentions <- articles_text %>%
  filter(!is.na(sentence)) %>%
  filter(str_detect(sentence, "Dewey"))

print(paste("Total number of sentences mentioning dewey:", nrow(dewey_mentions)))
```

```{r}
politicians <- rbind(nixon_articles2, goldwater_articles2, roosevelt_articles2, dewey_articles2, eisenhower_articles2)

#write.csv(politicians, "politicians_mentions_feb21.csv")
```
###Candidates visualized
```{r}
politicians |> 
  group_by(year, politician) |> 
  summarize(total = sum(mentions)) |> 
ggplot(aes(y=total, x= year, fill=politician)) +
  geom_bar(stat="identity", position="dodge") +
  scale_x_continuous(breaks = seq(1936, 1968, by = 2))+
  labs(title = "Moley's Coverage of Major Candidates",
       subtitle = "Two or more mentions of politician in Newsweek",
        y = "Count of Politician Mentions", 
        x = "",
        caption = "n=1,487 Newsweek articles, Graphic by Rob Wells") +
  theme(axis.text.x = element_text(angle = 45, vjust=0.5)) 


```




```{r echo = F}
nixon_articles <- articles_text %>%
  filter(str_detect(sentence, "Nixon")) %>%
  select(filename, year) %>%
  distinct(filename, year)


goldwater_articles <- articles_text %>%
  filter(str_detect(sentence, "Goldwater")) %>%
  select(filename, year) %>%
  distinct(filename, year)

roosevelt_articles <- articles_text %>%
  filter(str_detect(sentence, "Roosevelt")) %>%
  select(filename, year) %>%
  distinct(filename, year)

dewey_articles <- articles_text %>%
  filter(str_detect(sentence, "Dewey")) %>%
  select(filename, year) %>%
  distinct(filename, year)


nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")

```

## Sentiment

### Dewey

```{r echo = F, results = 'hide', error=F, warning=F, message=F}

dewey_text <- articles_text %>%
  select(filename, year, sentence) %>%
  filter(filename %in% dewey_articles$filename)


dewey_text_tokenized <- dewey_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

dewey_sentiments_all <- dewey_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total = round(n/sum(n), digits=4)) %>%
  mutate(president = "Dewey") %>%
  # Reorder sentiment based on pct_total
  mutate(sentiment = reorder(sentiment, -pct_total))

ggplot(dewey_sentiments_all, aes(sentiment, pct_total, fill=pct_total)) +
  geom_bar(stat="identity", position="dodge") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.01)) +  # Format y-axis as percentages
  labs(title = "Sentiment of Thomas Dewey Articles",
        x = "Sentiment",
        y = "Percentage of Total Text", 
        caption = "Newsweek articles, Graphic by Rob Wells and Bridget Lang") +
  theme(axis.text.x = element_text(angle = 45, vjust=0.5),legend.position = "none") 
```

### Dewey Sentiment Over Time

```{r}
dewey_text_tokenized  <- dewey_text |> 
  filter(!is.na(sentence), sentence != "") |>  # ADD THIS LINE
  mutate(sentence = str_replace_all(sentence, "- ", "")) |> 
  unnest_tokens(word, sentence) |> 
  filter(!word %in% stop_words$word) |> 
  filter(!grepl('[0-9]', word)) |> 
  inner_join(afinn_sentiments, relationship = "many-to-many") |> 
  group_by(filename, year) |>  
  summarise(sentiment_score = sum(value), .groups = "drop") |>  
  group_by(year) |> 
  summarise(total_score = sum(sentiment_score))


ggplot(dewey_text_tokenized, aes(x= year, y=total_score, fill=total_score)) +
  geom_bar(stat="identity", position="dodge") +
   scale_x_continuous(breaks = seq(1936, 1968, by = 2))+
  labs(title = "Sentiment of Thomas Dewey Articles Over Time",
        x = "Sentiment",
        y = "Percentage of Total Text", 
        caption = "Newsweek articles, Graphic by Rob Wells. June 25, 2025") +
  theme(axis.text.x = element_text(angle = 90, vjust=0.5),legend.position = "none") 

```




### Nixon

```{r}

nixon_text <- articles_text %>%
  select(filename, year, sentence) %>%
  filter(filename %in% nixon_articles$filename)


nixon_text_tokenized <- nixon_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

nixon_sentiments_all <- nixon_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total = round(n/sum(n), digits=4)) %>%
  mutate(president = "Nixon") %>%
  # Reorder sentiment based on pct_total
  mutate(sentiment = reorder(sentiment, -pct_total))

ggplot(nixon_sentiments_all, aes(sentiment, pct_total, fill=pct_total)) +
  geom_bar(stat="identity", position="dodge") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.01)) +  # Format y-axis as percentages
  labs(title = "Sentiment of Nixon Articles",
        x = "Sentiment",
        y = "Percentage of Total Text", 
        caption = "Newsweek articles, Graphic by Rob Wells and Bridget Lang") +
  theme(axis.text.x = element_text(angle = 45, vjust=0.5),legend.position = "none") 
```

### Nixon Sentiment Over Time

```{r}
nixon_text <- articles_text %>%
  select(filename, year, sentence) %>%
  filter(filename %in% nixon_articles$filename)

nixon_text_tokenized  <- nixon_text |> 
  filter(!is.na(sentence), sentence != "") |>  # ADD THIS LINE
  mutate(sentence = str_replace_all(sentence, "- ", "")) |> 
  unnest_tokens(word, sentence) |> 
  filter(!word %in% stop_words$word) |> 
  filter(!grepl('[0-9]', word)) |> 
  inner_join(afinn_sentiments, relationship = "many-to-many") |> 
  group_by(filename, year) |>  
  summarise(sentiment_score = sum(value), .groups = "drop") |>  
  group_by(year) |> 
  summarise(total_score = sum(sentiment_score))


ggplot(nixon_text_tokenized, aes(x= year, y=total_score, fill=total_score)) +
  geom_bar(stat="identity", position="dodge") +
   scale_x_continuous(breaks = seq(1936, 1968, by = 2))+
  labs(title = "Moley's Coverage of Nixon in 1960 Campaign: Suddenly Positive",
       subtitle = "Sentiment of Moley's Nixon Coverage in Newsweek",
        x = "Sentiment",
        y = "Percentage of Total Text", 
        caption = "Newsweek articles, Graphic by Rob Wells. June 25, 2025") +
  theme(axis.text.x = element_text(angle = 90, vjust=0.5),legend.position = "none") 
```




### Nixon v Dewey Sentiment

```{r}

nixon_dewey_sentiments <- dewey_sentiments_all %>%
  bind_rows(nixon_sentiments_all)


ggplot(nixon_dewey_sentiments, aes(sentiment, pct_total, fill=president)) +
  geom_bar(stat="identity", position="dodge") +
  scale_y_continuous(labels = scales::percent, limits=c(0, .30)) +
  geom_text(aes(label=scales::percent(pct_total, accuracy=0.01), group=president), 
            position = position_dodge(width=0.9),  # Match the dodge width of bars
            angle = 90,                            # Rotate text 90 degrees
            hjust = -0.2,                         # Adjust horizontal position
            size=3) +
  labs(title = "Sentiment Nixon vs. Dewey in Moley Columns",
       x = "Sentiment",
       y = "Percentage of Total Text",
       caption = "Graphic by Bridget Lang and Rob Wells, 2/16/2025") + 
  theme(axis.text.x = element_text(angle = 45, vjust=0.5)) + 
  scale_fill_manual(values = c("forestgreen", "purple"))


```

# Words in headlines
Count the words in the Perspective column headlines
```{r}

headlines_tokens <- article_index |> 
  mutate(text= str_squish(title), 
      text = tolower(text),
      text = str_remove_all(text, "[[:punct:]]")) |> 
  unnest_tokens(word, text, token="ngrams", n=1 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!is.na(word)) |> 
  filter(!word == "perspective") %>%
  select(word, date, year, index)


headline_counts <- headlines_tokens %>%
  count(word, sort = TRUE) %>% 
  filter(!is.na(word))

datatable(headline_counts,
          caption = "Top Words in Moley Headlines",
          options = list(pageLength = 15))


```

Dewey and its variants appeared in headlines 18 times, which would have made it a top 10 term.

Nixon and its variants appeared 16 times in headlines, which would have made it a top 15 term.

Goldwater appeared 7 times. Eisenhower, 3; Wilkie, 0; Roosevelt, 9; Kennedy, 6; Truman, 7
Republican was the 8th most frequent term with 18 mentions; Democrats, 5, around 80th place.


# Articles - Politicians

```{r}




```


# MaxQDA Analysis
Import matrix from hand-coded Newsweek articles

```{r}
maxqda <- read.csv("./data/final_maxqda_matrix.csv") |> 
  janitor::clean_names() |> 
  mutate(date_new = ymd(date))

```

totals of topics 
```{r}
summary_max <- maxqda |> 
  summarize(across(everything(), ~sum(. != ""))) |>
  pivot_longer(cols = everything(), 
               names_to = "Column", 
               values_to = "Count") |>
  arrange(desc(Count))

```

converts responses to binary entries

```{r}
maxqda_binary <- maxqda |>
  mutate(across(-c(date, date_new, document, old_doc_id), 
                ~ifelse(. != "" & !is.na(.), 1, 0))) |> 
  select(-c(date, old_doc_id)) |> 
  mutate(year = year(date_new)) |>
    # Move date_new to the first column
 select(date_new, year, everything())
```

analysis by year    
```{r}
maxqda_binary |> 
  count(year)


```

Join with index
```{r}
maxqda_merged <- maxqda_binary |> 
  inner_join(article_index, by=c("date_new"="date"))

write.csv(maxqda_merged, "./data/maxqda_merged.csv")
```


Join with articles

```{r}
maxqda_articles <- maxqda_binary |> 
  inner_join(articles_text, by=c("date_new" = "date", "year" = "year"))


```



# Extract topics
```{r}
conservative <- maxqda_articles |> 
  select(date_new, year, filename, title, sentence, entry_date, conservative_politics) |> 
  filter(conservative_politics == 1)



```

```{r echo=FALSE}
process_articles <- function(df) {
  # First group by article identifiers and concatenate sentences
  # Using filename as the identifier, but you might need to use other columns
  compiled_df <- df %>%
    group_by(filename) %>%
    summarize(
      # Combine all non-NA sentences
      sentence = paste(na.omit(sentence), collapse = " "),
      # Keep one value for each of the other columns
      # consolidating metadata into single row per article
      title = first(title),
      abstract = first(abstract),
      store_id = first(store_id),
      article_type = first(article_type),
      authors = first(authors),
      copyright = first(copyright),
      document_type = first(document_type),
      entry_date = first(entry_date),
      issn = first(issn),
      issue = first(issue),
      language = first(language),
      language_of_summary = first(language_of_summary),
      pages = first(pages),
      place_of_publication = first(place_of_publication),
      pubdate = first(pubdate),
      pubtitle = first(pubtitle),
      year = first(year),
      volume = first(volume),
      document_url = first(document_url),
      document_features = first(document_features),
      start_page = first(start_page),
      find_a_copy = first(find_a_copy),
      database = first(database),
      date = first(date)
    )
  
  # Remove punctuation and spaces from the sentence column
  compiled_df <- compiled_df %>%
    mutate(sentence = str_replace_all(sentence, "[[:punct:][:space:]]", "")) |> 
    mutate(sentence = tolower(sentence))
  
  return(compiled_df)
}

# Apply the function to your dataframe
processed_llm_text <- process_articles(llm_text)

# Prepare data with filenames and content
articles_for_analysis <- processed_llm_text %>%
  select(filename, sentence) %>%
  mutate(article_data = paste("FILENAME:", filename, "\nCONTENT:", sentence))

# Combine with clear separators
combined_text_with_filenames <- paste(articles_for_analysis$article_data, 
                                    collapse = "\n\n---ARTICLE SEPARATOR---\n\n")
```

# The AI prompt

```{r echo=FALSE}
# Update system prompt to request the specific format you want
chat <- chat_gemini(
  system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
  
  1. If an article contains two or more mentions of President Richard Nixon, classify it as 'nixon'
  2. If an article contains two or more mentions the Vietnam War, classify it as 'vietnam'

  Return ONLY a CSV-formatted result with exactly two columns:
  filename,category
  
  For example:
  filename,category
  article1.txt,nixon
  article2.txt,vietnam
  article3.txt,nixon;vietnam
  
  No additional text, explanations, or summary counts."
)

# Send to the LLM
response <- chat$chat(combined_text_with_filenames)
```

#Process first response --We're taking the chat response and putting it into a dataframe

```{r echo=FALSE}
process_llm_response_to_df <- function(response) {
  # Extract lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Remove markdown code block markers if present
  lines <- lines[!grepl("^```", lines)]
  
  # Initialize vectors for data
  filenames <- c()
  categories <- c()
  
  # Flag to track if we're processing data (after header)
  header_found <- FALSE
  
  for (line in lines) {
    # Skip empty lines
    if (trimws(line) == "") next
    
    # Skip row numbers or other artifacts (lines with asterisks)
    if (grepl("\\*\\*", line)) next
    
    # Check if this is the header line
    if (grepl("filename,category", line, ignore.case = TRUE)) {
      header_found <- TRUE
      next
    }
    
    # Process data lines (only after header is found)
    if (header_found) {
      parts <- strsplit(line, ",")[[1]]
      if (length(parts) >= 2) {
        filenames <- c(filenames, parts[1])
        categories <- c(categories, paste(parts[2:length(parts)], collapse=","))
      }
    }
  }
  
  # Create and return the dataframe
  if (length(filenames) > 0) {
    data.frame(
      filename = filenames,
      category = categories,
      stringsAsFactors = FALSE
    )
  } else {
    # Return empty dataframe with correct structure if no data found
    data.frame(
      filename = character(0),
      category = character(0),
      stringsAsFactors = FALSE
    )
  }
}

result_df <- process_llm_response_to_df(response)

result_df

```

# validate results


```{r echo=FALSE}
nixon <- result_df |> 
  filter(str_detect(category, "nixon")) |> 
  mutate(path = paste0("./perspective_extracted/",filename)) |> 
  mutate(ai_correct = " ",
         ai_wrong = " ",
         unsure = " ",
         notes = " ")

write.csv(nixon, "nixon_ai_verification.csv")


file_paths1 <- nixon$path
nixon_list <- lapply(file_paths1, readLines)
```
# process the results to a single file, separated by file name

```{r echo=FALSE}

write_combined_files <- function(file_list, file_paths, output_file) {
  # Open connection to output file
  con <- file(output_file, "w")
  
  # Loop through each file
  for (i in seq_along(file_list)) {
    # Extract just the filename from the path
    filename <- basename(file_paths[i])
    
    # Write the separator with filename
    writeLines(paste0("=== FILE: ", filename, " ==="), con)
    
    # Write the content of the file
    writeLines(file_list[[i]], con)
    
    # Add a blank line between files (except after the last file)
    if (i < length(file_list)) {
      writeLines("", con)
    }
  }
  
  # Close the connection
  close(con)
  
  # Return a message
  message(paste("Successfully wrote", length(file_list), "files to", output_file))
}

write_combined_files(nixon_list, file_paths1, "combined_nixon_files.txt")
```

Now, open a Google Sheet, import nixon_ai_verification.csv, read the articles and rate the responses

# Dependency parsing

# ---------------------------

# Extracting adjectives, adverbs

# ---------------------------

**From Claude.ai**
This code will:

Install and load necessary packages
Download and load the English language model for POS tagging
Annotate your text with POS tags
Extract only the adjectives
Create two summaries:

Overall frequency count of adjectives across all articles
Frequency count of adjectives by article



The output will show you:

The adjective word (token)
How many times it appears total (frequency)
In how many different articles it appears (articles)



## Dependency parsing
```{r}
# Install and load required packages
if (!require(udpipe)) install.packages("udpipe")
if (!require(dplyr)) install.packages("dplyr")
library(udpipe)
library(tidyverse)
```

```{r}

# Download and load the English language model (only need to do this once)
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
```


```{r}
# Process the text and extract adjectives
# This will create a data frame with POS tags
annotated_text_moley <- udpipe_annotate(udmodel, x = articles_text$sentence, doc_id = articles_text$filename)
annotated_df_moley <- as.data.frame(annotated_text_moley)
```

### Extract adjective-noun pairs

**From claude.ai**
This modified version will:

Extract adjective-noun pairs where the adjective directly modifies the noun (amod dependency relation)
Create a summary showing:

The adjective-noun pair
How many times this pair appears (frequency)
In how many different articles the pair appears (articles)


Optionally show the distribution of pairs by article

The output will show combinations like "big house", "red car", "important issue", etc., along with their frequencies. Note that this will only catch direct adjectival modifications - more complex relationships might need additional parsing rules.


```{r}
adj_noun_pairs_moley <- annotated_df_moley %>%
  # Join the dataframe with itself to connect adjectives to their head words
  inner_join(
    annotated_df_moley %>% select(doc_id, sentence_id, token, token_id, upos),
    by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
  ) %>%
  # Filter for adjectives modifying nouns
  filter(
    dep_rel == "amod" &    # amod = adjectival modifier
    upos.x == "ADJ" &      # first token is adjective
    upos.y == "NOUN"       # head word is noun
  ) %>%
  # Select and rename relevant columns
  select(
    article_id = doc_id,
    sentence_id,
    adjective = token.x,
    noun = token.y
  )

# Count frequencies of adjective-noun pairs
adj_noun_counts_moley <- adj_noun_pairs_moley %>%
  group_by(adjective, noun) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(article_id)
  ) %>%
  arrange(desc(frequency))

top_200_moley <- adj_noun_counts_moley |> 
  head(200)

write.csv(top_200_moley, "./output/top_200_moley_adj_noun_counts.csv")

write.csv(adj_noun_counts_moley, "./output/all_adj_noun_counts_moley.csv")

# datatable(adj_noun_counts_moley,
#           caption = "Top adjectives-noun pairs in Moley Perspective coverage",
#           options = list(pageLength = 50)) 

```

From Claude.ai
Based on my analysis of the top 200 adjective-noun combinations from what appears to be Raymond Moley's writings, here are the 10 major themes:
10 Common Themes from the Data
1. Federal Government Authority
Central focus on federal power, government control, and administrative authority (Federal government, Federal power, political government)
2. Political Parties & Partisanship
Extensive discussion of party politics, Republican and Democratic parties, and political organizations
3. American Identity & Citizenship
Emphasis on the American people, national character, and civic identity (American people, American government, American war)
4. Temporal & Historical Context
Heavy focus on time periods, historical reflection, and duration (many years, past years, long time, last week)
5. Public Opinion & Social Discourse
Discussion of public sentiment, social attitudes, and collective viewpoints (public opinion, public power, public government)
6. National vs. State Authority
Tension between federal and state power, jurisdictional questions (Federal state, national government, own government)
7. War & Foreign Policy
International relations, military conflicts, and diplomatic strategy (American war, foreign policy, war-related terms)
8. Economic Policy & Business
Government's role in economics, federal aid, and business regulation (Federal money, Federal aid, economic matters)
9. Political Leadership & Terms
Focus on political leadership, electoral cycles, and governmental terms (third term, political leadership)
10. Social Groups & Demographics
References to various population segments and social categories (great people, other people, many people)


```{r}
# Load required libraries
install.packages("spacyr")
library(spacyr)
library(dplyr)
library(tidyr)

# Step 1: Install spaCy (this creates the Python environment)
spacy_install()

# Step 2: Download the English model
spacy_download_langmodel(model = "en_core_web_sm")

# Step 3: Initialize spaCy
spacy_initialize(model = "en_core_web_sm")

# Assuming your text data is in a dataframe with columns like 'doc_id' and 'text'
# If your data is in a different format, adjust accordingly

# Extract named entities from your corpus
# Replace 'your_text_data' with your actual dataframe name
entities_moley <- spacy_parse(
  x = articles_text$sentence,  # Your text column
  tag = FALSE,              # Don't need POS tags
  lemma = FALSE,            # Don't need lemmas
  entity = TRUE,            # DO extract entities
  nounphrase = FALSE        # Don't need noun phrases
)

# Extract just the named entities
named_entities <- spacy_extract_entity(
  x =  articles_text$sentence,
  type = "all"  # Extract all entity types
)

# Filter for person entities specifically
person_entities <- named_entities %>%
  filter(ent_type == "PERSON") %>%
  select(doc_id, text, ent_type)

# Count most frequent person names
count_moley_names <- person_entities %>%
  count(text, sort = TRUE) %>%
  head(1000)

write_csv(count_moley_names, "./output/count_moley_names.csv")
```


Asked claude.ai
take this list, clean it up, and group similar names such as nixon to richard nixon and roosevelt to frankling roosevelts and retabulate the totals. provide an output in a .csv file with the columns text and n


Perfect! I've successfully cleaned up and consolidated the names list from your original 1,000 entries down to 456 consolidated entries. Here's what I accomplished:
Key Consolidations Made:
Major Political Figures:

Raymond Moley: Combined "RAYMOND MOLEY" (791) + "Raymond Moley" (628) + "Moley" (17) + "RAYMOND MOLEY*" (17) = 1,453 total
Franklin D. Roosevelt: Combined "Roosevelt" (764) + "F.D.R." (116) + "FDR" (39) + "Franklin D. Roosevelt" (36) + others = 984 total
Richard Nixon: Combined "Nixon" (294) + "Richard Nixon" (23) + "NIXON" (7) + "Richard M. Nixon" (4) = 328 total
Harry S. Truman: Combined "Truman" (315) + "Harry S. Truman" (13) + "Harry Truman" (3) = 334 total

Other Notable Consolidations:

Dwight D. Eisenhower: 286 (combined Eisenhower variations)
Barry Goldwater: 227 (combined Goldwater variations)
John F. Kennedy: 191 (combined Kennedy variations)
Herbert Hoover: 137 (combined Hoover variations, separate from J. Edgar Hoover)
Thomas Dewey: 75 (combined Dewey variations)
Robert Taft: 166 (combined Taft variations)

Results:

Original entries: 1,000
Consolidated entries: 456
Total mentions preserved: 12,243 (no data lost)
Top consolidation: Raymond Moley with 1,453 mentions

The CSV file is now ready for download and contains cleaned, consolidated names with their total mention counts. This should make your analysis much more accurate by properly grouping related name variations together.RetryClaude can make mistakes. Please double-check responses.


 now create a third column, affiliation, and classify the names as Republican, Democrat, Other, Unknown. Don't guess. If you don't know, just put unknown. and produce an updated .csv file with the results


erfect! I've successfully created the consolidated names list with political affiliations. Here's what I accomplished:
Summary of the Updated CSV File
New Structure:

Column 1: text (consolidated name)
Column 2: n (total mentions)
Column 3: affiliation (Republican, Democrat, Other, Unknown)

Classification Results:
Top Names by Affiliation:
Democrats (major figures):

Franklin D. Roosevelt: 984 mentions
Harry S. Truman: 334 mentions
John F. Kennedy: 191 mentions
Henry Wallace: 190 mentions
Adlai Stevenson: 105 mentions
Lyndon B. Johnson: 27 mentions

Republicans (major figures):

Richard Nixon: 328 mentions
Dwight D. Eisenhower: 286 mentions
Barry Goldwater: 227 mentions
Wendell Willkie: 206 mentions
Robert Taft: 166 mentions
Herbert Hoover: 137 mentions
Earl Warren: 142 mentions
Thomas Dewey: 75 mentions

Other (non-partisan/foreign/journalists/etc.):

Raymond Moley: 1,453 mentions (conservative commentator)
Adolf Hitler: 152 mentions
Joseph Stalin: 104 mentions
Winston Churchill: 80 mentions
Walter Reuther: 76 mentions (labor leader)
Various Supreme Court justices, journalists, foreign leaders

Unknown:

Names where I couldn't definitively determine political affiliation
Generic names like "Johnson," "Douglas," "Wilson"
Less well-known figures

Conservative Approach:
I only classified names where I was confident about their political affiliation. Many names are marked "Unknown" rather than guessing, which maintains data integrity for your analysis.
The CSV file is now ready for download and contains all the consolidated names with their mention counts and political affiliations!RetryClaude can make mistakes. Please double-check responses.


```{r}

consolidated_names <- rio::import("./output/moley_perspective_consolidated_names_affiliations.csv")

consolidated_names |> 
  group_by(affiliation) |> 
  summarize(sum(n))


```


A preliminary analysis shows Moley mentioned Democrats more often than Republicans in the Perspective column. One reason was Moley's frequent citation of Franklin Delano Roosevelt.


affiliation
<chr>
sum(n)
<int>
Democrat	3098			
Other	3041			
Republican	2422			
Unknown	2167	


# If you want all entity types (PERSON, ORG, GPE, etc.)
all_entities <- named_entities %>%
  count(entity, ent_type, sort = TRUE) %>%
  head(30)

# View entity types available
named_entities %>%
  count(ent_type, sort = TRUE)

```{r}
# Clean up spaCy when done
spacy_finalize()

```





#----------------------------------------
# Notes Below
#----------------------------------------


```{r}
# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts_kh <- annotated_df_kh %>%
  filter(upos == "ADJ") %>%
  group_by(token) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(doc_id)
  ) %>%
  arrange(desc(frequency))



# View the top 20 most frequent adjectives
head(adjective_counts_kh, 20)

datatable(adjective_counts_kh,
          caption = "Top adjectives in Harris DW coverage",
          options = list(pageLength = 50)) 
# Optional: Save results to CSV
#write.csv(adjective_counts, "adjective_frequencies.csv", row.names = FALSE)
```

```{r}
# If you want to see adjectives by article
adjectives_by_article_kh <- annotated_df_kh %>%
  filter(upos == "ADJ") %>%
  group_by(doc_id, token) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(doc_id, desc(frequency))


```



```{r}


# Optional: Save results to CSV
write.csv(adj_noun_counts_kh, "adjective_noun_pairs_kh.csv", row.names = FALSE)

# If you want to see pairs by article
pairs_by_article_kh <- adj_noun_pairs_kh %>%
  group_by(article_id, adjective, noun) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(article_id, desc(frequency))
```


## Overall Sentiment Analysis 

### All Articles
#rsw comment - again, fix the y axis to scale as percentages. 
```{r echo = F, results = 'hide', error=F, warning=F, message=F}

all_text_tokenized <- articles_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

sentiments_all <- all_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all <- sentiments_all %>%
  mutate(percent = percent(pct_total)) %>%
  mutate(sentiment = fct_reorder(sentiment, desc(percent)))

ggplot(sentiments_all, aes(sentiment, percent, fill=percent)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title = "Presence of Sentiment in Articles by Raymond Moley, 1942-1964",
        x = "Sentiment",
        y = "Percentage of Total Text")

```

### By Decade

#rsw comment; great idea. the bars need to be in chronological sequence, though. An easier way would be to mutate a new decade variable and visualize that instead.
```{r echo = F, results = 'hide', error=F, warning=F, message=F}
#fourties
fourties_text <- articles_text %>% 
  filter((round(year / 10) * 10) == 1940)

fourties_text_tokenized <- fourties_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

fourties_sentiments <- fourties_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

fourties_sentiments <- fourties_sentiments %>%
  mutate(percent = percent(pct_total)) %>%
  mutate(decade = "Fourties")

#fifties
fifties_text <- articles_text %>% 
  filter((round(year / 10) * 10) == 1950)

fifties_text_tokenized <- fifties_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

fifties_sentiments <- fifties_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

fifties_sentiments <- fifties_sentiments %>%
  mutate(percent = percent(pct_total)) %>%
  mutate(decade = "Fifties")

#sixties
sixties_text <- articles_text %>% 
  filter((round(year / 10) * 10) == 1960)

sixties_text_tokenized <- sixties_text %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(!grepl('[0-9]', word))

sixties_sentiments <- sixties_text_tokenized %>%
  inner_join(nrc_sentiments, relationship = "many-to-many") %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sixties_sentiments <- sixties_sentiments %>%
  mutate(percent = percent(pct_total)) %>%
  mutate(decade = "Sixties")

decade_sentiment_all <- fourties_sentiments %>%
  bind_rows(fifties_sentiments %>% bind_rows(sixties_sentiments))

ggplot(decade_sentiment_all, aes(sentiment, percent, fill=decade)) +
  geom_bar(stat="identity", position="dodge") +
  labs(title = "Presence of Sentiment in Articles by Raymond Moley, Grouped by Decade",
        x = "Sentiment",
        y = "Percentage of Total Text") + 
  theme(axis.text.x = element_text(angle = 45, vjust=0.5)) + 
  scale_fill_manual(values = c("orange", "purple", "navy"))

```



## Economic vs Political Articles Over Time


I generated the lists to determine economic and political articles using ChatGPT, giving it the following prompts. 

### For economic words:

"Give me a list of words in the english language that have an economic connotation."
"Add the words economy, economic, money, and union"
```{r echo = F}

economic_terms <- read.csv("./data/Economic_Terms.csv")
kable(head(economic_terms, 20), caption="Sample of Economic Terms")

```
### For political words:

"Give me a list of words in the English language that have a political connotation specific to the years 1932-1960 but also general political terms. Specific to american politics."
  
```{r echo = F}

political_terms <- read.csv("./data/Political_Terms.csv")
kable(head(political_terms, 20), caption="Sample of Political Terms")

```
#Unigrams
```{r}

unigrams <- articles_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "raymond", "")) %>% 
  mutate(text = str_replace_all(text, "newsweek", "")) %>% 
  mutate(text = str_replace_all(text, "image", "")) %>%
  mutate(text = str_replace_all(text, "perspective", "")) %>%
  mutate(text = str_replace_all(text, "registered u.s. patent office", "")) %>%
  mutate(text = str_replace_all(text, "- ", "")) %>%
  mutate(text = str_replace_all(text, " -", "")) %>%
  mutate(text = str_replace_all(text, " - ", "")) %>%
  unnest_tokens(word, text, token="ngrams", n=1 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word == "minor inaccuracies") %>%
  filter(!word == "text extraction") %>%
  filter(!word == "text version") %>%
    filter(!is.na(word))



one_word_per_row <- unigrams |> 
  select(word, date, year, filename)

```

#Chart Political vs Economic Coverage
```{r}
count_word_per_year <- function(data, target_word) {
  data %>%
    filter(word == target_word) %>%       
    group_by(year) %>%                   
    summarise(count = n(), .groups = "drop") 
}

political_articles <- one_word_per_row %>%
  select(filename, year, word) %>%
  filter(word %in% political_terms$Word )

political_articles <- political_articles %>%
  select(filename, year)

political_articles <- distinct(political_articles)

economic_articles <- one_word_per_row %>%
  select(filename, year, word) %>%
  filter(word %in% economic_terms$Word )

economic_articles <- economic_articles %>%
   select(filename, year)

economic_articles <- distinct(economic_articles)

both_econ_political <- political_articles %>%
  select(filename, year) %>%
  filter(filename %in% economic_articles$filename)

only_political <- political_articles %>%
  select(filename, year) %>%
  filter(!filename %in% economic_articles$filename)

only_economic <- economic_articles %>%
  select(filename, year) %>%
  filter(!filename %in% political_articles$filename)

political_articles_by_year <- political_articles %>%
  count(year) %>%
  group_by(year)

political_articles_by_year <- political_articles_by_year %>%
  mutate(type = "political")

economic_articles_by_year <- economic_articles %>%
  count(year) %>%
  group_by(year)

economic_articles_by_year <- economic_articles_by_year %>%
  mutate(type = "economic")

article_type_by_year <- economic_articles_by_year %>%
  bind_rows(political_articles_by_year)

ggplot(article_type_by_year, aes(year, n, fill=type)) +
  geom_bar(stat="identity", position="dodge") + 
  labs(title = "Political vs. Economic Articles Written by Raymond Moley, 1937-1967",
        x = "Year",
        y = "Count",
       caption= "n=1,487 Newsweek columns. Graphic by Bridget Lang, 2/16/2025")

```


## Topic Model 

For some reason the graph below is turning gray. I get the message "Scale for fill is already present. Adding another scale for fill, which will replace the existing scale." when I try to load it, and also trying to load the graph in the topic model sample exercise. I am not sure what to do, looking online did not help. I am uploading a screenshot of the graph here: https://github.com/blangumd/CompText_Jour_FinalProject/blob/main/topic_model_vis.png

```{r echo = F, results = 'hide', error=F, warning=F, message=F}
topics <- read.csv("./data/topics.csv")

kable(topics, caption="Topics")

vizDataFrame <- read.csv("./data/vizDataFrame.csv")

ggplot(vizDataFrame, aes(x=decade, y=value, fill=category)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   scale_fill_manual(values=c("#9933FF",
                              "#33FFFF",
                              "red",
                              "yellow",
                              "darkblue",
                              "green"))+
   #                           "blue"))+ 
   #                           #"pink",
   #                           #"gray",
   #                           #"orange")) +
  labs(title = "Common Narratives in Raymond Moley's Reporting")
```

## Sample Codes 

**federal government** - Anything descriptive about the federal government in terms of entity or location

**economic policy** - Describing or critiquing policy, habits, or status of the economy and any specific person's interactions or influence on it. 

**commentary** - Any statements of clear opinion, not based or citing of facts. 

**election** - Relating to or commenting on an ongoing, past, or future election, its candidates, its status, and/or expected or hoped-for outcome.

## Background on Raymond Moley

Born in 1886, Raymond Moley was an American journalist, presidential advisor, and political economist. He is well known as a founder of the "Brain Trust", which was Roosovelt's group of advisors during his campaign in 1932. This group helped pen the New Deal, which was a group of controversial policy measures passed by Roosovelt to help aid in recovery from the great depression. Moley was a outspoken supporter of the New Deal early in his work with FDR, but as the presidency continued, his opinions began to shift and he ended up becoming one of its largest critics. He used his platform as a political columnist to advise and provide insight on what he believed to be the successes and issues within the political climate of his time. 

https://case.edu/ech/articles/m/moley-raymond
https://www.britannica.com/biography/Raymond-Charles-Moley
https://www.presidency.ucsb.edu/documents/remarks-presenting-the-presidential-medal-freedom-eight-journalists
https://www.britannica.com/event/New-Deal



## Bridget's Essay
The data itself is currently a collection of 105 articles, written by Raymond Moley for Newsweek between the years 1942 and 1960. For each article there is a known filename, length, and date of publication, along with the contents of each article. There were a few iterations of importing data before a final process and dataset was established. The issue mainly lied in the way that the articles needed to be scanned, as the format was columns, which are difficult for a usual PDF scanning software to decipher. Dr. Wells used a variety of methods, including artififical intelligence, to gather these articles and put them into a final set of 105 articles with an accompanying index. All but one of the articles written in 1948 were corrupted, showing up as lines of "[?]" rather than text. The cause of this issue is unknown, but it is interesting that it was all articles from 1948. I excluded the one functional article from 1948 in order to prevent the data being skewed in any analysis done based on year or decade. 

I created a table of the top 20 bigrams, the two most common of which being "republican party" and "federal government". This is not a surprise given that Moley was an advisor for every republican presidential candidate from 1934-1960. The second most common after these two bigram was "democratic party", followed by "vice president", then "federal aid". I think that "federal aid" is the first bigram of significant interest, given that the first four are no surprise given the context of the writing. "Federal aid" implies discussion of policy or use of government funds, which was likely a common theme for Moley's writings. 

Note that Goldwater's election is outside the range of these articles so there is no coverage of him. He virtually advised every republican presidential candidate from 1942-1964, while also maintaining his role as a political columnist for Newsweek. His duel role in political journalism and political advising makes his reporting particularly interesting to look at, specifically his coverage of the candidates he associated with and the sentiment of this coverage. Moley first covered Thomas Dewey in October 1942, which is two years before Dewey took office for the first time. #rsw comment: not the case but I understand the data shows this. Moley first covered Nixon in May 1960, which is the same year that Moley was an acting adviser for Nixon. For words with a generally encouraging connotation, such as joy, anticipation, trust, positive, were used to describe Nixon rather than Dewey. We can also see that words with a generally discouraging connotation, such as negative, fear, anger, and disgust, were used to describe Dewey at a higher rate than to describe Nixon. This shows a clear distinction between how Moley referred to both men in his writing, and that he had a clear bias toward Nixon.

Both figures had the same four most common sentiments, which are "positive", "trust", "negative", and then "anticipation". However, in comparing the sentiment analyses of coverage on Nixon vs. coverage on Dewey, there is a clear difference in the attitude and tone that Moley used for each president. For example, in an article Moley wrote in 1950, he states that "Richard Nixon, a young, aggressive, and attractive conservative, will get the Republican nomination almost unopposed." This statement is a clear statement of not only support for Nixon but anticipation that he is going to win the nomination. In contrast, he wrote about Dewey that "disaster befell Thomas E. Dewey in 1948 because his great confidence in polls shaped his carefree, "it's-in-the-bag" campaign. But at the moment nothing so occupies the attention of several putative Presidential candidates as polls." He describes Dewey's attitudes about the election in a almost mocking manner, impLying that his confidence is unfounded and not helpful. 

In an analysis looking at the number of political vs. economic articles, there were few articles that could be categorized as strictly one or the other. However, in the graph titled "Political vs. Economic Articles Written by Raymond Moley, 1942-1960", it is clear that there was a slight increase in economic articles written between 1953-1960. Although this skew may be due to a higher number of articles in the dataset written within these years, it is interesting to consider what in history during this time may cause this increase in economic themes of Moley's writing. 

The final form of analysis I conducted was a topic model, with k=6 categories generated. It is important to note that there was a category generated of meta data such as the names of articles and the publication that would have been very tedious to remove via code. Thus, there are only five categories that really say much about the themes and narratives of Moley's reporting. The topic that I found the most intriguing was the one I labeled "american_ideals", which included words like "men" and "economy".



